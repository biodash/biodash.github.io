[{"authors":["admin"],"categories":null,"content":"Jelmer Poelstra started at the MCIC (Molecular and Cellular Imaging Center) in June 2020, where he provides bioinformatics support and education. His background is in evolutionary and population genetics, and most of his research has focused on understanding speciation using genomic approaches.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1616606334,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://biodash.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Jelmer Poelstra started at the MCIC (Molecular and Cellular Imaging Center) in June 2020, where he provides bioinformatics support and education. His background is in evolutionary and population genetics, and most of his research has focused on understanding speciation using genomic approaches.","tags":null,"title":"Jelmer Poelstra","type":"authors"},{"authors":["jessica-cooperstone"],"categories":null,"content":"Jessica Cooperstone is an Assistant Professor in Horticulture and Crop Science, and Food Science and Technology at The Ohio State University working at the intersection of plant science and human nutrition.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1616606334,"objectID":"da60494872ddaf739115b5da033f1fed","permalink":"https://biodash.github.io/authors/jessica-cooperstone/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jessica-cooperstone/","section":"authors","summary":"Jessica Cooperstone is an Assistant Professor in Horticulture and Crop Science, and Food Science and Technology at The Ohio State University working at the intersection of plant science and human nutrition.","tags":null,"title":"Jessica Cooperstone","type":"authors"},{"authors":["michael-broe"],"categories":null,"content":"Michael Broe is a Bioinformatics Research Scientist at the Department of Evolution, Ecology, and Organismal Biology.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1616606334,"objectID":"f515857d0961e2ded569db22cc57c70c","permalink":"https://biodash.github.io/authors/michael-broe/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/michael-broe/","section":"authors","summary":"Michael Broe is a Bioinformatics Research Scientist at the Department of Evolution, Ecology, and Organismal Biology.","tags":null,"title":"Michael Broe","type":"authors"},{"authors":["mike-sovic"],"categories":null,"content":"Mike Sovic is a Bioinformatics Research Scientist at CAPS, the Center for Applied Plant Sciences.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1616606334,"objectID":"e25a3ca217243530f65efb3cd930207b","permalink":"https://biodash.github.io/authors/mike-sovic/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/mike-sovic/","section":"authors","summary":"Mike Sovic is a Bioinformatics Research Scientist at CAPS, the Center for Applied Plant Sciences.","tags":null,"title":"Mike Sovic","type":"authors"},{"authors":["stephen-opiyo"],"categories":null,"content":"Stephen Opiyo is a bioinformatics and biostatistics research scientist at MCIC Columbus.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1616606334,"objectID":"9c82e356b8ac88d63b60055c202796b4","permalink":"https://biodash.github.io/authors/stephen-opiyo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/stephen-opiyo/","section":"authors","summary":"Stephen Opiyo is a bioinformatics and biostatistics research scientist at MCIC Columbus.","tags":null,"title":"Stephen Opiyo","type":"authors"},{"authors":["Jelmer Poelstra"],"categories":null,"content":" Setting up Like last time, we\u0026rsquo;ll mostly use tidyverse tools to explore the diamonds dataset, which is also part of the tidyverse.\nWe\u0026rsquo;ll also have one look at the flights dataset, for which we\u0026rsquo;ll need to load the nycflights13 package:\n## You only need to install packages if you haven't previously done so # install.packages(\"nycflights13\") # install.packages(\"tidyverse\")  ## But you'll have to load packages for every R session: library(nycflights13) library(tidyverse) #\u0026gt; ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ── #\u0026gt; ✔ ggplot2 3.3.6 ✔ purrr  0.3.5  #\u0026gt; ✔ tibble  3.1.8 ✔ dplyr  1.0.10 #\u0026gt; ✔ tidyr  1.2.1 ✔ stringr 1.4.1  #\u0026gt; ✔ readr  2.1.3 ✔ forcats 0.5.2  #\u0026gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── #\u0026gt; ✖ dplyr::filter() masks stats::filter() #\u0026gt; ✖ dplyr::lag() masks stats::lag()  Let\u0026rsquo;s again take a quick look at the diamonds dataset before we begin:\ndiamonds #\u0026gt; # A tibble: 53,940 × 10 #\u0026gt; carat cut color clarity depth table price x y z #\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;ord\u0026gt; \u0026lt;ord\u0026gt; \u0026lt;ord\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 #\u0026gt; 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 #\u0026gt; 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 #\u0026gt; 4 0.29 Premium I VS2 62.4 58 334 4.2 4.23 2.63 #\u0026gt; 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 #\u0026gt; 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 #\u0026gt; 7 0.24 Very Good I VVS1 62.3 57 336 3.95 3.98 2.47 #\u0026gt; 8 0.26 Very Good H SI1 61.9 55 337 4.07 4.11 2.53 #\u0026gt; 9 0.22 Fair E VS2 65.1 61 337 3.87 3.78 2.49 #\u0026gt; 10 0.23 Very Good H VS1 59.4 61 338 4 4.05 2.39 #\u0026gt; # … with 53,930 more rows  On each row, we have information about one individual diamond, such as its carat and price (x, y, and z represent the diamond\u0026rsquo;s length, width, and depth, respectively).\nFinally, we\u0026rsquo;ll again set a ggplot2 \u0026ldquo;theme\u0026rdquo; that is a little better-looking than the default one (this setting will apply until you restart R/RStudio):\n# This changes two things: # - theme_minimal() gives an overall different look, with a white background # - base_size = 14 will make the text relatively bigger theme_set(theme_minimal(base_size = 14))   Chapter 7.4: Missing values Removing outliers Let\u0026rsquo;s assume you have established that certain outlier values in your data are untrustworthy. For instance, see the plot below for the diamonds data, a scatterplot of diamond width (y) versus depth (z):\nggplot(data = diamonds,  mapping = aes(x = z, y = y)) +  geom_point()   Three rather extreme outliers deviate not just in their absolute values, but also in their relative values: overall, depth and width are strongly correlated, yet the extremely large z value does not correspond to a large y value at all (and so on for the other outliers).\nTo get rid of outliers in your dataset, you have two main options. First, you could completely remove rows that contain outliers, for example with dplyr\u0026rsquo;s filter() function:\n# Remove rows for which column y is smaller than 3 or larger than 20:  diamonds_no_outliers \u0026lt;- diamonds %\u0026gt;% filter(y \u0026lt; 3 | y \u0026gt; 20)  But you may not want throw out entire rows, because the values for the other variables (columns) in these rows might be valid and valuable. Therefore, an alternative is to convert outliers to NAs (missing values), and a convenient way to do that is with the ifelse() function:\ndiamonds_NA_outliers \u0026lt;- diamonds %\u0026gt;%  mutate(y = ifelse(test = y \u0026lt; 3 | y \u0026gt; 20, yes = NA, no = y))   More on ifelse() To better understand ifelse(), a simple example may help. First, we create a vector with integers from 1 to 10:\nx \u0026lt;- 1:10 x #\u0026gt; [1] 1 2 3 4 5 6 7 8 9 10  The following expression will return a logical vector that indicates, for each value of x, whether it is smaller than 5:\nx \u0026lt; 5 #\u0026gt; [1] TRUE TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE  We can use that expression as the test in ifelse(), and turn values smaller than 5 into NA (yes = NA), while leaving the other values unchanged (no = x):\nifelse(test = x \u0026lt; 5, yes = NA, no = x) #\u0026gt; [1] NA NA NA NA 5 6 7 8 9 10    Comparing observations with and without missing data It can be useful to compare distributions among rows with and without missing values. To do that, we can first create a new column that indicates whether a value for a variable of interest is missing or not. Then, we can map an aesthetic like color to this missing-or-not column to show the two groups separately.\nBelow, we\u0026rsquo;ll compare flights with and without missing values for departure time (dep_time), i.e. cancelled and not-cancelled flights, using the geom geom_freqpoly that we also saw last time:\nflights %\u0026gt;%   mutate(cancelled = is.na(dep_time)) %\u0026gt;%   ggplot(mapping = aes(x = sched_dep_time, color = cancelled)) +  geom_freqpoly(binwidth = 100) # (100 = 1 hour, so we plot by hour)   However, the large difference in the absolute counts of cancelled versus not-cancelled flights makes it hard to see relative differences along the x-axis.\nWe can use geom_density() to produce a density plot, where the height of the lines is only determined by the relative counts, allowing us to see if cancelled flights have a different distribution:\nflights %\u0026gt;%   mutate(cancelled = is.na(dep_time)) %\u0026gt;%   ggplot(mapping = aes(x = sched_dep_time, color = cancelled)) +   geom_density()   It looks like flights at the end of the day are much more commonly cancelled than those early on, which is what we might have expected.\n Chapter 7.5: Covariation This section of the book covers the exploration of covariation among two variables. For example, is there a relationship between the cut and the price of a diamond (a categorical and continuous variable)? Or an association between the cut and color of a diamond (two categorical variables)? Or between the carat and the price of a diamond (two continuous variables)?\n7.5.1: A categorical and continuous variable In out last plot above, we already explored the relationship between a categorical variable (cancelled \u0026amp; not-cancelled flights) and a continuous one (departure time), and we did so with so with a frequency polygon (geom_freqpoly).\nLet\u0026rsquo;s see another example, this time for the diamonds dataset, to examine whether prices differ among diamond cuts:\nggplot(data = diamonds,  mapping = aes(x = price, color = cut)) +   geom_density()   A boxplot is another classic way to show the relationship between a categorical and a continuous variable. The book has a good explanation of what the components of a boxplot (box, median line, whiskers, outliers) represent. Let\u0026rsquo;s make a boxplot of diamond price by cut:\n# To make the plot better-looking, and to see the same colors as above, # we'll also map the fill aesthetic to cut: ggplot(data = diamonds,  mapping = aes(x = cut, y = price, fill = cut)) +  geom_boxplot() +  guides(fill = \"none\") # Just to turn the legend for fill off   A less formal, but sometimes more informative variant of this type of plot is a violin plot, where the width represents the number of data points:\nggplot(data = diamonds,  mapping = aes(x = cut, y = price, fill = cut)) +  geom_violin() +  guides(fill = \"none\") # Just to turn the legend for fill off   7.5.2: Two categorical variables As the books says,\n To visualise the covariation between categorical variables, you\u0026rsquo;ll need to count the number of observations for each combination.\n A quick way to do that is with geom_count():\nggplot(data = diamonds,  mapping = aes(x = color, y = cut)) +  geom_count()   A heatmap is a slightly more visually appealing way to plot this. We can create one using geom_tile() by first calculating the counts ourselves, and then mapping these counts to the fill color of the tiles:\ndiamonds %\u0026gt;%   count(color, cut) %\u0026gt;% # This will create a column 'n' with the counts  ggplot(mapping = aes(x = color, y = cut, fill = n)) +  geom_tile()   7.5.3: Two continuous variables Visualizing the relationship between two continuous variables is perhaps the most intuitive of the variable type combinations. It can be done with simple scatterplots (geom_point()), of which we have already seen a couple of examples.\nThe books covers a few strategies that can be useful when dealing with large datasets, when relationships may be hidden due to overplotting. Consider the relationship between the carat (weight) and price of diamonds:\nggplot(data = diamonds,  mapping = aes(x = carat, y = price)) +  geom_point()   There is clearly some overplotting going on here, with areas of solid black \u0026ndash; though this type of thing can get a lot worse. Here, the overall pattern is still apparent.\nMaking points transparent is one strategy to more clearly see patterns in the data in the face of overplotting:\n# An alpha of 1 (the default) is opaque and an alpha of 1 is transparent  ggplot(data = diamonds,  mapping = aes(x = carat, y = price)) +   geom_point(alpha = 0.01)   Two other strategies covered in the book are:\n  Use a geom that does 2-dimensional binning for you (e.g. geom_bin2d).\n  Bin one of the continuous variables, effectively turning it into a categorical variable, so that we can use plot types like boxplots. You\u0026rsquo;ll try that in the exercises.\n   Breakout Rooms All the exercises use the diamonds dataset. After loading the tidyverse, this dataset should be available to you.\n Exercise 1 Use the function cut_number() to divide the carat values into 10 bins, and create a boxplot of diamond prices for each of these bins.\nTip: If you can\u0026rsquo;t read the x-axis labels (bin names) in the resulting plot because they overlap, consider flipping the plot: simply swap the x and y aesthetic assignments (alternatively, use the stand-alone function coord_flip()).\n  Hints (click here)    Run ?cut_number to see the documentation for this function.\n  You can start by creating a binned column with mutate() and cut_number(carat, n = 10), or you can create the bins \u0026ldquo;on the fly\u0026rdquo;, by simply using cut_number(carat, n = 10) as the x or y aesthetic.\n     Solution (click here)  To be able to read the axis labels, I moved carat to the y axis (and I also added a y-axis label):\ndiamonds %\u0026gt;%  ggplot(mapping = aes(y = cut_number(carat, n = 10), x = price)) +   geom_boxplot() +  labs(y = \"carat (binned)\")   The book has a different way of doing this, using the group aesthetic. This is less intuitive but has the advantage of keeping the carat axis labels as if it still were a regular continuous variable. It also makes the width of the boxes represent the width of the bins, which you may or may not like:\ndiamonds %\u0026gt;%   ggplot(mapping = aes(x = carat, y = price)) +   geom_boxplot(mapping = aes(group = cut_number(carat, n = 10)))   Alternatively, if you want to have control over the ranges that the bins cover (but not over the number of points in each bin), you can use the cut_width() function:\ndiamonds %\u0026gt;%  ggplot(mapping = aes(y = cut_width(carat, width = 0.4), x = price)) +   geom_boxplot() +  labs(y = \"carat (binned)\")   In that case, consider using varwidth = TRUE to make the width of the boxes to reflect the number of data points:\ndiamonds %\u0026gt;%  ggplot(mapping = aes(y = cut_width(carat, width = 0.4), x = price)) +   geom_boxplot(varwidth = TRUE) +  labs(y = \"carat (binned)\")       Exercise 2   Create a heatmap (geom_tile()) that shows the mean diamond price for each diamond color and cut combination (you\u0026rsquo;ll have to compute the mean price first).\n  From your heatmap, would you say that going from color D =\u0026gt; J is associated with an increase or a decrease in the mean price?\n  Tip: add + scale_fill_viridis_c() to your code for a much nicer color scale.\n  Hints (click here)    In the heatmap, you\u0026rsquo;ll want color along the x axis and cut along the y axis (or vice versa), and you\u0026rsquo;ll want to fill the tiles by the mean price.\n  You\u0026rsquo;ll first have to compute the mean diamond price for each of the color-cut combinations: use group_by() and then summarize().\n     Solution just for getting the mean price (click here)  diamonds %\u0026gt;%  group_by(color, cut) %\u0026gt;%  summarize(price = mean(price)) #\u0026gt; `summarise()` has grouped output by 'color'. You can override using the #\u0026gt; `.groups` argument.#\u0026gt; # A tibble: 35 × 3 #\u0026gt; # Groups: color [7] #\u0026gt; color cut price #\u0026gt; \u0026lt;ord\u0026gt; \u0026lt;ord\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 D Fair 4291. #\u0026gt; 2 D Good 3405. #\u0026gt; 3 D Very Good 3470. #\u0026gt; 4 D Premium 3631. #\u0026gt; 5 D Ideal 2629. #\u0026gt; 6 E Fair 3682. #\u0026gt; 7 E Good 3424. #\u0026gt; 8 E Very Good 3215. #\u0026gt; 9 E Premium 3539. #\u0026gt; 10 E Ideal 2598. #\u0026gt; # … with 25 more rows     Full solution (click here)  diamonds %\u0026gt;%  group_by(color, cut) %\u0026gt;%  summarize(price = mean(price)) %\u0026gt;%  ggplot(mapping = aes(x = color, y = cut, fill = price)) +  geom_tile() +  scale_fill_viridis_c() #\u0026gt; `summarise()` has grouped output by 'color'. You can override using the #\u0026gt; `.groups` argument.  It looks like going from color D to J is associated with an overall increase in the mean price of diamonds.\n    Exercise 3   Make a plot to visualize the relationship between color and carat.\n  Seeing this relationship, and knowing that carat and price are strongly related (see the scatterplots above), do you think this might have influenced (confounded) the apparent relationship between color and price?\n  How could you quickly create a price index that controls for carat? Make a heatmap with that price index instead of the raw price.\n    Hints (click here)    Use a boxplot, violin plot and/or a density plot to visualize the relationship between color and carat.\n  If higher carats are causally associated with higher prices, and certain colors have higher mean carats than others, it is not fair to look at the effect of color on price without somehow taking carat into account.\n  A simple way of taking carat into account is by using \u0026ldquo;price per carat\u0026rdquo; (price divided by carat) rather than the raw price in your heatmap.\n     Solution (click here)   To visualize the relationship between color and carat, you could for example use a boxplot and/or a density plot:  diamonds %\u0026gt;%  ggplot(aes(x = color, y = carat)) +  geom_boxplot()   diamonds %\u0026gt;%  ggplot(aes(x = carat, color = color)) +  geom_density()     It looks like going from D =\u0026gt; J, carat is typically higher.\n  Since carat is strongly positively associated with price, it is not fair to compare prices among colors without controlling for carat.\n  A simple way to do so is dividing price by carat to create an index that represents the \u0026ldquo;price per carat\u0026rdquo;. Then, you can use that index instead of the raw price in your heatmap:\n  diamonds %\u0026gt;%  group_by(color, cut) %\u0026gt;%  mutate(price_per_carat = price / carat) %\u0026gt;%   summarize(price_per_carat = mean(price_per_carat)) %\u0026gt;%  ggplot(mapping = aes(x = color, y = cut, fill = price_per_carat)) +  geom_tile() +  scale_fill_viridis_c() #\u0026gt; `summarise()` has grouped output by 'color'. You can override using the #\u0026gt; `.groups` argument.   Now, it looks like going from D =\u0026gt; J is associated with a decrease rather than an increase in the mean price!      Exercise 4 (bonus) To get another perspecective on the relationship between color, carat, and price (see the previous exercise), modify the earlier scatterplot of carat and price simply by mapping diamond color to the color aesthetic.\n  Solution (click here)  A scatterplot of carat and price that includes diamond color confirms that D diamonds are more expensive than J diamonds (and so on) once you take carat into account:\ndiamonds %\u0026gt;%  ggplot(aes(x = carat, y = price, color = color)) +  geom_point()       Bonus: (re)ordering factor levels In the plots with diamond cut, you might have noticed that the cuts are ordered in a custom, sensible way rather than alphabetically. This is possible because the cut column has the data type factor.\nIf we convert cut to a regular character data type, the custom order disappears (it is now ordered alphabetically):\ndiamonds %\u0026gt;%  mutate(cut = as.character(cut)) %\u0026gt;%   ggplot(mapping = aes(x = cut, y = price)) +  geom_boxplot()   We could set a different custom order using the levels argument of the factor function (the same code would work if cut would not yet have been a factor at all):\ncut_order \u0026lt;- c(\"Very Good\", \"Fair\", \"Good\", \"Ideal\", \"Premium\")  diamonds %\u0026gt;%  mutate(cut = factor(cut, levels = cut_order)) %\u0026gt;%   ggplot(mapping = aes(x = cut, y = price)) +  geom_boxplot()   Or, you could make the order of the factor levels depend on the data (!):\ndiamonds %\u0026gt;%  mutate(cut = reorder(cut, price, FUN = median)) %\u0026gt;%   ggplot(mapping = aes(x = cut, y = price)) +  geom_boxplot()   ","date":1669852800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669923271,"objectID":"21fd2a808d0d57df43ebd0798b7d15a6","permalink":"https://biodash.github.io/codeclub/s04e16/","publishdate":"2022-12-01T00:00:00Z","relpermalink":"/codeclub/s04e16/","section":"codeclub","summary":"We continue with this chapter on Exploratory Data Analysis (EDA), now covering missing values (and the ifelse function to turn outliers into missing values) and covariation among variables, with a couple of new plot types: boxplots and heatmaps.","tags":["codeclub","r4ds"],"title":"S04E16: R for Data Science - Exploratory Data Analysis II","type":"codeclub"},{"authors":["Jelmer Poelstra"],"categories":null,"content":" Setting up To start with, we\u0026rsquo;ll only need to load the tidyverse, as we\u0026rsquo;ll explore a dataset that is automatically loaded along with it.\n## You only need to install if you haven't previously done so # install.packages(\"tidyverse\")  library(tidyverse) #\u0026gt; ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ── #\u0026gt; ✔ ggplot2 3.3.6 ✔ purrr  0.3.5  #\u0026gt; ✔ tibble  3.1.8 ✔ dplyr  1.0.10 #\u0026gt; ✔ tidyr  1.2.1 ✔ stringr 1.4.1  #\u0026gt; ✔ readr  2.1.3 ✔ forcats 0.5.2  #\u0026gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── #\u0026gt; ✖ dplyr::filter() masks stats::filter() #\u0026gt; ✖ dplyr::lag() masks stats::lag()  We\u0026rsquo;ll be working with the diamonds dataset today, so let\u0026rsquo;s take a quick look at it before we begin:\ndiamonds #\u0026gt; # A tibble: 53,940 × 10 #\u0026gt; carat cut color clarity depth table price x y z #\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;ord\u0026gt; \u0026lt;ord\u0026gt; \u0026lt;ord\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 #\u0026gt; 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 #\u0026gt; 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 #\u0026gt; 4 0.29 Premium I VS2 62.4 58 334 4.2 4.23 2.63 #\u0026gt; 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 #\u0026gt; 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 #\u0026gt; 7 0.24 Very Good I VVS1 62.3 57 336 3.95 3.98 2.47 #\u0026gt; 8 0.26 Very Good H SI1 61.9 55 337 4.07 4.11 2.53 #\u0026gt; 9 0.22 Fair E VS2 65.1 61 337 3.87 3.78 2.49 #\u0026gt; 10 0.23 Very Good H VS1 59.4 61 338 4 4.05 2.39 #\u0026gt; # … with 53,930 more rows  On each row, we have information about one individual diamond, such as its carat and price. (x, y, and z represent the diamond\u0026rsquo;s length, width, and depth, respectively.)\nSince we\u0026rsquo;ll be making a bunch of plots with ggplot2, let\u0026rsquo;s use the following trick to set an overarching \u0026ldquo;theme\u0026rdquo; for all plots that is a little better-looking than the default one:\n# This changes two things: # - theme_minimal() gives an overall different look, with a white background # - base_size = 14 will make the text relatively bigger theme_set(theme_minimal(base_size = 14))   Chapter 7.3: Variation Exploring variation in a categorical variable Let\u0026rsquo;s say we want to see how many diamonds there are for each value of cut. When we printed the first lines of the dataframe above, we could see that cut has values like Ideal, Premium, and Good: this is therefore a \u0026ldquo;categorical\u0026rdquo; and not a \u0026ldquo;continuous\u0026rdquo; variable.\nWe could also see that the data type indication for cut was \u0026lt;ord\u0026gt;, which is short for ordered factor. In R, categorical variables can be represented not just as character strings or integers, but also as factors. Factors have a defined set of levels which can be assigned a custom order. That is handy when plotting or when you need to set a reference level in a statistical model. (For more, see the page for this previous Code Club session on factors.)\nTo quickly see which values the variable cut contains, and what their frequencies are, we can use count():\ndiamonds %\u0026gt;% count(cut) #\u0026gt; # A tibble: 5 × 2 #\u0026gt; cut n #\u0026gt; \u0026lt;ord\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 Fair 1610 #\u0026gt; 2 Good 4906 #\u0026gt; 3 Very Good 12082 #\u0026gt; 4 Premium 13791 #\u0026gt; 5 Ideal 21551  To get a feel for the distribution of a categorical variable, making a barplot can also be useful. Recall that when making a plot with ggplot2, we at least need the following components:\n  The ggplot() function, in which we supply the dataframe that we want to use.\n  A geom function, which is basically the type of plot we want to make, such as geom_point() for a scatterplot and geom_bar() for a barplot.\n  An \u0026ldquo;aesthetic mapping\u0026rdquo; that defines which variables to plot along the axes.\n  For a barplot showing cut, our ggplot2 code would look as follows:\nggplot(data = diamonds) +  geom_bar(mapping = aes(x = cut))   When making plots, we typically specify which variable should go along the y-axis, too. But that is not the case for barplots, where the default is to automatically plot a count which is computed from the data.\nExploring variation in a continuous variable We\u0026rsquo;ll take another look at the diamonds dataframe and pick a continuous variable:\ndiamonds #\u0026gt; # A tibble: 53,940 × 10 #\u0026gt; carat cut color clarity depth table price x y z #\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;ord\u0026gt; \u0026lt;ord\u0026gt; \u0026lt;ord\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 #\u0026gt; 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 #\u0026gt; 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 #\u0026gt; 4 0.29 Premium I VS2 62.4 58 334 4.2 4.23 2.63 #\u0026gt; 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 #\u0026gt; 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 #\u0026gt; 7 0.24 Very Good I VVS1 62.3 57 336 3.95 3.98 2.47 #\u0026gt; 8 0.26 Very Good H SI1 61.9 55 337 4.07 4.11 2.53 #\u0026gt; 9 0.22 Fair E VS2 65.1 61 337 3.87 3.78 2.49 #\u0026gt; 10 0.23 Very Good H VS1 59.4 61 338 4 4.05 2.39 #\u0026gt; # … with 53,930 more rows  Let\u0026rsquo;s explore the variation in the continuous variable carat, and do so by making a histogram using geom_histogram():\nggplot(data = diamonds) +  geom_histogram(mapping = aes(x = carat)) #\u0026gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.  Under the hood, a histogram discretizes the continuous data into bins, and then shows the counts (here: number of diamonds) in each bin. We may want to play around with the width of the bins to see more fine-grained or coarse-grained patterns, and can do so using the binwidth argument:\nggplot(data = diamonds) +  geom_histogram(mapping = aes(x = carat), binwidth = 0.5)   If we wanted to see this kind of representation in table-form, using count() directly wouldn\u0026rsquo;t work: we don\u0026rsquo;t have a column with bins for carat, only the raw, numeric values.\nTo create bins, we can use the ggplot2 function cut_width, whose width argument is equivalent to geom_histogram\u0026rsquo;s binwidth:\ndiamonds %\u0026gt;%  mutate(carat_discrete = cut_width(carat, width = 0.5)) %\u0026gt;%  count(carat_discrete) #\u0026gt; # A tibble: 11 × 2 #\u0026gt; carat_discrete n #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 [-0.25,0.25] 785 #\u0026gt; 2 (0.25,0.75] 29498 #\u0026gt; 3 (0.75,1.25] 15977 #\u0026gt; 4 (1.25,1.75] 5313 #\u0026gt; 5 (1.75,2.25] 2002 #\u0026gt; 6 (2.25,2.75] 322 #\u0026gt; 7 (2.75,3.25] 32 #\u0026gt; 8 (3.25,3.75] 5 #\u0026gt; 9 (3.75,4.25] 4 #\u0026gt; 10 (4.25,4.75] 1 #\u0026gt; 11 (4.75,5.25] 1  Multiple variables If we want to show the variation in carat separately for each level of cut, we can map carate also to fill, which is the fill color of the bars:\n# First, let's subset the data to only keep relatively small diamonds:  smaller \u0026lt;- diamonds %\u0026gt;% filter(carat \u0026lt; 3)  # Then, we make the plot: ggplot(data = smaller,  mapping = aes(x = carat, fill = cut)) +  geom_histogram(binwidth = 0.1, color = \"grey20\")   # Above, note that: # - The mapping is now inside 'ggplot()', and we used 'cut' twice # - In geom_histogram(), color is _not_ a mapping and is for the color of the border  Though in a case like this, a linegraph with geom_freqpoly() might be easier to interpret:\nggplot(data = smaller,  mapping = aes(x = carat, colour = cut)) +  geom_freqpoly(binwidth = 0.1, size = 1.5) # (Making thicker lines with 'size')   Unusual values (outliers) Sometimes, plots like histograms have very wide axis limits yet no visible bars on the sides:\nggplot(diamonds) +   geom_histogram(mapping = aes(x = y), binwidth = 0.5)   The x-axis limits are automatically picked based on the data, so there really should be some values all the way up to about 60. We just can\u0026rsquo;t see them, since the y-axis scale goes all the way up to 12,000.\nIf we want to see these counts in the graph, we can zoom in on the y-axis with coord_cartesian():\nggplot(diamonds) +   geom_histogram(mapping = aes(x = y), binwidth = 0.5) +  coord_cartesian(ylim = c(0, 50)) # c(\u0026lt;lower-limit\u0026gt;, \u0026lt;upper-limit\u0026gt;)   Note that in ggplot2, zooming in on a graph and setting axis limits isn\u0026rsquo;t the same thing: you\u0026rsquo;ll learn more about that in the exercises.\nOf course we could also try to find these values in the dataframe itself, which might be more useful than a graph in cases like this. To do so, we can use the filter() function we learned about in the previous chapter:\ndiamonds %\u0026gt;% filter(y \u0026lt; 3 | y \u0026gt; 20) #\u0026gt; # A tibble: 9 × 10 #\u0026gt; carat cut color clarity depth table price x y z #\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;ord\u0026gt; \u0026lt;ord\u0026gt; \u0026lt;ord\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 1 Very Good H VS2 63.3 53 5139 0 0 0  #\u0026gt; 2 1.14 Fair G VS1 57.5 67 6381 0 0 0  #\u0026gt; 3 2 Premium H SI2 58.9 57 12210 8.09 58.9 8.06 #\u0026gt; 4 1.56 Ideal G VS2 62.2 54 12800 0 0 0  #\u0026gt; 5 1.2 Premium D VVS1 62.1 59 15686 0 0 0  #\u0026gt; 6 2.25 Premium H SI2 62.8 59 18034 0 0 0  #\u0026gt; 7 0.51 Ideal E VS1 61.8 55 2075 5.15 31.8 5.12 #\u0026gt; 8 0.71 Good F SI2 64.1 60 2130 0 0 0  #\u0026gt; 9 0.71 Good F SI2 64.1 60 2130 0 0 0   Breakout Rooms These exercises will continue to use the diamonds data, which is automatically loaded when you load the tidyverse.\n Exercise 1 In the diamonds data, explore the distribution of price, which is the price of a diamond in USD. Do you discover anything unusual or surprising?\nMake sure to try different values for the binwidth argument!\n  Hints (click here)    This is a continuous variable, so use geom_histogram().\n  A more fine-grained plot (smaller bins with binwidth) than the default should reveal something odd.\n  You might want to use coord_cartesian() to see the area with the odd pattern in more detail. (Alternatively, you could try filter()ing the data before plotting.)\n     Solution (click here)   geom_histogram() with default settings doesn\u0026rsquo;t reveal anything too weird:  ggplot(data = diamonds,  mapping = aes(x = price)) +  geom_histogram() #\u0026gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.   But with a binwidth of e.g. 100, we start to see something odd: a gap in the distribution.  ggplot(data = diamonds,  mapping = aes(x = price)) +  geom_histogram(binwidth = 100)    Let\u0026rsquo;s take a closer look by zooming in on prices of $2,500 or less:  diamonds %\u0026gt;%  ggplot(mapping = aes(x = price)) +  geom_histogram(binwidth = 25) +  coord_cartesian(xlim = c(0, 2500))   (An alternative approach would be to filter the data before plotting:)\ndiamonds %\u0026gt;%  filter(price \u0026lt; 2500) %\u0026gt;%   ggplot(mapping = aes(x = price)) +  geom_histogram(binwidth = 25)  I have no idea why there are no diamonds with a price of around $1,500 \u0026ndash; anybody?\n    Exercise 2 Compare coord_cartesian() with the similar function lims() to see a narrower range along the y-axis for a histogram. Specifically, make two histograms of price with a y-axis that only goes up to 3,000: one with coord_cartesian(ylim = ...) and one with lims(y = ...).\nWhat is happening in the graph made with lims()?\n(See the hint for example usage of lims(), a function we haven\u0026rsquo;t seen yet.)\n  Hints (click here)  You can use lims() to set arbitrary axis limits:\nggplot(diamonds) +  geom_point(mapping = aes(x = x, y = y)) +  lims(x = c(5, 10), # c(\u0026lt;lower-limit\u0026gt;, \u0026lt;upper-limit\u0026gt;)  y = c(0, 20)) # c(\u0026lt;lower-limit\u0026gt;, \u0026lt;upper-limit\u0026gt;) #\u0026gt; Warning: Removed 17593 rows containing missing values (geom_point).  You could also use the very similar ylim() / xlim() pair of functions, though note the slightly simplified syntax:\nggplot(diamonds) +  geom_point(mapping = aes(x = x, y = y)) +  xlim(5, 10) + # Note: you don't pass a vector inside 'c()' here  ylim(0, 20) #\u0026gt; Warning: Removed 17593 rows containing missing values (geom_point).     Solution (click here)  Whereas the graph produced with coord_cartesian() is simply \u0026ldquo;cut off\u0026rdquo; at the specified limit, the graph produced with lims() is missing bars!\nIt turns out that ggplot2 removes the bars that can\u0026rsquo;t be shown given our y-limit. Notice that it warns us about doing so: #\u0026gt; Warning: Removed 5 rows containing missing values (geom_bar).\nggplot(diamonds) +  geom_histogram(mapping = aes(x = price)) +  coord_cartesian(ylim = c(0, 3000)) #\u0026gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.  ggplot(diamonds) +  geom_histogram(mapping = aes(x = price)) +  lims(y = c(0, 3000)) #\u0026gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.#\u0026gt; Warning: Removed 5 rows containing missing values (geom_bar).      Exercise 3 Using scatterplots, explore the relationship between the width y and the depth z of the diamonds.\nWhat do you think about the outliers? Are they more likely to be unusual diamonds or data entry errors?\n  Hints (click here)    Make a scatterplot with geom_point().\n  Zoom in on the area with most points, to get a better feel for the overall relationship between y and z.\n  Could a diamond with a value of y larger than 20 just be a very large diamond? Or does the corresponding value for z, and the overall relationship between y and z make it more likely that they are outliers?\n     Solution (click here)  Let\u0026rsquo;s start with a simple scatterplot with all data and default axis limits:\nggplot(data = diamonds,  mapping = aes(x = z, y = y)) +  geom_point()   Phew! There are definitely some striking outliers. Let\u0026rsquo;s zoom in on the main cloud of points:\nggplot(data = diamonds,  mapping = aes(x = z, y = y)) +  geom_point() +  coord_cartesian(xlim = c(0, 10), ylim = c(0, 15))   That looks like an overall very tight correlation between width (y) and depth (z).\nTherefore, the outliers of y and z don\u0026rsquo;t just seem to represent very large or very small diamonds, and are likely data entry errors or something along those lines.\n    Exercise 4 (bonus) Explore the distribution of carat. Specifically, compare the number of diamonds of 0.99 (and a little less) carat and those of 1 (and a little more) carat? What do you think is the cause of the difference?\n  Hints (click here)    Make a histogram (geom_histogram()) for carat, and optionally zoom in to the area around 1.\n  Use filter() and count() to specifically check out the diamond counts with a carat of around 1.\n     Solution (click here)  We can start by simply making a histogram for carat:\nggplot(data = diamonds,  mapping = aes(x = carat)) +  geom_histogram(binwidth = 0.01)   That\u0026rsquo;s a weird pattern, with a bunch of peaks and valleys! Let\u0026rsquo;s just show the area around a carat of 1:\ndiamonds %\u0026gt;%  filter(carat \u0026gt; 0.9, carat \u0026lt; 1.1) %\u0026gt;%  ggplot(mapping = aes(x = carat)) +  geom_histogram(binwidth = 0.01)   There\u0026rsquo;s clearly a big uptick around 1, but checking out the raw counts would make it easier to answer the original question:\ndiamonds %\u0026gt;%  filter(carat \u0026gt; 0.9, carat \u0026lt; 1.1) %\u0026gt;%  count(carat) #\u0026gt; # A tibble: 19 × 2 #\u0026gt; carat n #\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 0.91 570 #\u0026gt; 2 0.92 226 #\u0026gt; 3 0.93 142 #\u0026gt; 4 0.94 59 #\u0026gt; 5 0.95 65 #\u0026gt; 6 0.96 103 #\u0026gt; 7 0.97 59 #\u0026gt; 8 0.98 31 #\u0026gt; 9 0.99 23 #\u0026gt; 10 1 1558 #\u0026gt; 11 1.01 2242 #\u0026gt; 12 1.02 883 #\u0026gt; 13 1.03 523 #\u0026gt; 14 1.04 475 #\u0026gt; 15 1.05 361 #\u0026gt; 16 1.06 373 #\u0026gt; 17 1.07 342 #\u0026gt; 18 1.08 246 #\u0026gt; 19 1.09 287  There are suspiciously few diamonds with a carat of 0.99 (and, to a lesser extent, with a carat anywhere above 0.9): could there be some rounding-up going on?\n   ","date":1668470400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669909121,"objectID":"b5a1c50dab4208f997e698970ae56c89","permalink":"https://biodash.github.io/codeclub/s04e15/","publishdate":"2022-11-15T00:00:00Z","relpermalink":"/codeclub/s04e15/","section":"codeclub","summary":"This chapter covers so-called Exploratory Data Analysis (EDA): computing summary stats and especially making quick plots to explore the variation in and distributions of single variables (this session), and looking at covariation among variables (next session).","tags":["codeclub","r4ds"],"title":"S04E15: R for Data Science - Exploratory Data Analysis","type":"codeclub"},{"authors":["Jessica Cooperstone"],"categories":null,"content":" Get data and package # only if you haven't done so before, install the packages install.packages(\"tidyverse\") install.packages(\"nycflights13\") install.packages(\"palmerpenguins\")  Load libraries\nlibrary(tidyverse) # for everything library(palmerpenguins) # for penguins data library(nycflights13) # for flights data  Preview the Dataset We are going to start with the penguins.\nglimpse(penguins) #\u0026gt; Rows: 344 #\u0026gt; Columns: 8 #\u0026gt; $ species \u0026lt;fct\u0026gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel… #\u0026gt; $ island \u0026lt;fct\u0026gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse… #\u0026gt; $ bill_length_mm \u0026lt;dbl\u0026gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, … #\u0026gt; $ bill_depth_mm \u0026lt;dbl\u0026gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, … #\u0026gt; $ flipper_length_mm \u0026lt;int\u0026gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186… #\u0026gt; $ body_mass_g \u0026lt;int\u0026gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, … #\u0026gt; $ sex \u0026lt;fct\u0026gt; male, female, female, NA, female, male, female, male… #\u0026gt; $ year \u0026lt;int\u0026gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…   Review of summarize() Different from select() (which picks columns but retains all rows) and filter() (which picks observations but retains all columns), summarize() creates a wholly new dataframe by making calculations or pulling parts of your original dataframe.\nFor example, if we want to know the mean body_mass_g across all penguins in the dataset, we can calculate that like this:\npenguins %\u0026gt;%  summarize(mean_body_mass = mean(body_mass_g, # calculate mean body mass  na.rm = TRUE)) # remove the NAs #\u0026gt; # A tibble: 1 × 1 #\u0026gt; mean_body_mass #\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 4202.  But we may actually be interested in the different mean body_mass_g calculated separately for the different species of penguins. We can do that by adding the function group_by() like this:\npenguins %\u0026gt;%  group_by(species) %\u0026gt;% # group by species  summarize(mean_body_mass = mean(body_mass_g, # calculate mean body mass by species  na.rm = TRUE)) # remove the NAs #\u0026gt; # A tibble: 3 × 2 #\u0026gt; species mean_body_mass #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 Adelie 3701. #\u0026gt; 2 Chinstrap 3733. #\u0026gt; 3 Gentoo 5076.  And, you can also add more than one group_by() factor, for example, by the full combination species and sex variables.\npenguins %\u0026gt;%  group_by(species, sex) %\u0026gt;% # group by species and sex combos  summarize(mean_body_mass = mean(body_mass_g, # calculate mean body mass by species  na.rm = TRUE)) # remove the NAs #\u0026gt; # A tibble: 8 × 3 #\u0026gt; # Groups: species [3] #\u0026gt; species sex mean_body_mass #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 Adelie female 3369. #\u0026gt; 2 Adelie male 4043. #\u0026gt; 3 Adelie NA 3540  #\u0026gt; 4 Chinstrap female 3527. #\u0026gt; 5 Chinstrap male 3939. #\u0026gt; 6 Gentoo female 4680. #\u0026gt; 7 Gentoo male 5485. #\u0026gt; 8 Gentoo NA 4588.   Using more functions in summarize() Last week we went over using a few different functions within summarize().\n mean(): calculates the mean (set na.rm = TRUE if you was the NAs to be removed before calculating) n(): counts the number of observations  But, there are lots more functions you might be interested to add into summarize(), including:\n median(): when you want to calculate the median instead of the mean sd(): calculates the standard deviation IQR(): calculates the interquartile range sum(): calculates the sum min(): calculates the minimum max(): calculates the maximum n_distinct(): calculates the number of distinct or unique occurences  You can always combine functions or write your own functions to use within summarize() as well. For example, let\u0026rsquo;s say you wanted the standard error of the mean, but there isn\u0026rsquo;t a function built into tidyverse or base R that does that. You can still get your standard error but using a combination of other functions!\npenguins %\u0026gt;%  group_by(species) %\u0026gt;%  drop_na(species, body_mass_g) %\u0026gt;% # dropping observations with missing values  summarize(mean_body_mass = mean(body_mass_g), # mean body mass  n_observations = n(), # how many observations are there  std_error_body_mass = sd(body_mass_g)/(sqrt(n()))) # sd/sqrt(n) #\u0026gt; # A tibble: 3 × 4 #\u0026gt; species mean_body_mass n_observations std_error_body_mass #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 Adelie 3701. 151 37.3 #\u0026gt; 2 Chinstrap 3733. 68 46.6 #\u0026gt; 3 Gentoo 5076. 123 45.5   Breakout Exercises 1 Back to the flights tibble for each of the following exercises.\nExercise 1.1  Determine the mean, standard deviation, min, and max departure delay for each airport of origin. Also determine how many observations you have for each airport of origin.\n  Hints (click here)  Group the tibble by origin. Summarize for each summary you want to calculate. Make sure you exclude missing data.\n   Solution (click here)  flights %\u0026gt;%  group_by(origin) %\u0026gt;%  drop_na(dep_delay) %\u0026gt;%  summarize(mean_dep_delay = mean(dep_delay),  sd_dep_delay = sd(dep_delay),  min_dep_delay = min(dep_delay),  max_dep_delay = max(dep_delay),  n_dep_delay = n()) #\u0026gt; # A tibble: 3 × 6 #\u0026gt; origin mean_dep_delay sd_dep_delay min_dep_delay max_dep_delay n_dep_delay #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 EWR 15.1 41.3 -25 1126 117596 #\u0026gt; 2 JFK 12.1 39.0 -43 1301 109416 #\u0026gt; 3 LGA 10.3 40.0 -33 911 101509      Exercise 1.2  How many distinct destinations are there to fly from for each EWR, JFK, and LGA?\n  Hints (click here)  Think about what you want to group_by() and what of the new functions we talked about today you could use in summarize().\n   Solution (click here)  flights %\u0026gt;%  group_by(origin) %\u0026gt;%   summarize(n_distinct_dest = n_distinct(dest)) #\u0026gt; # A tibble: 3 × 2 #\u0026gt; origin n_distinct_dest #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 EWR 86 #\u0026gt; 2 JFK 70 #\u0026gt; 3 LGA 68      Using slice_() Using slice_() you can subset rows based on their position.\npenguins %\u0026gt;%  drop_na(species, body_mass_g) %\u0026gt;% # dropping observations with missing values  group_by(species) %\u0026gt;%  summarize(mean_body_mass = mean(body_mass_g)) %\u0026gt;%  slice_min(mean_body_mass, n = 1) # min based on mean_body_mass, just give me 1 #\u0026gt; # A tibble: 1 × 2 #\u0026gt; species mean_body_mass #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 Adelie 3701.   slice() allows you to pick rows by the position (i.e., indexing, e.g., slice(4) will give you the 4th row of that df) slice_head() and slice_tail() select the first or last rows. slice_sample() randomly selects rows. slice_min() and slice_max() select rows with highest or lowest values of a variable.  You can use slice_() functions on dataframes themselves (not just on summarized data).\nFor example:\npenguins %\u0026gt;%  drop_na(body_mass_g) %\u0026gt;% # dropping observations with missing values  slice_max(body_mass_g, n = 10) # the 10 heaviest penguins #\u0026gt; # A tibble: 11 × 8 #\u0026gt; species island bill_length_mm bill_depth_mm flipper_len…¹ body_…² sex year #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 Gentoo Biscoe 49.2 15.2 221 6300 male 2007 #\u0026gt; 2 Gentoo Biscoe 59.6 17 230 6050 male 2007 #\u0026gt; 3 Gentoo Biscoe 51.1 16.3 220 6000 male 2008 #\u0026gt; 4 Gentoo Biscoe 48.8 16.2 222 6000 male 2009 #\u0026gt; 5 Gentoo Biscoe 45.2 16.4 223 5950 male 2008 #\u0026gt; 6 Gentoo Biscoe 49.8 15.9 229 5950 male 2009 #\u0026gt; 7 Gentoo Biscoe 48.4 14.6 213 5850 male 2007 #\u0026gt; 8 Gentoo Biscoe 49.3 15.7 217 5850 male 2007 #\u0026gt; 9 Gentoo Biscoe 55.1 16 230 5850 male 2009 #\u0026gt; 10 Gentoo Biscoe 49.5 16.2 229 5800 male 2008 #\u0026gt; 11 Gentoo Biscoe 48.6 16 230 5800 male 2008 #\u0026gt; # … with abbreviated variable names ¹​flipper_length_mm, ²​body_mass_g  Note, we actually got 11, that is because in 10th place there is a tie. Ties are kept by default, but can be turned off by setting with_ties = FALSE.\nYou can also set the proportion of results to return using prop =.\npenguins %\u0026gt;%  drop_na(body_mass_g) %\u0026gt;% # dropping observations with missing values  slice_max(body_mass_g, prop = 0.1) # the top 10% heaviest penguins #\u0026gt; # A tibble: 34 × 8 #\u0026gt; species island bill_length_mm bill_depth_mm flipper_len…¹ body_…² sex year #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 Gentoo Biscoe 49.2 15.2 221 6300 male 2007 #\u0026gt; 2 Gentoo Biscoe 59.6 17 230 6050 male 2007 #\u0026gt; 3 Gentoo Biscoe 51.1 16.3 220 6000 male 2008 #\u0026gt; 4 Gentoo Biscoe 48.8 16.2 222 6000 male 2009 #\u0026gt; 5 Gentoo Biscoe 45.2 16.4 223 5950 male 2008 #\u0026gt; 6 Gentoo Biscoe 49.8 15.9 229 5950 male 2009 #\u0026gt; 7 Gentoo Biscoe 48.4 14.6 213 5850 male 2007 #\u0026gt; 8 Gentoo Biscoe 49.3 15.7 217 5850 male 2007 #\u0026gt; 9 Gentoo Biscoe 55.1 16 230 5850 male 2009 #\u0026gt; 10 Gentoo Biscoe 49.5 16.2 229 5800 male 2008 #\u0026gt; # … with 24 more rows, and abbreviated variable names ¹​flipper_length_mm, #\u0026gt; # ²​body_mass_g  If data are grouped, the operation will be performed group wise, like we see below.\npenguins %\u0026gt;%  drop_na(body_mass_g, species) %\u0026gt;% # dropping observations with missing values  group_by(species) %\u0026gt;%  slice_min(body_mass_g, n = 1)  #\u0026gt; # A tibble: 4 × 8 #\u0026gt; # Groups: species [3] #\u0026gt; species island bill_length_mm bill_depth_mm flipper_le…¹ body_…² sex year #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 Adelie Biscoe 36.5 16.6 181 2850 fema… 2008 #\u0026gt; 2 Adelie Biscoe 36.4 17.1 184 2850 fema… 2008 #\u0026gt; 3 Chinstrap Dream 46.9 16.6 192 2700 fema… 2008 #\u0026gt; 4 Gentoo Biscoe 42.7 13.7 208 3950 fema… 2008 #\u0026gt; # … with abbreviated variable names ¹​flipper_length_mm, ²​body_mass_g  We get the minimum body_mass_g for each species, and in this case we get 2 for Adelie penguins because there is a tie.\n Using summarize() with across() There are also ways that you can combine summarize() to function across() your different variables. The function where() allows you to pick variables where the function is TRUE.\npenguins %\u0026gt;%  drop_na() %\u0026gt;%  group_by(species) %\u0026gt;%  summarize(across(where(is.numeric), mean)) #\u0026gt; # A tibble: 3 × 6 #\u0026gt; species bill_length_mm bill_depth_mm flipper_length_mm body_mass_g year #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 Adelie 38.8 18.3 190. 3706. 2008. #\u0026gt; 2 Chinstrap 48.8 18.4 196. 3733. 2008. #\u0026gt; 3 Gentoo 47.6 15.0 217. 5092. 2008.  # from column bill_length_mm to column flipper_length_mm penguins %\u0026gt;%  drop_na() %\u0026gt;%  group_by(species) %\u0026gt;%  summarize(across(bill_length_mm:flipper_length_mm, mean)) #\u0026gt; # A tibble: 3 × 4 #\u0026gt; species bill_length_mm bill_depth_mm flipper_length_mm #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 Adelie 38.8 18.3 190. #\u0026gt; 2 Chinstrap 48.8 18.4 196. #\u0026gt; 3 Gentoo 47.6 15.0 217. # using the helper contains() penguins %\u0026gt;%  drop_na() %\u0026gt;%  group_by(species) %\u0026gt;%  summarize(across(contains(\"mm\"), mean)) #\u0026gt; # A tibble: 3 × 4 #\u0026gt; species bill_length_mm bill_depth_mm flipper_length_mm #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 Adelie 38.8 18.3 190. #\u0026gt; 2 Chinstrap 48.8 18.4 196. #\u0026gt; 3 Gentoo 47.6 15.0 217.  All of these helper functions can be used outside of summarize() too.\n Breakout Exercises 2 Let\u0026rsquo;s practice using slice() and summarize() with across(), still using the flights data.\nExercise 2.1  What is the tail number (tailnum) for the plane that has, on average, the least arrival delay?? What about the most arrival delay? How many flights did this plane take in this dataset? How would your answer change if you required that a plane take at least 50 flights?\n  Hints (click here)  Group the tibble by the tailnum variable, and summarize to get mean arr_delay and also n(). Then pick the right filter() parameters if necessary, and the appropriate slice_() function.\n   Solution (click here)  Least delayed flights\n# any number of flights flights %\u0026gt;%  group_by(tailnum) %\u0026gt;%  summarize(mean_arr_delay = mean(arr_delay),  n_flights = n()) %\u0026gt;%  slice_min(mean_arr_delay, n = 1) #\u0026gt; # A tibble: 1 × 3 #\u0026gt; tailnum mean_arr_delay n_flights #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 N560AS -53 1 # at least 50 flights flights %\u0026gt;%  group_by(tailnum) %\u0026gt;%  summarize(mean_arr_delay = mean(arr_delay),  n_flights = n()) %\u0026gt;%  filter(n_flights \u0026gt;= 50) %\u0026gt;%  slice_min(mean_arr_delay, n = 1) #\u0026gt; # A tibble: 1 × 3 #\u0026gt; tailnum mean_arr_delay n_flights #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 N548UW -13.0 52  Most delayed flights\n# any number of flights flights %\u0026gt;%  group_by(tailnum) %\u0026gt;%  summarize(mean_arr_delay = mean(arr_delay),  n_flights = n()) %\u0026gt;%  slice_max(mean_arr_delay, n = 1) #\u0026gt; # A tibble: 1 × 3 #\u0026gt; tailnum mean_arr_delay n_flights #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 N844MH 320 1 # at least 50 flights flights %\u0026gt;%  group_by(tailnum) %\u0026gt;%  summarize(mean_arr_delay = mean(arr_delay),  n_flights = n()) %\u0026gt;%  filter(n_flights \u0026gt;= 50) %\u0026gt;%  slice_max(mean_arr_delay, n = 1) #\u0026gt; # A tibble: 1 × 3 #\u0026gt; tailnum mean_arr_delay n_flights #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 N8698A 27.5 57      Exercise 2.2  Calculate the median for air_time, arr_delay and dep_delay by origin. Try to not do each manually.\n  Hints (click here)  Group the tibble by the origin variable, and combine summarize() and across() to get the median for each variable. Try listing them together using c().\n   Solution (click here)  flights %\u0026gt;%  drop_na(air_time, arr_delay, dep_delay, origin) %\u0026gt;%  group_by(origin) %\u0026gt;%  summarize(across(c(air_time, arr_delay, dep_delay), median,   .names = \"\u0026#123;col\u0026#125;_median\")) # extra fun thing to rename columns #\u0026gt; # A tibble: 3 × 4 #\u0026gt; origin air_time_median arr_delay_median dep_delay_median #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 EWR 130 -4 -1 #\u0026gt; 2 JFK 149 -6 -1 #\u0026gt; 3 LGA 115 -5 -3      Exercise 2.3  Which destinations have the longest maximum arrival delay? Which destinations have the shortest? Pull data for the top 10 for both the longest and shortest maximum arrival delay. Keep track of how many flights there are to each location in case that might be useful information.\n  Hints (click here)  Group the tibble by the dest variable, and summarize to get max arr_delay and also n(). Then pick the right slice_() function.\n   Solution (click here)  Least delayed flights %\u0026gt;%  drop_na(dest, arr_delay) %\u0026gt;%  group_by(dest) %\u0026gt;%  summarize(max_arr_delay = max(arr_delay),  n_flights = n()) %\u0026gt;%  slice_min(max_arr_delay, n = 10) #\u0026gt; # A tibble: 10 × 3 #\u0026gt; dest max_arr_delay n_flights #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 LEX -22 1 #\u0026gt; 2 PSP 17 18 #\u0026gt; 3 ANC 39 8 #\u0026gt; 4 HDN 43 14 #\u0026gt; 5 EYW 45 17 #\u0026gt; 6 SBN 53 10 #\u0026gt; 7 MTJ 101 14 #\u0026gt; 8 ILM 143 107 #\u0026gt; 9 ABQ 153 254 #\u0026gt; 10 BZN 154 35  Most delayed\nflights %\u0026gt;%  drop_na(dest, arr_delay) %\u0026gt;%  group_by(dest) %\u0026gt;%  summarize(max_arr_delay = max(arr_delay),  n_flights = n()) %\u0026gt;%  slice_max(max_arr_delay, n = 10) #\u0026gt; # A tibble: 10 × 3 #\u0026gt; dest max_arr_delay n_flights #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 HNL 1272 701 #\u0026gt; 2 CMH 1127 3326 #\u0026gt; 3 ORD 1109 16566 #\u0026gt; 4 SFO 1007 13173 #\u0026gt; 5 CVG 989 3725 #\u0026gt; 6 TPA 931 7390 #\u0026gt; 7 MSP 915 6929 #\u0026gt; 8 ATL 895 16837 #\u0026gt; 9 MIA 878 11593 #\u0026gt; 10 LAS 852 5952      ","date":1668038400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668091815,"objectID":"07cc02bf37e4b520c40442a904369f2f","permalink":"https://biodash.github.io/codeclub/s04e14_r4ds-ch5-5-again/","publishdate":"2022-11-10T00:00:00Z","relpermalink":"/codeclub/s04e14_r4ds-ch5-5-again/","section":"codeclub","summary":"Today we will continue to investigate the summarize() function. Together with group_by(), this function is extremely useful to produce summary statistics of your data by group.","tags":["codeclub","r4ds"],"title":"S04E14: R for Data Science - Chapter 5.6: summarize, some more","type":"codeclub"},{"authors":["Mike Sovic"],"categories":null,"content":" Get the Dataset and Packages # Only if you haven't done so before, install the packages install.packages(\"tidyverse\") install.packages(\"nycflights13\")  # Load the flights dataset and tidyverse library(nycflights13) library(tidyverse)  Preview the Dataset glimpse(flights) #\u0026gt; Rows: 336,776 #\u0026gt; Columns: 19 #\u0026gt; $ year \u0026lt;int\u0026gt; 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2… #\u0026gt; $ month \u0026lt;int\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… #\u0026gt; $ day \u0026lt;int\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… #\u0026gt; $ dep_time \u0026lt;int\u0026gt; 517, 533, 542, 544, 554, 554, 555, 557, 557, 558, 558, … #\u0026gt; $ sched_dep_time \u0026lt;int\u0026gt; 515, 529, 540, 545, 600, 558, 600, 600, 600, 600, 600, … #\u0026gt; $ dep_delay \u0026lt;dbl\u0026gt; 2, 4, 2, -1, -6, -4, -5, -3, -3, -2, -2, -2, -2, -2, -1… #\u0026gt; $ arr_time \u0026lt;int\u0026gt; 830, 850, 923, 1004, 812, 740, 913, 709, 838, 753, 849,… #\u0026gt; $ sched_arr_time \u0026lt;int\u0026gt; 819, 830, 850, 1022, 837, 728, 854, 723, 846, 745, 851,… #\u0026gt; $ arr_delay \u0026lt;dbl\u0026gt; 11, 20, 33, -18, -25, 12, 19, -14, -8, 8, -2, -3, 7, -1… #\u0026gt; $ carrier \u0026lt;chr\u0026gt; \"UA\", \"UA\", \"AA\", \"B6\", \"DL\", \"UA\", \"B6\", \"EV\", \"B6\", \"… #\u0026gt; $ flight \u0026lt;int\u0026gt; 1545, 1714, 1141, 725, 461, 1696, 507, 5708, 79, 301, 4… #\u0026gt; $ tailnum \u0026lt;chr\u0026gt; \"N14228\", \"N24211\", \"N619AA\", \"N804JB\", \"N668DN\", \"N394… #\u0026gt; $ origin \u0026lt;chr\u0026gt; \"EWR\", \"LGA\", \"JFK\", \"JFK\", \"LGA\", \"EWR\", \"EWR\", \"LGA\",… #\u0026gt; $ dest \u0026lt;chr\u0026gt; \"IAH\", \"IAH\", \"MIA\", \"BQN\", \"ATL\", \"ORD\", \"FLL\", \"IAD\",… #\u0026gt; $ air_time \u0026lt;dbl\u0026gt; 227, 227, 160, 183, 116, 150, 158, 53, 140, 138, 149, 1… #\u0026gt; $ distance \u0026lt;dbl\u0026gt; 1400, 1416, 1089, 1576, 762, 719, 1065, 229, 944, 733, … #\u0026gt; $ hour \u0026lt;dbl\u0026gt; 5, 5, 5, 5, 6, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 6, 6, 6… #\u0026gt; $ minute \u0026lt;dbl\u0026gt; 15, 29, 40, 45, 0, 58, 0, 0, 0, 0, 0, 0, 0, 0, 0, 59, 0… #\u0026gt; $ time_hour \u0026lt;dttm\u0026gt; 2013-01-01 05:00:00, 2013-01-01 05:00:00, 2013-01-01 0…   Review of select(), filter(), mutate() select() The flights tibble has 19 variables \u0026ndash; to keep things simple, we\u0026rsquo;ll focus on just a few of these for now. Let\u0026rsquo;s choose the variables (columns) carrier, flight, air_time, and dep_delay:\nflights_exp \u0026lt;- select(flights, carrier, flight, air_time, dep_delay)  flights_exp #\u0026gt; # A tibble: 336,776 × 4 #\u0026gt; carrier flight air_time dep_delay #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 UA 1545 227 2 #\u0026gt; 2 UA 1714 227 4 #\u0026gt; 3 AA 1141 160 2 #\u0026gt; 4 B6 725 183 -1 #\u0026gt; 5 DL 461 116 -6 #\u0026gt; 6 UA 1696 150 -4 #\u0026gt; 7 B6 507 158 -5 #\u0026gt; 8 EV 5708 53 -3 #\u0026gt; 9 B6 79 140 -3 #\u0026gt; 10 AA 301 138 -2 #\u0026gt; # … with 336,766 more rows  filter() There are \u0026gt;336,000 observations (flights) in this dataset. Let\u0026rsquo;s reduce it to just American Airlines flights:\nflights_exp \u0026lt;- filter(flights_exp, carrier == \"AA\")  flights_exp #\u0026gt; # A tibble: 32,729 × 4 #\u0026gt; carrier flight air_time dep_delay #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 AA 1141 160 2 #\u0026gt; 2 AA 301 138 -2 #\u0026gt; 3 AA 707 257 -1 #\u0026gt; 4 AA 1895 152 -4 #\u0026gt; 5 AA 1837 153 13 #\u0026gt; 6 AA 413 192 -2 #\u0026gt; 7 AA 303 140 -1 #\u0026gt; 8 AA 711 248 0 #\u0026gt; 9 AA 305 143 -4 #\u0026gt; 10 AA 1815 142 -3 #\u0026gt; # … with 32,719 more rows  mutate() The column air_time is measured in minutes. What if we wanted a new column air_time_hrs that reports the air time in hours?\nflights_exp \u0026lt;- mutate(flights_exp, air_time_hrs = air_time/60)  flights_exp #\u0026gt; # A tibble: 32,729 × 5 #\u0026gt; carrier flight air_time dep_delay air_time_hrs #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 AA 1141 160 2 2.67 #\u0026gt; 2 AA 301 138 -2 2.3  #\u0026gt; 3 AA 707 257 -1 4.28 #\u0026gt; 4 AA 1895 152 -4 2.53 #\u0026gt; 5 AA 1837 153 13 2.55 #\u0026gt; 6 AA 413 192 -2 3.2  #\u0026gt; 7 AA 303 140 -1 2.33 #\u0026gt; 8 AA 711 248 0 4.13 #\u0026gt; 9 AA 305 143 -4 2.38 #\u0026gt; 10 AA 1815 142 -3 2.37 #\u0026gt; # … with 32,719 more rows   Section 5.6: summarise() Here\u0026rsquo;s an example of how to use summarise() in the simplest way. Notice the similarity in syntax with mutate():\nsummarise(flights, delay = mean(dep_delay, na.rm = TRUE)) #\u0026gt; # A tibble: 1 × 1 #\u0026gt; delay #\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 12.6  summarise() is typically combined with group_by(), calculating the new summarized variable separately for each set of grouped observations in the dataset:\nby_day \u0026lt;- group_by(flights, year, month, day)  summarise(by_day, delay = mean(dep_delay, na.rm = TRUE)) #\u0026gt; # A tibble: 365 × 4 #\u0026gt; # Groups: year, month [12] #\u0026gt; year month day delay #\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 2013 1 1 11.5  #\u0026gt; 2 2013 1 2 13.9  #\u0026gt; 3 2013 1 3 11.0  #\u0026gt; 4 2013 1 4 8.95 #\u0026gt; 5 2013 1 5 5.73 #\u0026gt; 6 2013 1 6 7.15 #\u0026gt; 7 2013 1 7 5.42 #\u0026gt; 8 2013 1 8 2.55 #\u0026gt; 9 2013 1 9 2.28 #\u0026gt; 10 2013 1 10 2.84 #\u0026gt; # … with 355 more rows  And more than one new summarized variable can be calculated - here, there are 3:\nby_dest \u0026lt;- group_by(flights, dest)  delay \u0026lt;- summarise(by_dest,  count = n(),  dist = mean(distance, na.rm = TRUE),  delay = mean(arr_delay, na.rm = TRUE)  )  delay #\u0026gt; # A tibble: 105 × 4 #\u0026gt; dest count dist delay #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 ABQ 254 1826 4.38 #\u0026gt; 2 ACK 265 199 4.85 #\u0026gt; 3 ALB 439 143 14.4  #\u0026gt; 4 ANC 8 3370 -2.5  #\u0026gt; 5 ATL 17215 757. 11.3  #\u0026gt; 6 AUS 2439 1514. 6.02 #\u0026gt; 7 AVL 275 584. 8.00 #\u0026gt; 8 BDL 443 116 7.05 #\u0026gt; 9 BGR 375 378 8.03 #\u0026gt; 10 BHM 297 866. 16.9  #\u0026gt; # … with 95 more rows  Let\u0026rsquo;s look a bit closer at what this is doing (note, this is not in the book):\nflights_sub \u0026lt;- select(flights, dest, distance, arr_delay)  flights_sub #\u0026gt; # A tibble: 336,776 × 3 #\u0026gt; dest distance arr_delay #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 IAH 1400 11 #\u0026gt; 2 IAH 1416 20 #\u0026gt; 3 MIA 1089 33 #\u0026gt; 4 BQN 1576 -18 #\u0026gt; 5 ATL 762 -25 #\u0026gt; 6 ORD 719 12 #\u0026gt; 7 FLL 1065 19 #\u0026gt; 8 IAD 229 -14 #\u0026gt; 9 MCO 944 -8 #\u0026gt; 10 ORD 733 8 #\u0026gt; # … with 336,766 more rows  albuquerque_data \u0026lt;- filter(flights_sub, dest == \"ABQ\")  albuquerque_data #\u0026gt; # A tibble: 254 × 3 #\u0026gt; dest distance arr_delay #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 ABQ 1826 -35 #\u0026gt; 2 ABQ 1826 -18 #\u0026gt; 3 ABQ 1826 -16 #\u0026gt; 4 ABQ 1826 16 #\u0026gt; 5 ABQ 1826 -20 #\u0026gt; 6 ABQ 1826 -14 #\u0026gt; 7 ABQ 1826 -15 #\u0026gt; 8 ABQ 1826 -32 #\u0026gt; 9 ABQ 1826 -28 #\u0026gt; 10 ABQ 1826 -13 #\u0026gt; # … with 244 more rows  mean_albuquerque_delay \u0026lt;- mean(albuquerque_data$arr_delay, na.rm = TRUE)  mean_albuquerque_delay #\u0026gt; [1] 4.38189  Compare the value mean_albuquerque_delay to that in the summarized delay tibble created above:\nmean_albuquerque_delay #\u0026gt; [1] 4.38189 delay #\u0026gt; # A tibble: 105 × 4 #\u0026gt; dest count dist delay #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 ABQ 254 1826 4.38 #\u0026gt; 2 ACK 265 199 4.85 #\u0026gt; 3 ALB 439 143 14.4  #\u0026gt; 4 ANC 8 3370 -2.5  #\u0026gt; 5 ATL 17215 757. 11.3  #\u0026gt; 6 AUS 2439 1514. 6.02 #\u0026gt; 7 AVL 275 584. 8.00 #\u0026gt; 8 BDL 443 116 7.05 #\u0026gt; 9 BGR 375 378 8.03 #\u0026gt; 10 BHM 297 866. 16.9  #\u0026gt; # … with 95 more rows   Breakout Exercises 1 Start with the original flights tibble for each of the following exercises.\nExercise 1  Overall, which carrier had the longest mean arrival delay (arr_delay) in 2013? (Note, all these data are from 2013.)\n  Hints (click here)  Group the tibble by the carrier variable, then summarise to calculate the mean arr_delay for each group. Remember to ignore missing values by setting na.rm = TRUE within the mean() function.\n   Solution (click here)  carrier_grp \u0026lt;- group_by(flights, carrier)  summarise(carrier_grp,  mean_delay = mean(arr_delay, na.rm = TRUE)  )  #\u0026gt; # A tibble: 16 × 2 #\u0026gt; carrier mean_delay #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 9E 7.38  #\u0026gt; 2 AA 0.364 #\u0026gt; 3 AS -9.93  #\u0026gt; 4 B6 9.46  #\u0026gt; 5 DL 1.64  #\u0026gt; 6 EV 15.8  #\u0026gt; 7 F9 21.9  #\u0026gt; 8 FL 20.1  #\u0026gt; 9 HA -6.92  #\u0026gt; 10 MQ 10.8  #\u0026gt; 11 OO 11.9  #\u0026gt; 12 UA 3.56  #\u0026gt; 13 US 2.13  #\u0026gt; 14 VX 1.76  #\u0026gt; 15 WN 9.65  #\u0026gt; 16 YV 15.6      Exercise 2  Evaluate arrival delay by carrier again, but this time, evaluate only for carriers who made at least 10,000 flights.\n  Hints (click here)  Include a second variable in the summarized data from above to reflect the number of observations that went in to calculating the mean value for each group, then filter on this sample size variable. Consider using the function n().\n   Solution (click here)  carrier_delays \u0026lt;- summarise(carrier_grp,  mean_delay = mean(arr_delay, na.rm = TRUE),  n = n()  )  filter(carrier_delays, n \u0026gt;= 10000) #\u0026gt; # A tibble: 9 × 3 #\u0026gt; carrier mean_delay n #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 9E 7.38 18460 #\u0026gt; 2 AA 0.364 32729 #\u0026gt; 3 B6 9.46 54635 #\u0026gt; 4 DL 1.64 48110 #\u0026gt; 5 EV 15.8 54173 #\u0026gt; 6 MQ 10.8 26397 #\u0026gt; 7 UA 3.56 58665 #\u0026gt; 8 US 2.13 20536 #\u0026gt; 9 WN 9.65 12275      Exercise 3  Based on these data, what airport is the worst to fly in to with regards to arriving on time? In other words, which airport (dest) is associated with the highest average arrival delays?\n  Hints (click here)  Group the tibble by the dest variable, summarise with the mean arr_delay for each group, then use arrange() to sort the new variable in descending order, which can be done with desc().\n   Solution (click here)  dest_grp \u0026lt;- group_by(flights, dest)  dest_delays \u0026lt;- summarise(dest_grp,  mean_delay = mean(arr_delay, na.rm = TRUE)  )  arrange(dest_delays, desc(mean_delay)) #\u0026gt; # A tibble: 105 × 2 #\u0026gt; dest mean_delay #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 CAE 41.8 #\u0026gt; 2 TUL 33.7 #\u0026gt; 3 OKC 30.6 #\u0026gt; 4 JAC 28.1 #\u0026gt; 5 TYS 24.1 #\u0026gt; 6 MSN 20.2 #\u0026gt; 7 RIC 20.1 #\u0026gt; 8 CAK 19.7 #\u0026gt; 9 DSM 19.0 #\u0026gt; 10 GRR 18.2 #\u0026gt; # … with 95 more rows      Using Pipes We did this above\u0026hellip;\nby_dest \u0026lt;- group_by(flights, dest)  delay \u0026lt;- summarise(by_dest,  count = n(),  dist = mean(distance, na.rm = TRUE),  delay = mean(arr_delay, na.rm = TRUE)  )  That code can be rewritten with the pipe as follows\u0026hellip;\ndelays \u0026lt;- flights %\u0026gt;%   group_by(dest) %\u0026gt;%   summarise(  count = n(),  dist = mean(distance, na.rm = TRUE),  delay = mean(arr_delay, na.rm = TRUE)  )  Notice here we didn\u0026rsquo;t have to create intermediate/temporary tibbles.\n Breakout Exercises 2 Here we\u0026rsquo;ll redo the same analyses as in the first breakout session, but this time using the pipe.\nExercise 1  Overall, which carrier had the longest mean arrival delay (arr_delay) in 2013? (Note, all these data are from 2013.)\n  Solution (click here)  flights %\u0026gt;%   group_by(carrier) %\u0026gt;%  summarise(mean_delay = mean(arr_delay, na.rm = TRUE)) #\u0026gt; # A tibble: 16 × 2 #\u0026gt; carrier mean_delay #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 9E 7.38  #\u0026gt; 2 AA 0.364 #\u0026gt; 3 AS -9.93  #\u0026gt; 4 B6 9.46  #\u0026gt; 5 DL 1.64  #\u0026gt; 6 EV 15.8  #\u0026gt; 7 F9 21.9  #\u0026gt; 8 FL 20.1  #\u0026gt; 9 HA -6.92  #\u0026gt; 10 MQ 10.8  #\u0026gt; 11 OO 11.9  #\u0026gt; 12 UA 3.56  #\u0026gt; 13 US 2.13  #\u0026gt; 14 VX 1.76  #\u0026gt; 15 WN 9.65  #\u0026gt; 16 YV 15.6      Exercise 2  Evaluate arrival delay by carrier again, but this time, evaluate only for carriers who made at least 10,000 flights.\n  Solution (click here)  flights %\u0026gt;%  group_by(carrier) %\u0026gt;%  summarise(  mean_delay = mean(arr_delay, na.rm = TRUE),  n = n()  ) %\u0026gt;%  filter(n \u0026gt; 10000) #\u0026gt; # A tibble: 9 × 3 #\u0026gt; carrier mean_delay n #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 9E 7.38 18460 #\u0026gt; 2 AA 0.364 32729 #\u0026gt; 3 B6 9.46 54635 #\u0026gt; 4 DL 1.64 48110 #\u0026gt; 5 EV 15.8 54173 #\u0026gt; 6 MQ 10.8 26397 #\u0026gt; 7 UA 3.56 58665 #\u0026gt; 8 US 2.13 20536 #\u0026gt; 9 WN 9.65 12275      Exercise 3  Based on these data, what airport is the worst to fly in to with regards to arriving on time? In other words, which airport (dest) is associated with the highest average arrival delays?\n  Solution (click here)  flights %\u0026gt;%  group_by(dest) %\u0026gt;%  summarise(mean_delay = mean(arr_delay, na.rm = TRUE)) %\u0026gt;%  arrange(desc(mean_delay)) #\u0026gt; # A tibble: 105 × 2 #\u0026gt; dest mean_delay #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 CAE 41.8 #\u0026gt; 2 TUL 33.7 #\u0026gt; 3 OKC 30.6 #\u0026gt; 4 JAC 28.1 #\u0026gt; 5 TYS 24.1 #\u0026gt; 6 MSN 20.2 #\u0026gt; 7 RIC 20.1 #\u0026gt; 8 CAK 19.7 #\u0026gt; 9 DSM 19.0 #\u0026gt; 10 GRR 18.2 #\u0026gt; # … with 95 more rows      ","date":1667433600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667486098,"objectID":"08eb045b884db57380fd48406bc829dc","permalink":"https://biodash.github.io/codeclub/s04e13/","publishdate":"2022-11-03T00:00:00Z","relpermalink":"/codeclub/s04e13/","section":"codeclub","summary":"Today we will introduce summarize() function. Together with group_by(), this function is extremely useful to produce summary statistics of your data by group.","tags":["codeclub","r4ds"],"title":"S04E13: R for Data Science - Chapter 5.6: summarize","type":"codeclub"},{"authors":["Stephen Opiyo"],"categories":null,"content":"  Artwork by Allison Horst    Introduction Today we are going to cover the dplyr function mutate(). You can today\u0026rsquo;s topic in the R 4 Data Science book at: https://r4ds.had.co.nz/transform.html#add-new-variables-with-mutate.\nWe will again be using the nycflights13 and tidyverse packages, so we first need make sure these packages are installed, and then load them for the current session by doing library() commands:\n## Only if you haven't done so before, install the packages install.packages(\"tidyverse\") install.packages(\"nycflights13\")  ## Load the packages library(tidyverse) library(nycflights13)   The mutate() function The mutate() function always adds new columns at the end of your dataset. Let us create a small dataset so we can see the new variables:\nflights_sml \u0026lt;- select(flights,   year:day,   ends_with(\"delay\"),   distance,   air_time )  As a first example, let\u0026rsquo;s add columns gain (delay made up) and speed:\n#\u0026gt; # A tibble: 336,776 × 9 #\u0026gt; year month day dep_delay arr_delay distance air_time gain speed #\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 2013 1 1 2 11 1400 227 -9 370. #\u0026gt; 2 2013 1 1 4 20 1416 227 -16 374. #\u0026gt; 3 2013 1 1 2 33 1089 160 -31 408. #\u0026gt; 4 2013 1 1 -1 -18 1576 183 17 517. #\u0026gt; 5 2013 1 1 -6 -25 762 116 19 394. #\u0026gt; 6 2013 1 1 -4 12 719 150 -16 288. #\u0026gt; 7 2013 1 1 -5 19 1065 158 -24 404. #\u0026gt; 8 2013 1 1 -3 -14 229 53 11 259. #\u0026gt; 9 2013 1 1 -3 -8 944 140 5 405. #\u0026gt; 10 2013 1 1 -2 8 733 138 -10 319. #\u0026gt; # … with 336,766 more rows  mutate(flights_sml,  gain = dep_delay - arr_delay,  speed = distance / air_time * 60 )  transmute() is very similar to mutate(), except the returned dataframe only contains the new variables that were created:\ntransmute(flights,  gain = dep_delay - arr_delay,  hours = air_time / 60,  gain_per_hour = gain / hours )  #\u0026gt; # A tibble: 336,776 × 3 #\u0026gt; gain hours gain_per_hour #\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 -9 3.78 -2.38 #\u0026gt; 2 -16 3.78 -4.23 #\u0026gt; 3 -31 2.67 -11.6  #\u0026gt; 4 17 3.05 5.57 #\u0026gt; 5 19 1.93 9.83 #\u0026gt; 6 -16 2.5 -6.4  #\u0026gt; 7 -24 2.63 -9.11 #\u0026gt; 8 11 0.883 12.5  #\u0026gt; 9 5 2.33 2.14 #\u0026gt; 10 -10 2.3 -4.35 #\u0026gt; # … with 336,766 more rows   Useful creation functions to use with mutate Note that functions must be vectorised before use with mutate().\nArithmetic operators: + (addition), - (subtraction), * (multiplication), / (division), ^ (to the power)\nModular arithmetic: %/% (integer division) and %% (remainder). Modular arithmetic is a handy tool because it allows you to break integers up into pieces. For example:\ntransmute(flights,  dep_time,  hour = dep_time %/% 100,  minute = dep_time %% 100 )  #\u0026gt; # A tibble: 336,776 × 3 #\u0026gt; dep_time hour minute #\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 517 5 17 #\u0026gt; 2 533 5 33 #\u0026gt; 3 542 5 42 #\u0026gt; 4 544 5 44 #\u0026gt; 5 554 5 54 #\u0026gt; 6 554 5 54 #\u0026gt; 7 555 5 55 #\u0026gt; 8 557 5 57 #\u0026gt; 9 557 5 57 #\u0026gt; 10 558 5 58 #\u0026gt; # … with 336,766 more rows  Logs: log (), log2(), log10(). Logarithms are useful transformation for dealing with data that ranges across multiple orders of magnitude.\nOffsets: lead() and lag(). These allow you to refer to leading or lagging values:\n(x \u0026lt;- 1:10)  #\u0026gt; [1] 1 2 3 4 5 6 7 8 9 10  lag(x)  #\u0026gt; [1] NA 1 2 3 4 5 6 7 8 9  lead(x)  #\u0026gt; [1] 2 3 4 5 6 7 8 9 10 NA  Cumulative and rolling aggregates: R provides functions for sums cumsum(), products cumprod(), mins cummin(), and maxes cummax(); and dplyr provides function for mean cummean().\nx  #\u0026gt; [1] 1 2 3 4 5 6 7 8 9 10  cumsum(x)  #\u0026gt; [1] 1 3 6 10 15 21 28 36 45 55  cummean(x)  #\u0026gt; [1] 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5  Logical comparisons: \u0026lt; (less than), \u0026lt;= (equal to or less than), \u0026gt; (greater than), \u0026gt;= (equal to or greater than), != (not equal to), and == (equal to).\nRanking: min_rank() gives the smallest values the small rank, and desc(x) to give the largest values the smallest ranks.\ny \u0026lt;- c(1, 2, 2, NA, 3, 4)   min_rank(y)  #\u0026gt; [1] 1 2 2 NA 4 5  min_rank(desc(y))  #\u0026gt; [1] 5 3 3 NA 2 1  There are other ranks functions, too, e.g., row_number(), dense_rank(), percent_rank(), cume_dist(), ntile().\nrow_number(y)  #\u0026gt; [1] 1 2 3 NA 4 5  dense_rank(y)  #\u0026gt; [1] 1 2 2 NA 3 4  percent_rank(y)  #\u0026gt; [1] 0.00 0.25 0.25 NA 0.75 1.00   Breakout room exercises These correspond to the exercises in the R4DS book, but we\u0026rsquo;re skipping exercises 2 and 6.\nR4DS Exercise 1 Currently dep_time and sched_dep_time are convenient to look at, but hard to compute with because they\u0026rsquo;re not really continuous numbers. Convert them to a more convenient representation of number of minutes since midnight.\n  Hints (click here)   You can use the following code structure (we first select only relevant columns to more easily check the results):  flights %\u0026gt;%   select(dep_time, sched_dep_time) %\u0026gt;%   mutate(  flights_times,  dep_time_mins = YOUR_CODE_HERE,  sched_dep_time_mins = YOUR_CODE_HERE  )    Get the number of hours since midnight using %/% (as in the example in the book) and then convert this number to minutes.\n  Then, get the number of minutes past the hour using %% (again as in the example in the book), and add this number to the previous one to get the total number of minutes past midnight.\n  Now, you might think you\u0026rsquo;re all done, but there is one remaining problem: midnight was originally represented as 2400, which the code described above would convert to 1440 (24 * 60). But it should be 0 instead. This is a tricky one, but you can handle it using an additional %% calculation.\n     Solution (click here)   To handle the midnight case, use %% 1440: this would simply return the original value for all values below 1440, and would return 0 only for 1440 (midnight):  flights %\u0026gt;%  select(dep_time, sched_dep_time) %\u0026gt;%   mutate(  dep_time_mins = (dep_time %/% 100 * 60 + dep_time %% 100) %% 1440,  sched_dep_time_mins = (sched_dep_time %/% 100 * 60 + sched_dep_time %% 100) %% 1440  )  #\u0026gt; # A tibble: 336,776 × 4 #\u0026gt; dep_time sched_dep_time dep_time_mins sched_dep_time_mins #\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 517 515 317 315 #\u0026gt; 2 533 529 333 329 #\u0026gt; 3 542 540 342 340 #\u0026gt; 4 544 545 344 345 #\u0026gt; 5 554 600 354 360 #\u0026gt; 6 554 558 354 358 #\u0026gt; 7 555 600 355 360 #\u0026gt; 8 557 600 357 360 #\u0026gt; 9 557 600 357 360 #\u0026gt; 10 558 600 358 360 #\u0026gt; # … with 336,766 more rows   R4DS Exercise 3 Compare dep_time, sched_dep_time, and dep_delay. How would you expect those three numbers to be related? Test if this is indeed the case.\n  Hints (click here)    We should expect dep_delay to equal the difference between sched_dep_time and dep_time.\n  To check this, the first step is to use your code from exercise 1 to create columns with the sched_dep_time and the dep_time in minutes past midnight.\n  Then, create a column with the difference in minutes between sched_dep_time and dep_time (e.g. called our_delay_calc).\n  Next, create a column with the difference between our_delay_calc and dep_delay (e.g. called delay_diff).\n  Finally, use filter() to see if there are any rows where the delay_diff does not equal 0. Recall that we would expect no differences at all, if all is well. So what is going on with those rows?\n     Solution (click here)  flights %\u0026gt;%   select(dep_time, sched_dep_time, dep_delay) %\u0026gt;%   mutate(  dep_time_min = (dep_time %/% 100 * 60 + dep_time %% 100) %% 1440,  sched_dep_time_min = (sched_dep_time %/% 100 * 60 + sched_dep_time %% 100) %% 1440,  our_delay_calc = dep_time_min - sched_dep_time_min,  dep_delay_diff = our_delay_calc - dep_delay  ) %\u0026gt;%   filter(dep_delay_diff != 0)  #\u0026gt; # A tibble: 1,236 × 7 #\u0026gt; dep_time sched_dep_time dep_delay dep_time_min sched_dep_ti…¹ our_d…² dep_d…³ #\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 848 1835 853 528 1115 -587 -1440 #\u0026gt; 2 42 2359 43 42 1439 -1397 -1440 #\u0026gt; 3 126 2250 156 86 1370 -1284 -1440 #\u0026gt; 4 32 2359 33 32 1439 -1407 -1440 #\u0026gt; 5 50 2145 185 50 1305 -1255 -1440 #\u0026gt; 6 235 2359 156 155 1439 -1284 -1440 #\u0026gt; 7 25 2359 26 25 1439 -1414 -1440 #\u0026gt; 8 106 2245 141 66 1365 -1299 -1440 #\u0026gt; 9 14 2359 15 14 1439 -1425 -1440 #\u0026gt; 10 37 2230 127 37 1350 -1313 -1440 #\u0026gt; # … with 1,226 more rows, and abbreviated variable names ¹​sched_dep_time_min, #\u0026gt; # ²​our_delay_calc, ³​dep_delay_diff  These dep_delay_diff values are all 1440 minutes (= 24 hours), and as you can see, these are flights that were scheduled to depart one day but left only the next day (past midnight).\nSo, for our dep_time_min and sched_dep_time_min to always work for calculations, they should also take dates into account \u0026ndash; and should e.g. be minutes past some date in the past.\n R4DS Exercise 4 Find the 10 most delayed flights using a ranking function. How do you want to handle ties? Carefully read the documentation for min_rank().\n  Hints (click here)    Use mutate() to create a new column with the ranks, then arrange by this column and/or filter only top ranks to get the most delayed flights.\n  To see the differences in the handling of ties between row_number(), min_rank(), and dense_rank(), take a look this:\n  tibble(delays = c(3, 5, 5, 5, 130, 276),  rank_rownr = row_number(delays),  rank_min = min_rank(delays),  rank_dense = dense_rank(delays))  #\u0026gt; # A tibble: 6 × 4 #\u0026gt; delays rank_rownr rank_min rank_dense #\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 3 1 1 1 #\u0026gt; 2 5 2 2 2 #\u0026gt; 3 5 3 2 2 #\u0026gt; 4 5 4 2 2 #\u0026gt; 5 130 5 5 3 #\u0026gt; 6 276 6 6 4     Solution (click here)  flights %\u0026gt;%  select(dep_time, sched_dep_time, dep_delay) %\u0026gt;%  mutate(dep_delay_rank = min_rank(desc(dep_delay))) %\u0026gt;%   filter(dep_delay_rank \u0026lt;= 10) %\u0026gt;%  arrange(dep_delay_rank)#\u0026gt; # A tibble: 10 × 4 #\u0026gt; dep_time sched_dep_time dep_delay dep_delay_rank #\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 641 900 1301 1 #\u0026gt; 2 1432 1935 1137 2 #\u0026gt; 3 1121 1635 1126 3 #\u0026gt; 4 1139 1845 1014 4 #\u0026gt; 5 845 1600 1005 5 #\u0026gt; 6 1100 1900 960 6 #\u0026gt; 7 2321 810 911 7 #\u0026gt; 8 959 1900 899 8 #\u0026gt; 9 2257 759 898 9 #\u0026gt; 10 756 1700 896 10   R4DS Exercise 5 What does 1:3 + 1:10 return? Why?\n  Hints (click here)    First, you should realize that 1:3 expands to a vector with the values 1, 2, and 3, and similarly for 1:10.\n  Many R operations are vectorized, which means that when pairing two vectors (including the case where one of those vectors is just a single value), the shorter vector will be recycled. In the example below, 3 is recycled and added to every single value in the vector 1:5:\n  3 + 1:5  #\u0026gt; [1] 4 5 6 7 8     Solution (click here)  1:3 + 1:10  #\u0026gt; Warning in 1:3 + 1:10: longer object length is not a multiple of shorter object length#\u0026gt; [1] 2 4 6 5 7 9 8 10 12 11  R gives a warning because the length of the longer vector isn\u0026rsquo;t a multiple of the length of the shorter vector: it recycles 1:3 three times and is then left over with a single value from 1:3 to be paired with 10. This kind of thing is usually not intended.\n Bonus exercise You can use the paste0() function to combine strings of text, for instance:\ncarrier \u0026lt;- \"UA\" flight \u0026lt;- 1545 paste0(\"The full flight number is: \", carrier, flight)  #\u0026gt; [1] \"The full flight number is: UA1545\"  Use paste0() inside mutate() to create a new column flight_no which has the full flight number (carrier followed by flight, like above) for each flight.\n  Solution (click here)  flights %\u0026gt;%  select(carrier, flight) %\u0026gt;%  mutate(flight_no = paste0(carrier, flight))  #\u0026gt; # A tibble: 336,776 × 3 #\u0026gt; carrier flight flight_no #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt;  #\u0026gt; 1 UA 1545 UA1545  #\u0026gt; 2 UA 1714 UA1714  #\u0026gt; 3 AA 1141 AA1141  #\u0026gt; 4 B6 725 B6725  #\u0026gt; 5 DL 461 DL461  #\u0026gt; 6 UA 1696 UA1696  #\u0026gt; 7 B6 507 B6507  #\u0026gt; 8 EV 5708 EV5708  #\u0026gt; 9 B6 79 B679  #\u0026gt; 10 AA 301 AA301  #\u0026gt; # … with 336,766 more rows   \n Acknowledgements We used https://jrnold.github.io/r4ds-exercise-solutions to provide hints and solutions for the exercises.\n","date":1666828800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666892689,"objectID":"6321c0fe04ceced4603f1b3cb6108eed","permalink":"https://biodash.github.io/codeclub/s04e12_r4ds-ch5-5/","publishdate":"2022-10-27T00:00:00Z","relpermalink":"/codeclub/s04e12_r4ds-ch5-5/","section":"codeclub","summary":"Today we will cover the mutate() function to create new columns in dataframes. While this function itself is simple enough, we will get to see some interesting data manipulation techniques and operators such as those for integer division and remainder.","tags":["codeclub","r4ds"],"title":"S04E12: R for Data Science - Chapter 5.5: mutate","type":"codeclub"},{"authors":["Michael Broe"],"categories":null,"content":"\n Introduction Use this to download the R Markdown:\nrmd_file \u0026lt;- \"https://raw.githubusercontent.com/biodash/biodash.github.io/master/assets/scripts/Arrange%2C%20Select.Rmd\" download.file(rmd_file, destfile = \"arrange_select.Rmd\")  Today we are going to finish off the material on the dplyr function arrange() that we didn\u0026rsquo;t get to last week, and introduce a new dplyr function select().\nThe way I am presenting this is through an RMarkdown document, to interactively explore the material in these two sections of Chapter 5, Data transformation\nRecall that for all dplyr data manipulation functions there is a common template:\n  the first argument of the function is the input data frame\n  the next arguments say what you want to do with that data frame, using variable names (no quotes)\n  the result is a new dataframe\n  The fact that all these functions have a common template makes it possible to chain steps together, to make complex code chunks out of simple steps, one step at a time, as we will see below.\nWe will again be using the nycflights13 and tidyverse packages, so we first need make sure these packages are installed, and then load them for the current session by doing library() commands:\nlibrary(tidyverse) library(nycflights13)  arrange() Refer to the biodash page to see where we left off:\nS04E08: R for Data Science - Chapter 5.1 - 5.3\narrange() is the equivalent of the Excel sort command.\nSo what happens if just \u0026lsquo;arrange\u0026rsquo; flights (the data frame) with no other arguments?\narrange(flights)#\u0026gt; # A tibble: 336,776 × 19 #\u0026gt; year month day dep_time sched_de…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier #\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt;  #\u0026gt; 1 2013 1 1 517 515 2 830 819 11 UA  #\u0026gt; 2 2013 1 1 533 529 4 850 830 20 UA  #\u0026gt; 3 2013 1 1 542 540 2 923 850 33 AA  #\u0026gt; 4 2013 1 1 544 545 -1 1004 1022 -18 B6  #\u0026gt; 5 2013 1 1 554 600 -6 812 837 -25 DL  #\u0026gt; 6 2013 1 1 554 558 -4 740 728 12 UA  #\u0026gt; 7 2013 1 1 555 600 -5 913 854 19 B6  #\u0026gt; 8 2013 1 1 557 600 -3 709 723 -14 EV  #\u0026gt; 9 2013 1 1 557 600 -3 838 846 -8 B6  #\u0026gt; 10 2013 1 1 558 600 -2 753 745 8 AA  #\u0026gt; # … with 336,766 more rows, 9 more variables: flight \u0026lt;int\u0026gt;, tailnum \u0026lt;chr\u0026gt;, #\u0026gt; # origin \u0026lt;chr\u0026gt;, dest \u0026lt;chr\u0026gt;, air_time \u0026lt;dbl\u0026gt;, distance \u0026lt;dbl\u0026gt;, hour \u0026lt;dbl\u0026gt;, #\u0026gt; # minute \u0026lt;dbl\u0026gt;, time_hour \u0026lt;dttm\u0026gt;, and abbreviated variable names #\u0026gt; # ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay  Nothing changes. This is identical to the original data frame:\nhead(flights)#\u0026gt; # A tibble: 6 × 19 #\u0026gt; year month day dep_time sched_dep…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier #\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt;  #\u0026gt; 1 2013 1 1 517 515 2 830 819 11 UA  #\u0026gt; 2 2013 1 1 533 529 4 850 830 20 UA  #\u0026gt; 3 2013 1 1 542 540 2 923 850 33 AA  #\u0026gt; 4 2013 1 1 544 545 -1 1004 1022 -18 B6  #\u0026gt; 5 2013 1 1 554 600 -6 812 837 -25 DL  #\u0026gt; 6 2013 1 1 554 558 -4 740 728 12 UA  #\u0026gt; # … with 9 more variables: flight \u0026lt;int\u0026gt;, tailnum \u0026lt;chr\u0026gt;, origin \u0026lt;chr\u0026gt;, #\u0026gt; # dest \u0026lt;chr\u0026gt;, air_time \u0026lt;dbl\u0026gt;, distance \u0026lt;dbl\u0026gt;, hour \u0026lt;dbl\u0026gt;, minute \u0026lt;dbl\u0026gt;, #\u0026gt; # time_hour \u0026lt;dttm\u0026gt;, and abbreviated variable names ¹​sched_dep_time, #\u0026gt; # ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay  But there a whole bunch of variables we can sort by. An easy way to see them is using the glimpse() function, which basically puts the \u0026lsquo;columns into rows\u0026rsquo;, so you can see them more easily, without scrolling off the screen. This is a great way to see just what the columns are in a complex data frame that you inherit from someone or other.\nglimpse(flights)#\u0026gt; Rows: 336,776 #\u0026gt; Columns: 19 #\u0026gt; $ year \u0026lt;int\u0026gt; 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2… #\u0026gt; $ month \u0026lt;int\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… #\u0026gt; $ day \u0026lt;int\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… #\u0026gt; $ dep_time \u0026lt;int\u0026gt; 517, 533, 542, 544, 554, 554, 555, 557, 557, 558, 558, … #\u0026gt; $ sched_dep_time \u0026lt;int\u0026gt; 515, 529, 540, 545, 600, 558, 600, 600, 600, 600, 600, … #\u0026gt; $ dep_delay \u0026lt;dbl\u0026gt; 2, 4, 2, -1, -6, -4, -5, -3, -3, -2, -2, -2, -2, -2, -1… #\u0026gt; $ arr_time \u0026lt;int\u0026gt; 830, 850, 923, 1004, 812, 740, 913, 709, 838, 753, 849,… #\u0026gt; $ sched_arr_time \u0026lt;int\u0026gt; 819, 830, 850, 1022, 837, 728, 854, 723, 846, 745, 851,… #\u0026gt; $ arr_delay \u0026lt;dbl\u0026gt; 11, 20, 33, -18, -25, 12, 19, -14, -8, 8, -2, -3, 7, -1… #\u0026gt; $ carrier \u0026lt;chr\u0026gt; \"UA\", \"UA\", \"AA\", \"B6\", \"DL\", \"UA\", \"B6\", \"EV\", \"B6\", \"… #\u0026gt; $ flight \u0026lt;int\u0026gt; 1545, 1714, 1141, 725, 461, 1696, 507, 5708, 79, 301, 4… #\u0026gt; $ tailnum \u0026lt;chr\u0026gt; \"N14228\", \"N24211\", \"N619AA\", \"N804JB\", \"N668DN\", \"N394… #\u0026gt; $ origin \u0026lt;chr\u0026gt; \"EWR\", \"LGA\", \"JFK\", \"JFK\", \"LGA\", \"EWR\", \"EWR\", \"LGA\",… #\u0026gt; $ dest \u0026lt;chr\u0026gt; \"IAH\", \"IAH\", \"MIA\", \"BQN\", \"ATL\", \"ORD\", \"FLL\", \"IAD\",… #\u0026gt; $ air_time \u0026lt;dbl\u0026gt; 227, 227, 160, 183, 116, 150, 158, 53, 140, 138, 149, 1… #\u0026gt; $ distance \u0026lt;dbl\u0026gt; 1400, 1416, 1089, 1576, 762, 719, 1065, 229, 944, 733, … #\u0026gt; $ hour \u0026lt;dbl\u0026gt; 5, 5, 5, 5, 6, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 6, 6, 6… #\u0026gt; $ minute \u0026lt;dbl\u0026gt; 15, 29, 40, 45, 0, 58, 0, 0, 0, 0, 0, 0, 0, 0, 0, 59, 0… #\u0026gt; $ time_hour \u0026lt;dttm\u0026gt; 2013-01-01 05:00:00, 2013-01-01 05:00:00, 2013-01-01 0…  (Notice there are 19 columns).\nSay we wanted to sort by month, and pull all the \u0026lsquo;Christmassy\u0026rsquo; flights to the top. We can arrange by month, and sort descending:\narrange(flights, desc(month))#\u0026gt; # A tibble: 336,776 × 19 #\u0026gt; year month day dep_time sched_de…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier #\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt;  #\u0026gt; 1 2013 12 1 13 2359 14 446 445 1 B6  #\u0026gt; 2 2013 12 1 17 2359 18 443 437 6 B6  #\u0026gt; 3 2013 12 1 453 500 -7 636 651 -15 US  #\u0026gt; 4 2013 12 1 520 515 5 749 808 -19 UA  #\u0026gt; 5 2013 12 1 536 540 -4 845 850 -5 AA  #\u0026gt; 6 2013 12 1 540 550 -10 1005 1027 -22 B6  #\u0026gt; 7 2013 12 1 541 545 -4 734 755 -21 EV  #\u0026gt; 8 2013 12 1 546 545 1 826 835 -9 UA  #\u0026gt; 9 2013 12 1 549 600 -11 648 659 -11 US  #\u0026gt; 10 2013 12 1 550 600 -10 825 854 -29 B6  #\u0026gt; # … with 336,766 more rows, 9 more variables: flight \u0026lt;int\u0026gt;, tailnum \u0026lt;chr\u0026gt;, #\u0026gt; # origin \u0026lt;chr\u0026gt;, dest \u0026lt;chr\u0026gt;, air_time \u0026lt;dbl\u0026gt;, distance \u0026lt;dbl\u0026gt;, hour \u0026lt;dbl\u0026gt;, #\u0026gt; # minute \u0026lt;dbl\u0026gt;, time_hour \u0026lt;dttm\u0026gt;, and abbreviated variable names #\u0026gt; # ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay  And now say we want to just order the Christmas flights coming into Columbus. We can first filter() on CMH, and then chain (pipe) that filter statement into arrange, using the pipe notation.\nFirst filter on destination CMH:\nfilter(flights, dest == \"CMH\")#\u0026gt; # A tibble: 3,524 × 19 #\u0026gt; year month day dep_time sched_de…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier #\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt;  #\u0026gt; 1 2013 1 1 805 815 -10 1006 1010 -4 MQ  #\u0026gt; 2 2013 1 1 1107 1115 -8 1305 1310 -5 MQ  #\u0026gt; 3 2013 1 1 1153 1159 -6 1350 1341 9 EV  #\u0026gt; 4 2013 1 1 1257 1300 -3 1454 1450 4 MQ  #\u0026gt; 5 2013 1 1 1458 1500 -2 1658 1655 3 MQ  #\u0026gt; 6 2013 1 1 1459 1501 -2 1651 1651 0 EV  #\u0026gt; 7 2013 1 1 1522 1530 -8 1731 1725 6 MQ  #\u0026gt; 8 2013 1 1 1759 1759 0 1957 1949 8 EV  #\u0026gt; 9 2013 1 1 1816 1805 11 2013 1955 18 MQ  #\u0026gt; 10 2013 1 1 2008 2015 -7 2206 2210 -4 MQ  #\u0026gt; # … with 3,514 more rows, 9 more variables: flight \u0026lt;int\u0026gt;, tailnum \u0026lt;chr\u0026gt;, #\u0026gt; # origin \u0026lt;chr\u0026gt;, dest \u0026lt;chr\u0026gt;, air_time \u0026lt;dbl\u0026gt;, distance \u0026lt;dbl\u0026gt;, hour \u0026lt;dbl\u0026gt;, #\u0026gt; # minute \u0026lt;dbl\u0026gt;, time_hour \u0026lt;dttm\u0026gt;, and abbreviated variable names #\u0026gt; # ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay  (Notice we now have only 3,524 rows, as opposed to 336,776 in the full data frame).\nWe can pass that output data frame on to arrange() using the \u0026lsquo;pipe symbol\u0026rsquo; %\u0026gt;%:\nfilter(flights, dest == \"CMH\") %\u0026gt;%   arrange(desc(month))#\u0026gt; # A tibble: 3,524 × 19 #\u0026gt; year month day dep_time sched_de…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier #\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt;  #\u0026gt; 1 2013 12 1 644 614 30 836 805 31 EV  #\u0026gt; 2 2013 12 1 1129 1135 -6 1316 1330 -14 MQ  #\u0026gt; 3 2013 12 1 1253 1259 -6 1435 1454 -19 MQ  #\u0026gt; 4 2013 12 1 1328 1326 2 1507 1524 -17 EV  #\u0026gt; 5 2013 12 1 1513 1515 -2 1709 1720 -11 MQ  #\u0026gt; 6 2013 12 1 1629 1455 94 1822 1650 92 MQ  #\u0026gt; 7 2013 12 1 1728 1730 -2 1918 1925 -7 MQ  #\u0026gt; 8 2013 12 1 1922 1930 -8 2102 2130 -28 MQ  #\u0026gt; 9 2013 12 1 1951 1930 21 2121 2115 6 MQ  #\u0026gt; 10 2013 12 2 622 610 12 759 801 -2 EV  #\u0026gt; # … with 3,514 more rows, 9 more variables: flight \u0026lt;int\u0026gt;, tailnum \u0026lt;chr\u0026gt;, #\u0026gt; # origin \u0026lt;chr\u0026gt;, dest \u0026lt;chr\u0026gt;, air_time \u0026lt;dbl\u0026gt;, distance \u0026lt;dbl\u0026gt;, hour \u0026lt;dbl\u0026gt;, #\u0026gt; # minute \u0026lt;dbl\u0026gt;, time_hour \u0026lt;dttm\u0026gt;, and abbreviated variable names #\u0026gt; # ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay  So this is a first example of chaining together simple steps, to get a more complex result.\nFinally, with arrange, you can add other variables to \u0026lsquo;break ties\u0026rsquo;. In the following example, we first sort on month (descending), and then sort on dep_delay:\nfilter(flights, dest == \"CMH\") %\u0026gt;%   arrange(desc(month), dep_delay)#\u0026gt; # A tibble: 3,524 × 19 #\u0026gt; year month day dep_time sched_de…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier #\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt;  #\u0026gt; 1 2013 12 4 1910 1930 -20 2101 2130 -29 MQ  #\u0026gt; 2 2013 12 7 1243 1259 -16 1434 1454 -20 MQ  #\u0026gt; 3 2013 12 18 1715 1730 -15 1919 1925 -6 MQ  #\u0026gt; 4 2013 12 26 1918 1930 -12 2116 2130 -14 MQ  #\u0026gt; 5 2013 12 27 1918 1930 -12 2057 2130 -33 MQ  #\u0026gt; 6 2013 12 10 1504 1515 -11 1731 1720 11 MQ  #\u0026gt; 7 2013 12 12 1719 1730 -11 1937 1925 12 MQ  #\u0026gt; 8 2013 12 16 1919 1930 -11 2108 2130 -22 MQ  #\u0026gt; 9 2013 12 31 1444 1455 -11 1637 1650 -13 MQ  #\u0026gt; 10 2013 12 12 1125 1135 -10 1321 1330 -9 MQ  #\u0026gt; # … with 3,514 more rows, 9 more variables: flight \u0026lt;int\u0026gt;, tailnum \u0026lt;chr\u0026gt;, #\u0026gt; # origin \u0026lt;chr\u0026gt;, dest \u0026lt;chr\u0026gt;, air_time \u0026lt;dbl\u0026gt;, distance \u0026lt;dbl\u0026gt;, hour \u0026lt;dbl\u0026gt;, #\u0026gt; # minute \u0026lt;dbl\u0026gt;, time_hour \u0026lt;dttm\u0026gt;, and abbreviated variable names #\u0026gt; # ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay  Break out exercises: Again, see here: S04E08: R for Data Science - Chapter 5.1 - 5.3\nat the bottom of the page, under III \u0026ndash; Chapter 5.3: arrange().\nselect() The previous data manipulation functions we\u0026rsquo;ve looked at, filter() and arrange(), work on observations (i.e. rows).\nThe next function we\u0026rsquo;ll look at, select(), works on directly on variables (i.e. columns).\nIf you have hundreds of columns in a data frame (many of which you may not be interested in for the current analysis) you can subset the columns. We saw above that flights has 19 columns. This is a serious example in terms of rows (336,776), but pretty trivial in terms of columns. But it\u0026rsquo;s still a good toy example.\nIt will be useful to import this example data frame into our local Environment so we can visualize it in RStudio (at the moment it is just \u0026lsquo;floating\u0026rsquo; out there in the system: we can access it, but we can\u0026rsquo;t see it).\nSo after:\nlibrary(nycflights13)  We want to do:\nmy_flights \u0026lt;- flights  (We\u0026rsquo;ve just created a personal \u0026lsquo;local object\u0026rsquo; data frame in our current session, pulling in data from the nycflights13 package). And it should appear in the Environment tab in RStudio.\nSo, how can we focus on a subset of variables in the flights data frame? One way is to explicitly name the columns you want to keep using the select() function (remember, the first argument is the data frame; the following arguments are the column names):\nselect(flights, year, month, day)#\u0026gt; # A tibble: 336,776 × 3 #\u0026gt; year month day #\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 2013 1 1 #\u0026gt; 2 2013 1 1 #\u0026gt; 3 2013 1 1 #\u0026gt; 4 2013 1 1 #\u0026gt; 5 2013 1 1 #\u0026gt; 6 2013 1 1 #\u0026gt; 7 2013 1 1 #\u0026gt; 8 2013 1 1 #\u0026gt; 9 2013 1 1 #\u0026gt; 10 2013 1 1 #\u0026gt; # … with 336,766 more rows  Often this is fine, but as you move on to larger data sets, you might want to use various selection features instead of just explictly listing want you want to keep.\nThese are listed if you do:\n?select  These use the same syntax and semantics as filter() for rows, using the same logical combinations.\nSelection features : \u0026lsquo;range\u0026rsquo; You can select a range of columns using the : range operator. This is really only efficient if the original data frame is organized in a way that is useful for your purposes. The good news is that don\u0026rsquo;t have to select all varibles explicitly, one by one. The range operator selects consecutive variables in the data frame.\nselect(flights, year:day)#\u0026gt; # A tibble: 336,776 × 3 #\u0026gt; year month day #\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 2013 1 1 #\u0026gt; 2 2013 1 1 #\u0026gt; 3 2013 1 1 #\u0026gt; 4 2013 1 1 #\u0026gt; 5 2013 1 1 #\u0026gt; 6 2013 1 1 #\u0026gt; 7 2013 1 1 #\u0026gt; 8 2013 1 1 #\u0026gt; 9 2013 1 1 #\u0026gt; 10 2013 1 1 #\u0026gt; # … with 336,766 more rows  Notice that the month variable is automatically included, even though it\u0026rsquo;s not mentioned in the select statement.\nBut there is more organization in this data frame. Say we wanted to drill down just into the departure and arrival times:\nselect(flights, dep_time:arr_delay)#\u0026gt; # A tibble: 336,776 × 6 #\u0026gt; dep_time sched_dep_time dep_delay arr_time sched_arr_time arr_delay #\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 517 515 2 830 819 11 #\u0026gt; 2 533 529 4 850 830 20 #\u0026gt; 3 542 540 2 923 850 33 #\u0026gt; 4 544 545 -1 1004 1022 -18 #\u0026gt; 5 554 600 -6 812 837 -25 #\u0026gt; 6 554 558 -4 740 728 12 #\u0026gt; 7 555 600 -5 913 854 19 #\u0026gt; 8 557 600 -3 709 723 -14 #\u0026gt; 9 557 600 -3 838 846 -8 #\u0026gt; 10 558 600 -2 753 745 8 #\u0026gt; # … with 336,766 more rows  ! \u0026lsquo;complement\u0026rsquo; You can also drop columns (and ranges of columns) using the logical complement sign:\nselect(flights, !(year:day))#\u0026gt; # A tibble: 336,776 × 16 #\u0026gt; dep_t…¹ sched…² dep_d…³ arr_t…⁴ sched…⁵ arr_d…⁶ carrier flight tailnum origin #\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt;  #\u0026gt; 1 517 515 2 830 819 11 UA 1545 N14228 EWR  #\u0026gt; 2 533 529 4 850 830 20 UA 1714 N24211 LGA  #\u0026gt; 3 542 540 2 923 850 33 AA 1141 N619AA JFK  #\u0026gt; 4 544 545 -1 1004 1022 -18 B6 725 N804JB JFK  #\u0026gt; 5 554 600 -6 812 837 -25 DL 461 N668DN LGA  #\u0026gt; 6 554 558 -4 740 728 12 UA 1696 N39463 EWR  #\u0026gt; 7 555 600 -5 913 854 19 B6 507 N516JB EWR  #\u0026gt; 8 557 600 -3 709 723 -14 EV 5708 N829AS LGA  #\u0026gt; 9 557 600 -3 838 846 -8 B6 79 N593JB JFK  #\u0026gt; 10 558 600 -2 753 745 8 AA 301 N3ALAA LGA  #\u0026gt; # … with 336,766 more rows, 6 more variables: dest \u0026lt;chr\u0026gt;, air_time \u0026lt;dbl\u0026gt;, #\u0026gt; # distance \u0026lt;dbl\u0026gt;, hour \u0026lt;dbl\u0026gt;, minute \u0026lt;dbl\u0026gt;, time_hour \u0026lt;dttm\u0026gt;, and abbreviated #\u0026gt; # variable names ¹​dep_time, ²​sched_dep_time, ³​dep_delay, ⁴​arr_time, #\u0026gt; # ⁵​sched_arr_time, ⁶​arr_delay  Now we just have 16 columns, as opposed to the original 19.\nJust a note. In the text we are using, the syntax is:\nselect(flights, -(year:day)) But based on this:\nselect.R\nI get the sense that ! is the current recommendation for taking the complement in select() statements and that - is deprecated. And FYI: documentation always lags behind implementation.\nWe can do exactly the same thing by explicitly listing the columns we want to drop, but, there is a gotcha here.\nThis (which was my first guess) does not work:\nselect(flights, !(year, month, day) Instead, we need to wrap the dropped columns in a c() vector:\nselect(flights, !c(year, month, day)) #\u0026gt; # A tibble: 336,776 × 16 #\u0026gt; dep_t…¹ sched…² dep_d…³ arr_t…⁴ sched…⁵ arr_d…⁶ carrier flight tailnum origin #\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt;  #\u0026gt; 1 517 515 2 830 819 11 UA 1545 N14228 EWR  #\u0026gt; 2 533 529 4 850 830 20 UA 1714 N24211 LGA  #\u0026gt; 3 542 540 2 923 850 33 AA 1141 N619AA JFK  #\u0026gt; 4 544 545 -1 1004 1022 -18 B6 725 N804JB JFK  #\u0026gt; 5 554 600 -6 812 837 -25 DL 461 N668DN LGA  #\u0026gt; 6 554 558 -4 740 728 12 UA 1696 N39463 EWR  #\u0026gt; 7 555 600 -5 913 854 19 B6 507 N516JB EWR  #\u0026gt; 8 557 600 -3 709 723 -14 EV 5708 N829AS LGA  #\u0026gt; 9 557 600 -3 838 846 -8 B6 79 N593JB JFK  #\u0026gt; 10 558 600 -2 753 745 8 AA 301 N3ALAA LGA  #\u0026gt; # … with 336,766 more rows, 6 more variables: dest \u0026lt;chr\u0026gt;, air_time \u0026lt;dbl\u0026gt;, #\u0026gt; # distance \u0026lt;dbl\u0026gt;, hour \u0026lt;dbl\u0026gt;, minute \u0026lt;dbl\u0026gt;, time_hour \u0026lt;dttm\u0026gt;, and abbreviated #\u0026gt; # variable names ¹​dep_time, ²​sched_dep_time, ³​dep_delay, ⁴​arr_time, #\u0026gt; # ⁵​sched_arr_time, ⁶​arr_delay  This is the syntax for combining selections in the current dplyr package.\nLet\u0026rsquo;s check if it works on positive selections:\nselect(flights, c(year, month, day))#\u0026gt; # A tibble: 336,776 × 3 #\u0026gt; year month day #\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 2013 1 1 #\u0026gt; 2 2013 1 1 #\u0026gt; 3 2013 1 1 #\u0026gt; 4 2013 1 1 #\u0026gt; 5 2013 1 1 #\u0026gt; 6 2013 1 1 #\u0026gt; 7 2013 1 1 #\u0026gt; 8 2013 1 1 #\u0026gt; 9 2013 1 1 #\u0026gt; 10 2013 1 1 #\u0026gt; # … with 336,766 more rows  Yep, all the same.\nSo it seems it\u0026rsquo;s optional for positive selections, but necessary for negative selections.\nSelection helpers There are other ways to select columns efficiently without explicitly naming them.\nHere are some examples of helpers which select variables by pattern-matching over the names. Note that the search term must be wrapped in quotes (since we are searching on text in the column name):\nflights %\u0026gt;%   select(ends_with(\"time\"))#\u0026gt; # A tibble: 336,776 × 5 #\u0026gt; dep_time sched_dep_time arr_time sched_arr_time air_time #\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 517 515 830 819 227 #\u0026gt; 2 533 529 850 830 227 #\u0026gt; 3 542 540 923 850 160 #\u0026gt; 4 544 545 1004 1022 183 #\u0026gt; 5 554 600 812 837 116 #\u0026gt; 6 554 558 740 728 150 #\u0026gt; 7 555 600 913 854 158 #\u0026gt; 8 557 600 709 723 53 #\u0026gt; 9 557 600 838 846 140 #\u0026gt; 10 558 600 753 745 138 #\u0026gt; # … with 336,766 more rows  flights %\u0026gt;%   select(starts_with(\"time\"))#\u0026gt; # A tibble: 336,776 × 1 #\u0026gt; time_hour  #\u0026gt; \u0026lt;dttm\u0026gt;  #\u0026gt; 1 2013-01-01 05:00:00 #\u0026gt; 2 2013-01-01 05:00:00 #\u0026gt; 3 2013-01-01 05:00:00 #\u0026gt; 4 2013-01-01 05:00:00 #\u0026gt; 5 2013-01-01 06:00:00 #\u0026gt; 6 2013-01-01 05:00:00 #\u0026gt; 7 2013-01-01 06:00:00 #\u0026gt; 8 2013-01-01 06:00:00 #\u0026gt; 9 2013-01-01 06:00:00 #\u0026gt; 10 2013-01-01 06:00:00 #\u0026gt; # … with 336,766 more rows  We can combine these two helpers (you can also think of them as \u0026lsquo;filters\u0026rsquo; or \u0026lsquo;constraints\u0026rsquo; to make contact with other programming languages) using logical operators. Here is an \u0026lsquo;OR\u0026rsquo; statement using the | syntax. It means the selection returns all selections that match either columns that end with \u0026ldquo;time\u0026rdquo; or start with \u0026ldquo;time\u0026rdquo;.\nflights %\u0026gt;%   select(ends_with(\"time\") | starts_with(\"time\"))#\u0026gt; # A tibble: 336,776 × 6 #\u0026gt; dep_time sched_dep_time arr_time sched_arr_time air_time time_hour  #\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dttm\u0026gt;  #\u0026gt; 1 517 515 830 819 227 2013-01-01 05:00:00 #\u0026gt; 2 533 529 850 830 227 2013-01-01 05:00:00 #\u0026gt; 3 542 540 923 850 160 2013-01-01 05:00:00 #\u0026gt; 4 544 545 1004 1022 183 2013-01-01 05:00:00 #\u0026gt; 5 554 600 812 837 116 2013-01-01 06:00:00 #\u0026gt; 6 554 558 740 728 150 2013-01-01 05:00:00 #\u0026gt; 7 555 600 913 854 158 2013-01-01 06:00:00 #\u0026gt; 8 557 600 709 723 53 2013-01-01 06:00:00 #\u0026gt; 9 557 600 838 846 140 2013-01-01 06:00:00 #\u0026gt; 10 558 600 753 745 138 2013-01-01 06:00:00 #\u0026gt; # … with 336,766 more rows  In this particular case there is a more compact way to get the same result, using the contains() helper. But this solution has lower resolution, since \u0026ldquo;time\u0026rdquo; could be anywhere in the column name.\nflights %\u0026gt;%   select(contains(\"time\"))#\u0026gt; # A tibble: 336,776 × 6 #\u0026gt; dep_time sched_dep_time arr_time sched_arr_time air_time time_hour  #\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dttm\u0026gt;  #\u0026gt; 1 517 515 830 819 227 2013-01-01 05:00:00 #\u0026gt; 2 533 529 850 830 227 2013-01-01 05:00:00 #\u0026gt; 3 542 540 923 850 160 2013-01-01 05:00:00 #\u0026gt; 4 544 545 1004 1022 183 2013-01-01 05:00:00 #\u0026gt; 5 554 600 812 837 116 2013-01-01 06:00:00 #\u0026gt; 6 554 558 740 728 150 2013-01-01 05:00:00 #\u0026gt; 7 555 600 913 854 158 2013-01-01 06:00:00 #\u0026gt; 8 557 600 709 723 53 2013-01-01 06:00:00 #\u0026gt; 9 557 600 838 846 140 2013-01-01 06:00:00 #\u0026gt; 10 558 600 753 745 138 2013-01-01 06:00:00 #\u0026gt; # … with 336,766 more rows  But you get the idea. The usefulness of these \u0026lsquo;selection helpers\u0026rsquo; depends on the column naming conventions you create (or most likely inherit) from a colleague or online.\nBreak out exercises: See 5.4.1 in the text.\nBut just state with these:\nCreate a couple of select queries which pull out dep_time, dep_delay, arr_time, and arr_delay from flights, just using what we learned above.\nOne should be explicit, and others (at least slightly!) more efficient.\nIf you get through these with no issues, feel free to explore more of the execises in 5.4.1.\n","date":1665619200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666892689,"objectID":"4e67ef2061adce34ad15b7cfcc56388e","permalink":"https://biodash.github.io/codeclub/s04e10-11_r4ds-ch5-4/","publishdate":"2022-10-13T00:00:00Z","relpermalink":"/codeclub/s04e10-11_r4ds-ch5-4/","section":"codeclub","summary":"In these two sessions of Code Club, we look at sorting dataframes with arrange() and filtering rows of a dataframe based on certain conditions with filter().","tags":["codeclub","r4ds"],"title":"S04E10 and S04E11: R for Data Science - Chapters 5.3 and 5.4","type":"codeclub"},{"authors":["Jelmer Poelstra"],"categories":null,"content":"  Artwork by Allison Horst    I \u0026ndash; Chapter 5.1: Introduction Key points   Function name conflicts: The function filter() in the stats package (which is loaded by default in R) will be \u0026ldquo;masked\u0026rdquo; / \u0026ldquo;overwritten\u0026rdquo; by dplyr\u0026rsquo;s filter() function when you load the tidyverse. To still use a masked function (or a function from an installed-but-not-loaded package!), use the \u0026ldquo;full\u0026rdquo; notation, e.g. stats::filter().\n  A data frame is rectangular data structure (with rows and columns), while a \u0026ldquo;tibble\u0026rdquo; is a tidyverse-style data frame. Tibbles mainly differ from regular data frames in how they are printed to screen by default. See the two examples below: cars is a regular data frame and flights is a tibble.\n  The most common R data types are integers (tibble abbreviation: int), doubles (dbl), character strings (chr), logicals (lgl), and factors (fctr).\n  The dplyr package is designed to work with dataframes: both the input and the output is a dataframe.\n  # 'mtcars' is a regular dataframe head(mtcars)  #\u0026gt; mpg cyl disp hp drat wt qsec vs am gear carb #\u0026gt; Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 #\u0026gt; Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 #\u0026gt; Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 #\u0026gt; Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 #\u0026gt; Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 #\u0026gt; Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1  # 'flights' is a tibble, which affects its printing behavior library(nycflights13) head(flights)  #\u0026gt; # A tibble: 6 × 19 #\u0026gt; year month day dep_time sched_dep…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier #\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt;  #\u0026gt; 1 2013 1 1 517 515 2 830 819 11 UA  #\u0026gt; 2 2013 1 1 533 529 4 850 830 20 UA  #\u0026gt; 3 2013 1 1 542 540 2 923 850 33 AA  #\u0026gt; 4 2013 1 1 544 545 -1 1004 1022 -18 B6  #\u0026gt; 5 2013 1 1 554 600 -6 812 837 -25 DL  #\u0026gt; 6 2013 1 1 554 558 -4 740 728 12 UA  #\u0026gt; # … with 9 more variables: flight \u0026lt;int\u0026gt;, tailnum \u0026lt;chr\u0026gt;, origin \u0026lt;chr\u0026gt;, #\u0026gt; # dest \u0026lt;chr\u0026gt;, air_time \u0026lt;dbl\u0026gt;, distance \u0026lt;dbl\u0026gt;, hour \u0026lt;dbl\u0026gt;, minute \u0026lt;dbl\u0026gt;, #\u0026gt; # time_hour \u0026lt;dttm\u0026gt;, and abbreviated variable names ¹​sched_dep_time, #\u0026gt; # ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay   II \u0026ndash; Chapter 5.2: filter() Key points   The filter() function removes rows (observations) from a dataframe based on certain conditions. You specify those conditions for one or more columns.\n  When you refer to a column, don\u0026rsquo;t quote its name! (e.g. in filter(flights, month == 1), where month is the column name.)\n  Use \u0026ldquo;comparison operators\u0026rdquo; like \u0026gt; (greater than) to specify conditions. Note that two equals signs == (and not a single, =) signifies equality, and that != means \u0026ldquo;does not equal\u0026rdquo;.\n  To combine multiple conditions, use logical (Boolean) operators: \u0026amp; (and), | (or), and ! (not). Separating conditions by a comma also means \u0026ldquo;and\u0026rdquo; in dplyr, e.g. in filter(flights, month == 1, day == 1).\n  The %in% operator tests if the value(s) on the left-hand side are contained in the values on the right hand side, e.g. 4 %in% 1:5 asks whether 4 is contained in the sequence of numbers from 1 to 5, which will return TRUE.\n  Missing values are denoted by NA, and almost any operation with an NA will return another NA. To test if x is or contains NAs, don\u0026rsquo;t use x == NA but is.na(x). When you filter based on a column, rows with NAs in that column will by default be removed by filter().\n   Exercise 1 Find all flights that\u0026hellip;\n Had an arrival delay of two or more hours Flew to Houston (IAH or HOU) Were operated by United (UA), American (AA), or Delta (DL) Departed in summer (July, August, and September) Arrived more than two hours late, but didn\u0026rsquo;t leave late Were delayed by at least an hour, but made up over 30 minutes in flight Departed between midnight and 6am (inclusive)  Before you start, load the necessary packages:\nlibrary(nycflights13) library(tidyverse)    Hints (click here)    Delays are given in minutes.\n  Times of day are numbered from 0001 (1 minute past midnight) to 2400 (midnight).\n     Solution (click here)  In the solutions below, I am piping the output to nrow(), so you can check if you got the same number of rows.\n\\1. Had an arrival delay (=\u0026gt; arr_delay) of two or more hours (=\u0026gt; \u0026gt;= 120):\nfilter(flights,  arr_delay \u0026gt;= 120) %\u0026gt;%  nrow()  #\u0026gt; [1] 10200  \\2. Flew to Houston (IAH or HOU) \u0026ndash; destination is the dest column:\nfilter(flights,  dest %in% c(\"IAH\", \"HOU\")) %\u0026gt;%  nrow()  #\u0026gt; [1] 9313  \\3. Were operated by United (UA), American (AA), or Delta (DL) \u0026mdash; this information is in the carrier column:\nfilter(flights,  carrier %in% c(\"UA\", \"AA\", \"DL\")) %\u0026gt;%  nrow()  #\u0026gt; [1] 139504  \\4. Departed in summer (July, August, and September) \u0026mdash; use the month column:\nfilter(flights,  month %in% 7:9) %\u0026gt;%  nrow()  #\u0026gt; [1] 86326  This would also work:\nfilter(flights,  month \u0026gt;= 7, month \u0026lt;= 9) %\u0026gt;%  nrow()  \\5. Arrived more than two hours late, but didn\u0026rsquo;t leave late \u0026mdash; use the arr_delay (arrival delay) and dep_delay (departure delay) columns:\nfilter(flights,  arr_delay \u0026gt; 120, dep_delay \u0026lt;= 0) %\u0026gt;%  nrow()  #\u0026gt; [1] 29  \\6. Were delayed by at least an hour, but made up over 30 minutes in flight \u0026mdash; use the dep_delay and arr_delay columns, and note that \u0026ldquo;making up over 30 miniutes\u0026rdquo; implies that the arrival delay was more than 30 minutes smaller than the departure delay:\nfilter(flights,  dep_delay \u0026gt;= 60, dep_delay - arr_delay \u0026gt; 30) %\u0026gt;%  nrow()  #\u0026gt; [1] 1844  \\7. Departed between midnight and 6am (inclusive) \u0026mdash; use the dep_time column and note that 2400 is midnight:\nfilter(flights,  dep_time \u0026lt;= 600 | dep_time == 2400) %\u0026gt;%  nrow()  #\u0026gt; [1] 9373      Exercise 3 How many flights have a missing dep_time? What other variables are missing for these flights? What might these rows represent?\n  Hints (click here)   A \u0026ldquo;missing\u0026rdquo; dep_time means that this cell contains the value NA. Recall that you can test if something is NA with the is.na() function! To count the number of flights, you can look at the information printed along with the dataframe (... with X more rows), or pipe (%\u0026gt;%) the dataframe into the nrow() function, which counts the number of rows.     Solution (click here)   How many flights have a missing dep_time?  filter(flights,  is.na(dep_time)) %\u0026gt;%  nrow()  #\u0026gt; [1] 8255    What other variables are missing for these flights?\nFor example, arrival times.\n  What might these rows represent?\nThese are cancelled flights.\n     \n","date":1665014400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666892689,"objectID":"903db28b106eaa48155141e76135810e","permalink":"https://biodash.github.io/codeclub/s04e09_r4ds-ch5-1/","publishdate":"2022-10-06T00:00:00Z","relpermalink":"/codeclub/s04e09_r4ds-ch5-1/","section":"codeclub","summary":"This chapter covers the manipulation of rectangular data (data frames, think data from spreadsheets) with the dplyr package that is part of the tidyverse. We'll learn about data frames \u0026 tibbles, R variable types, comparison and logical operators, and missing values (NAs) in addition to the first of the core dplyr functions: filter()","tags":["codeclub","r4ds"],"title":"S04E09: R for Data Science - Chapter 5.1 - 5.2","type":"codeclub"},{"authors":["Jessica Cooperstone"],"categories":null,"content":" I \u0026ndash; Brief Code Club Introduction Organizers  Michael Broe \u0026ndash; Evolution, Ecology and Organismal Biology (EEOB) Jessica Cooperstone \u0026ndash; Horticulture \u0026amp; Crop Science (HCS) / Food Science \u0026amp; Technology (FST) Stephen Opiyo \u0026ndash; Molecular \u0026amp; Cellular Imaging Center (MCIC) - Columbus Jelmer Poelstra \u0026ndash; Molecular \u0026amp; Cellular Imaging Center (MCIC) - Wooster Mike Sovic \u0026ndash; Infectious Diseases Institute AMSL - Genomics Lab  Code Club practicalities   In-person (Columbus \u0026amp; Wooster) and Zoom hybrid\n  Mix of instruction/discussion with the entire group, and doing exercises in breakout groups of up to 4-5 people.\n  When doing exercises in breakout groups, we encourage you:\n To briefly introduce yourselves and to do the exercises as a group On Zoom, to turn your cameras on and to have someone share their screen (use the Ask for help button in Zoom to get help from an organizer) To let a less experienced person do the screen sharing and coding    You can ask a question at any time, by speaking or typing in the Zoom chat.\n  You can generally come early or stay late for troubleshooting but also for questions related to your research.\n  Some more notes:\n  We recommend that you read the relevant (part of the) chapter before each session, especially if the material in the chapter is new to you.\n  We try to make each session as stand-alone as possible. Still, if you missed one or more sessions, you would ideally catch up on reading those parts of the book, especially when we split a chapter across multiple sessions.\n  We record the whole-group parts of the Zoom call, and share the recordings only with Code Club participants.\n  New to Code Club or R? Take a look at these pages on our website:\n Computer setup for Code Club Resources and tips to get started with R List of all previous Code Club session topics   II \u0026ndash; The R for Data Science book (R4DS) This excellent book by Hadley Wickham (also author of many of the R packages used in the book!) and Garret Grolemund, has a freely available online version that is regularly updated and contains exercises. It was originally published in 2016.\nThe book focuses on the so-called \u0026quot;tidyverse\u0026quot; ecosystem in R. The tidyverse can be seen as a modern dialect of R. In previous Code Clubs, we have often \u0026ndash;but not always!\u0026ndash; been doing things \u0026ldquo;the tidyverse way\u0026rdquo; as well.\nFor today\u0026rsquo;s chapters, The R4DS exercises I think are not so good, so I\u0026rsquo;ve replaced some and added some of my own.\n III \u0026ndash; R4DS Chapter 4 In the first two R4DS exercises for this chapter, the message is that R does not handle typos so make sure you spell things correctly.\nExercise 3: take a look at the RStudio keyboard shortcuts by clicking Tools \u0026gt; Keyboard Shortcut Help, or you can press Alt + Shift + K on a PC.\n IV \u0026ndash; R4DS Chapter 6  Run the following code:\nglimpse(cars)  What went wrong?\n  Solution (click here)  glimpse() is a function from the dplyr package, one of the core tidyverse packages that are loaded as part of the tidyverse.\nHowever, in every R session in which you want to use tidyverse function, you always need call library(tidyverse).\nlibrary(tidyverse)  Now you can use glimpse():\nglimpse(cars)#\u0026gt; Rows: 50 #\u0026gt; Columns: 2 #\u0026gt; $ speed \u0026lt;dbl\u0026gt; 4, 4, 7, 7, 8, 9, 10, 10, 10, 11, 11, 12, 12, 12, 12, 13, 13, 13… #\u0026gt; $ dist \u0026lt;dbl\u0026gt; 2, 10, 4, 22, 16, 10, 18, 26, 34, 17, 28, 14, 20, 24, 28, 26, 34…  Note, if you got an error like this when running library(tidyverse):\n#\u0026gt; Error in library(tidyverse) : there is no package called ‘tidyverse’  \u0026hellip;that means you still need to install it:\ninstall.packages(\"tidyverse\") library(tidyverse)      IV \u0026ndash; R4DS Chapter 8  Create an RStudio Project for Code Club.\n   Run the code below in your new Project:\nlibrary(tidyverse)  ggplot(diamonds, aes(carat, price)) +   geom_hex()  ggsave(\"diamonds.pdf\")  write_csv(diamonds, \"diamonds.csv\")    What does the code above do?\n  Find the files diamonds.pdf and diamonds.csv on your computer, without using a search function. How did you know where to look for them?\n  Where is the R working directory on your computer?\n    Solution (click here)    The code does the following:\n Load the tidyverse package Create a simple plot using the tidyverse diamonds dataset Save the plot to disk as a PDF file Save the diamonds dataframe to disk as a CSV file    The files were saved in the same folder as your newly created RStudio project. (See also the next point.)\n  Whenever you have an active RStudio Project, R\u0026rsquo;s working directory will be in the same folder as your RStudio project.\n     \n","date":1663200000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1663258299,"objectID":"81e9e675960c41a86ba5861925da13a8","permalink":"https://biodash.github.io/codeclub/s04e08_r4ds-ch4-6-8/","publishdate":"2022-09-15T00:00:00Z","relpermalink":"/codeclub/s04e08_r4ds-ch4-6-8/","section":"codeclub","summary":"In this first session of Code Club for Fall '22, we will continue working our way through the book R for Data Science. Today, we'll look at three very short chapters on some R and RStudio basics.","tags":["codeclub","r4ds"],"title":"S04E08: R for Data Science - Chapters 4, 6, and 8","type":"codeclub"},{"authors":["Jelmer Poelstra"],"categories":null,"content":" I \u0026ndash; Intro to this season of Code Club Organizers  Michael Broe \u0026ndash; Evolution, Ecology and Organismal Biology (EEOB) Jessica Cooperstone \u0026ndash; Horticulture \u0026amp; Crop Science (HCS) / Food Science \u0026amp; Technology (FST) Stephen Opiyo \u0026ndash; Molecular \u0026amp; Cellular Imaging Center (MCIC) - Columbus Jelmer Poelstra \u0026ndash; Molecular \u0026amp; Cellular Imaging Center (MCIC) - Wooster Mike Sovic \u0026ndash; Center for Applied Plant Sciences (CAPS)  Code Club practicalities   In-person (Columbus \u0026amp; Wooster) and Zoom hybrid\n  Mix of instruction/discussion with the entire group, and doing exercises in breakout groups of up to 4-5 people.\n  When doing exercises in breakout groups, we encourage you:\n To briefly introduce yourselves and to do the exercises as a group On Zoom, to turn your cameras on and to have someone share their screen (use the Ask for help button in Zoom to get help from an organizer) To let a less experienced person do the screen sharing and coding    You can ask a question at any time, by speaking or typing in the Zoom chat.\n  You can generally come early or stay late for troubleshooting but also for questions related to your research.\n  More general notes:\n  We recommend that you read the relevant (part of the) chapter before each session, especially if the material in the chapter is new to you.\n  We try to make each session as stand-alone as possible. Still, if you missed one or more sessions, you would ideally catch up on reading those parts of the book, especially when we split a chapter across multiple sessions.\n  We record the whole-group parts of the Zoom call, and share the recordings only with Code Club participants.\n  We\u0026rsquo;re always hoping for someone outside of the group of organizers to lead a session \u0026ndash; this might be more feasible now that we\u0026rsquo;re going through a book?\n  New to Code Club or R? Take a look at these pages on our website:\n Computer setup for Code Club Resources and tips to get started with R List of all previous Code Club session topics   II \u0026ndash; The R for Data Science book (R4DS) This excellent book by Hadley Wickham (also author of many of the R packages used in the book!) and Garret Grolemund, has a freely available online version that is regularly updated and contains exercises. It was originally published in 2016.\nThe book focuses on the so-called \u0026quot;tidyverse\u0026quot; ecosystem in R. The tidyverse can be seen as a modern dialect of R. Most of its functionality is also contained in \u0026ldquo;base R\u0026rdquo; (that which comes shipped with R by default), but it has an improved and more consistent programming interface or \u0026ldquo;syntax\u0026rdquo;. In previous Code Clubs, we have often \u0026ndash;but not always!\u0026ndash; been doing things \u0026ldquo;the tidyverse way\u0026rdquo; as well.\nThe book doesn\u0026rsquo;t technically assume any previous experience with R, but if you\u0026rsquo;re completely new to R and to coding in any language, we would recommend you take a look at some introductory R material (see this page for some resources) before we start with Chapter 2 next week.\nWe will not be able to finish the book by the end of the summer. But if folks are liking the book, we may carry on with it during the fall semester.\n III \u0026ndash; R4DS Chapter 1 notes 1.1 - What you will learn The data science process visualized:\nWhat is tidy data?   Brief explanation and examples (Click here)  First, you can think of this along the lines of the colloquial meaning of the word: the data is well-organized.\nAdditionally, computer-readability should be prioritized over human-readability (think: no color-coded cells, multiple header columns, or merged cells).\nBut, most of all, \u0026ldquo;tidy\u0026rdquo; in the context of the tidyverse refers to the following, as it is phrased in the book:\n In brief, when your data is tidy, each column is a variable, and each row is an observation.\n But what does this mean? In practice, it often means having your data not in a \u0026ldquo;wide format\u0026rdquo; (all the information about each sample/individual in one row) but in a \u0026ldquo;long format\u0026rdquo; (variables not spread across multiple columns) \u0026ndash; see the examples below.\nExample 1, not tidy:\n#\u0026gt; name quiz1 quiz2 test1 #\u0026gt; 1 Billy C D C #\u0026gt; 2 Suzy F A A #\u0026gt; 3 Lionel B C B  Example 1, tidy:\n#\u0026gt; # A tibble: 9 × 3 #\u0026gt; name assessment grade #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; #\u0026gt; 1 Billy quiz1 C  #\u0026gt; 2 Billy quiz2 D  #\u0026gt; 3 Billy test1 C  #\u0026gt; 4 Lionel quiz1 B  #\u0026gt; 5 Lionel quiz2 C  #\u0026gt; 6 Lionel test1 B  #\u0026gt; 7 Suzy quiz1 F  #\u0026gt; 8 Suzy quiz2 A  #\u0026gt; 9 Suzy test1 A  Example 2, not tidy (in matrix form):\n#\u0026gt; gene1 gene2 gene3 gene4 gene5 #\u0026gt; sample1 48 53 42 50 52 #\u0026gt; sample2 39 43 37 64 47 #\u0026gt; sample3 45 55 51 58 52 #\u0026gt; sample4 42 40 41 64 49 #\u0026gt; sample5 48 49 54 49 43  Example 2, tidy:\n#\u0026gt; # A tibble: 25 × 3 #\u0026gt; sample gene count #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 sample1 gene1 48 #\u0026gt; 2 sample1 gene2 53 #\u0026gt; 3 sample1 gene3 42 #\u0026gt; 4 sample1 gene4 50 #\u0026gt; 5 sample1 gene5 52 #\u0026gt; 6 sample2 gene1 39 #\u0026gt; 7 sample2 gene2 43 #\u0026gt; 8 sample2 gene3 37 #\u0026gt; 9 sample2 gene4 64 #\u0026gt; 10 sample2 gene5 47 #\u0026gt; # … with 15 more rows  The tidyverse is, as the name suggests, generally designed to work with data that is \u0026ldquo;tidy\u0026rdquo; as shown above. With ggplot2, in particular, you\u0026rsquo;ll quickly run into difficulties when trying to make plots using wide-format dataframes.\nFor more, the book has a separate chapter on tidy data, and there is also tidyr package explainer on tidy data.\n 1.3 - What you won\u0026rsquo;t learn Some perhaps unfamiliar terms from this section:\n1.3.1 - Processing big data  Fortunately each problem is independent of the others (a setup that is sometimes called embarrassingly parallel), so you just need a system (like Hadoop or Spark) that allows you to send different datasets to different computers for processing.\n At OSU, and most other universities, we instead tend to make use of \u0026ldquo;supercomputers\u0026rdquo; when we want to simultaneously run an analysis many times, and more broadly, if we have \u0026ldquo;big data\u0026rdquo;. Specifically, we have the \u0026ldquo;Ohio Supercomputer Center\u0026rdquo; (OSC) here.\n1.3.3 - Non-rectangular data Rectangular data is basically data that can be effectively entered in a spreadsheet (and in R, we tend to put this in a so-called \u0026ldquo;dataframe\u0026rdquo; or \u0026ldquo;tibble\u0026rdquo;). The tidyverse is highly dataframe-oriented, so it makes sense that the book focuses on rectangular data.\n1.4.2 - RStudio interface R itself simply provides a \u0026ldquo;console\u0026rdquo; (command-line interface) where you can type your commands. RStudio, on the other hand, allows you to see the R console side-by-side with your scripts, plots, and more.\nOnce you have a running instance of RStudio, create a new R script by clicking File \u0026gt; New File \u0026gt; R Script. Now, you should see all 4 \u0026ldquo;panes\u0026rdquo; that the RStudio window is divided into:\n Top-left: The Editor for your scripts and other documents (hidden when no file is open). Bottom-left: The R Console to interactively run your code (+ other tabs). Top-right: Your Environment with R objects you have created (+ other tabs). Bottom-right: Tabs for Files, Plots, Help, and others.   Check that you have R and RStudio working Take a moment to explore the RStudio interface. Were you able to open a new file to get the Editor pane?\nTake a look at your version of R: this was printed in the console when you started RStudio (see the RStudio screenshot above).\nThe current version of R is 4.2.0. If your version of R is below 4.0, it will be a good idea to update R. To do so, you can follow these instructions. But it is better to start this process at the very end of this session or after it, since it may take a while.\n  1.4.3 \u0026amp; 1.4.4 - R packages  An R package is a collection of functions, data, and documentation that extends the capabilities of base R.\n So, you can think of packages as \u0026ldquo;add-ons\u0026rdquo; / \u0026ldquo;extensions\u0026rdquo; to base R.\nInstallation versus loading Packages have to be separately installed (usually from within R, using R code) and once you have done this, you don\u0026rsquo;t need to redo it unless:\n You want a different version of the package You have switched to a different version of R  Unlike installation, loading a package is necessary again and again, in every R session that you want to use it.\nThe tidyverse The tidyverse is unusual in that it is a collection of packages that can still be installed and loaded with a single command. The individual tidyverse packages are the focus of several chapters in the book, for instance:\n   Package Functionality Main chapter     ggplot2 Creating plots Ch. 3   tidyr \u0026amp; dplyr Manipulating dataframes Ch. 5 \u0026amp; 7   readr Reading in data Ch. 11   stringr Working with \u0026ldquo;strings\u0026rdquo; (text) Ch. 14   forcats Working with \u0026ldquo;factors\u0026rdquo; (categorical variables) Ch. 15   purrr Iteration with functions Ch. 21    Data packages Additionally, the book uses a couple of \u0026ldquo;data packages\u0026rdquo; (packages that only contain data, and no functions): nycflights13, gapminder, and Lahman.\n IV \u0026ndash; Breakout rooms  1. Introduce yourselves! Please take a moment to introduce yourself to your breakout roommates. You may also want to mention:\n  Your level of experience with R (and perhaps other coding languages)\n  What you want to use R for, or what you are already using R for\n  Why you think this book might be useful, if you have an idea already\n     2. Install/load the packages Usually, exercises can be done with your breakout group on one computer, but the following should be done individually, to check that everyone has R up and running.\nMost of you should already have the tidyverse installed, so let\u0026rsquo;s start by trying to load it. This is done with the library() function. To check if you can load the tidyverse, run the following and see if you get similar output as printed below:\nlibrary(tidyverse)  If instead, you got something like:\n#\u0026gt; Error in library(tidyverse) : there is no package called ‘tidyverse’  \u0026hellip;that means you still need to install it:\n## Note: the package name is \"quoted\" in the install.packages() function: install.packages(\"tidyverse\") ## ... but it is not (normally) in library(): library(tidyverse)  Now, let\u0026rsquo;s also install the data packages \u0026ndash; we can do that all at once:\ninstall.packages(c(\"nycflights13\", \"gapminder\", \"Lahman\"))  The previous installation commands should return a whole bunch of output, but if all went well, you should not see any errors. Instead, look for phrases like * DONE (nycflights13), which indicate successful installation of a package.\nYou can also load the data packages (we have to do that for each package individually):\nlibrary(nycflights13) library(gapminder) library(Lahman)  You won\u0026rsquo;t see any output when loading most packages, like the three above (but unlike the tidyverse).\nBonus question: What are these \u0026ldquo;conflicts\u0026rdquo; in the tidyverse startup messages referring to?\n   3. Bonus: Questions or remarks about the chapter? Discuss or ask about whatever you thought was interesting/confusing/etc about the chapter!\nIf nothing else comes up, you could think about and discuss the following:\n  1.3.2: \u0026ldquo;Data science teams\u0026rdquo; \u0026ndash; Are grad students in a lab \u0026ldquo;data science teams\u0026rdquo;, or are they talking about something else? Do you think this might say something about the expected primary audience for the book?\n  1.3.4: \u0026ldquo;Hypothesis generation\u0026rdquo; vs. \u0026ldquo;hypothesis confirmation\u0026rdquo; \u0026ndash; are you familiar with this distinction and do you use it in practice?\n  1.3.2: Other languages commonly used for data analysis: Python and Julia. Are you familiar at all with these languages? Why did you want to learn R instead?\n    \n","date":1654732800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1654799215,"objectID":"6b9f298ea39f46200d830afbf1f34093","permalink":"https://biodash.github.io/codeclub/s04e01_r4ds-ch1/","publishdate":"2022-06-09T00:00:00Z","relpermalink":"/codeclub/s04e01_r4ds-ch1/","section":"codeclub","summary":"We will introduce a new season of Code Club, in which we'll do things a little differently than before: we are going to work our way through a book: R for Data Science. Today, we'll look at the first, introductory chapter of the book.","tags":["codeclub","r4ds"],"title":"S04E01: R for Data Science - Chapter 1","type":"codeclub"},{"authors":["Michael Broe"],"categories":null,"content":"\n New To Code Club?   First, check out the Code Club Computer Setup instructions, which also has some pointers that might be helpful if you\u0026rsquo;re new to R or RStudio.\n  Please open RStudio before Code Club to test things out \u0026ndash; if you run into issues, join the Zoom call early and we\u0026rsquo;ll troubleshoot.\n   Session Goals  Learn how to incorporate your own functions into loops. Learn how to efficiently save the outputs of your loop into a data structure. Learn how using a functional (like purr::map) saves you a lot of housekeeping.   Again we\u0026rsquo;ll be using tibble() from the tidyverse package, so we need to load that first.\nlibrary(tidyverse) #\u0026gt; ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ── #\u0026gt; ✔ ggplot2 3.3.5 ✔ purrr  0.3.4 #\u0026gt; ✔ tibble  3.1.6 ✔ dplyr  1.0.8 #\u0026gt; ✔ tidyr  1.2.0 ✔ stringr 1.4.0 #\u0026gt; ✔ readr  2.1.2 ✔ forcats 0.5.1 #\u0026gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── #\u0026gt; ✖ dplyr::filter() masks stats::filter() #\u0026gt; ✖ dplyr::lag() masks stats::lag()  We\u0026rsquo;ll also reuse the toy data frame from last Code Club:\ndf \u0026lt;- tibble( a = rnorm(10), b = rnorm(10), c = rnorm(10), d = rnorm(10) ) df #\u0026gt; # A tibble: 10 × 4 #\u0026gt; a b c d #\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 -1.57 0.647 1.39 0.851 #\u0026gt; 2 0.239 0.667 -0.108 -1.74  #\u0026gt; 3 -0.520 -0.663 -0.343 0.652 #\u0026gt; 4 -0.0359 -1.69 -1.30 -1.58  #\u0026gt; 5 1.27 0.357 0.158 -1.92  #\u0026gt; 6 -1.04 0.490 0.897 1.33  #\u0026gt; 7 -0.212 -0.753 -1.68 -0.503 #\u0026gt; 8 1.91 0.275 0.646 0.139 #\u0026gt; 9 -0.535 -0.00632 -1.02 -0.467 #\u0026gt; 10 0.223 -0.422 0.616 -0.553  And we\u0026rsquo;ll also be re-using our own normalize function:\nnormalize \u0026lt;- function(x) \u0026#123; rng \u0026lt;- range(x) (x - rng[1]) / (rng[2] - rng[1]) \u0026#125;  Last time we saw how to use this function to simplify our code:\ndf$a \u0026lt;- normalize(df$a) df$b \u0026lt;- normalize(df$b) df$c \u0026lt;- normalize(df$c) df$d \u0026lt;- normalize(df$d)  In previous Code Clubs we\u0026rsquo;ve seen how you can apply a built-in function like mean to each column of a data frame using a for loop, lapply, or map.\nWe can use exactly the same techniques with our own functions.\nBut I think it\u0026rsquo;s worth taking advantage of this time to revisit a couple of details (some of which were in the Bonus Material in S03E08).\nAccessing by value vs. index In our first session on loops, we saw an example like this:\nfor (a_number in c(10, 11, 12, 13)) \u0026#123; # We iterate over 10, 11, 12, 13 print(a_number * -1) \u0026#125; #\u0026gt; [1] -10 #\u0026gt; [1] -11 #\u0026gt; [1] -12 #\u0026gt; [1] -13  Here we are looping over the actual values in the vector. But we can also access the values by their index. Here we loop over an index, and include that index in the body of the loop. It is very common in this usage to use the name i for the variable. This is most common when the vector/list/data frame already exists as an object:\nnumbers \u0026lt;- c(10, 11, 12, 13) # We create a vector for (i in 1:4) \u0026#123; # We iterate over the indexes 1, 2, 3, 4 print(numbers[i] * -1) # We access the value using the index notation `[ ]` \u0026#125; #\u0026gt; [1] -10 #\u0026gt; [1] -11 #\u0026gt; [1] -12 #\u0026gt; [1] -13  Note that here we \u0026lsquo;hard-coded\u0026rsquo; the length of the vector inside the loop. We can generalize this so it will work on vectors of any length by using this syntax:\nnumbers \u0026lt;- c(10, 11, 12, 13) length(numbers) #\u0026gt; [1] 4 for (i in 1:length(numbers)) \u0026#123; # We iterate over 1, 2, 3,... print(numbers[i] * -1) \u0026#125; #\u0026gt; [1] -10 #\u0026gt; [1] -11 #\u0026gt; [1] -12 #\u0026gt; [1] -13  Storing loop outputs We have also seen that unless you issue a print() statement which runs on every separate iteration of the loop, the output values simply \u0026lsquo;go away\u0026rsquo;.\nfor (a_number in c(10, 11, 12, 13)) \u0026#123; a_number * -1 \u0026#125;  Similarly if we want to actually save the output of the loop in a vector, we need to save an output value on every separate iteration of the loop. And this means we have to build the output vector iteration-by-iteration. Here is a first guess how to do this:\noutputs \u0026lt;- vector() # We 'initialize' an *empty vector* to hold the outputs outputs #\u0026gt; logical(0) for (a_number in c(10, 11, 12, 13)) \u0026#123; outputs \u0026lt;- c(outputs, a_number * -1) # Each time round the loop we *append* a new value to the existing vector \u0026#125; outputs #\u0026gt; [1] -10 -11 -12 -13  This looks fine, however, there is a problem. The vector \u0026lsquo;grows\u0026rsquo; at each iteration, and this means that, as Jelmer pointed out in the bonus material on loops, \u0026lsquo;R has to create an entirely new object in each iteration of the loop, because the object\u0026rsquo;s memory requirements keep increasing.\u0026rsquo;\nThis is not an issue for the toy vector we are using here, but say you were using a loop to create a data frame, column by column, with thousands of rows, and hundreds of columns. On every iteration the entire data frame would have to be copied and extended, and copied and extended, and\u0026hellip;\nSo how do we avoid that?\nThe technique is to initialize a vector (or list, or data frame) of the appropriate size for the outputs, which preallocates the memory required to store it. Then instead of appending to it on each iteration, we write into it on each iteration. The size of the output vector is already fixed, and modifying values like this is way more efficient. Again, the magic is is to use indexes.\noutput_vector \u0026lt;- vector(length = 4) output_vector #\u0026gt; [1] FALSE FALSE FALSE FALSE numbers \u0026lt;- c(10, 11, 12, 13) for (i in 1:4) \u0026#123; output_vector[i] \u0026lt;- numbers[i] * -1 \u0026#125; output_vector #\u0026gt; [1] -10 -11 -12 -13  Breakout rooms, storing loop outputs Exercise 1 R has a function letters which returns a character vector:\nletters #\u0026gt; [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\" \"k\" \"l\" \"m\" \"n\" \"o\" \"p\" \"q\" \"r\" \"s\" #\u0026gt; [20] \"t\" \"u\" \"v\" \"w\" \"x\" \"y\" \"z\"  (letters is a bit like iris: it\u0026rsquo;s a character vector which is \u0026lsquo;just there\u0026rsquo;, like iris is a data frame which is \u0026lsquo;just there\u0026rsquo;).\nThe tidyverse also has a function str_to_upper() which converts the case of a character:\nstr_to_upper(\"a\") #\u0026gt; [1] \"A\"  Write a for loop that converts each element of a character vector to upper case, saving the output by writing the output of each iteration into an empty vector.\n  Hints (click here)  What is `letters[1]`?    Solution (click here)  upper_case \u0026lt;- vector(length = 26) for (i in 1:26) \u0026#123; upper_case[i] \u0026lt;- str_to_upper(letters[i]) \u0026#125;    Back to normalize This gives us the machinery to use our own function in a for loop.\nFirst, recall how we can access a column vector using the [[ ]] syntax:\ndf[[1]] #\u0026gt; [1] -1.56695706 0.23880352 -0.52028396 -0.03587572 1.26976225 -1.03948139 #\u0026gt; [7] -0.21172370 1.90549573 -0.53548764 0.22250909  So we can iteratively access each column in a for loop:\nfor (i in 1:4) \u0026#123; print(normalize(df[[i]])) \u0026#125; #\u0026gt; [1] 0.0000000 0.5200245 0.3014218 0.4409221 0.8169209 0.1519029 0.3902813 #\u0026gt; [8] 1.0000000 0.2970435 0.5153320 #\u0026gt; [1] 0.9915841 1.0000000 0.4352500 0.0000000 0.8684414 0.9250426 0.3970564 #\u0026gt; [8] 0.8335988 0.7141351 0.5377301 #\u0026gt; [1] 1.0000000 0.5131949 0.4367290 0.1242323 0.5998479 0.8402709 0.0000000 #\u0026gt; [8] 0.7587425 0.2170914 0.7489915 #\u0026gt; [1] 0.85264136 0.05580354 0.79127366 0.10586772 0.00000000 1.00000000 #\u0026gt; [7] 0.43627787 0.63372198 0.44736338 0.42096304  And again, we can generalize this to a data frame of of any length.\nfor (i in 1:length(df)) \u0026#123; print(normalize(df[[i]])) \u0026#125; #\u0026gt; [1] 0.0000000 0.5200245 0.3014218 0.4409221 0.8169209 0.1519029 0.3902813 #\u0026gt; [8] 1.0000000 0.2970435 0.5153320 #\u0026gt; [1] 0.9915841 1.0000000 0.4352500 0.0000000 0.8684414 0.9250426 0.3970564 #\u0026gt; [8] 0.8335988 0.7141351 0.5377301 #\u0026gt; [1] 1.0000000 0.5131949 0.4367290 0.1242323 0.5998479 0.8402709 0.0000000 #\u0026gt; [8] 0.7587425 0.2170914 0.7489915 #\u0026gt; [1] 0.85264136 0.05580354 0.79127366 0.10586772 0.00000000 1.00000000 #\u0026gt; [7] 0.43627787 0.63372198 0.44736338 0.42096304  Here again, we are just printing the output, not saving it to a new data frame.\nSo, according to our strategy, we want to create an empty data frame to hold our results. We can use information from our original data frame to do this.\nempty_vec \u0026lt;- vector(length = nrow(df)) # Empty vector with correct number of rows df_norm \u0026lt;- tibble(a = empty_vec, b = empty_vec, c = empty_vec, d = empty_vec) for (i in 1:length(df))\u0026#123; df_norm[[i]] \u0026lt;- normalize(df[[i]]) \u0026#125; df_norm #\u0026gt; # A tibble: 10 × 4 #\u0026gt; a b c d #\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 0 0.992 1 0.853  #\u0026gt; 2 0.520 1 0.513 0.0558 #\u0026gt; 3 0.301 0.435 0.437 0.791  #\u0026gt; 4 0.441 0 0.124 0.106  #\u0026gt; 5 0.817 0.868 0.600 0  #\u0026gt; 6 0.152 0.925 0.840 1  #\u0026gt; 7 0.390 0.397 0 0.436  #\u0026gt; 8 1 0.834 0.759 0.634  #\u0026gt; 9 0.297 0.714 0.217 0.447  #\u0026gt; 10 0.515 0.538 0.749 0.421  Using a map command. It\u0026rsquo;s a pain to have to manually set up the \u0026lsquo;container\u0026rsquo; that will house your results. Couldn\u0026rsquo;t the computer do that for us? Yes! All of this housekeeping, the for loop, the preallocation of data frame size, is done behind the scenes as part of the implementation of lapply() and map.\nmap_norm \u0026lt;- map(df, normalize) str(map_norm) #\u0026gt; List of 4 #\u0026gt; $ a: num [1:10] 0 0.52 0.301 0.441 0.817 ... #\u0026gt; $ b: num [1:10] 0.992 1 0.435 0 0.868 ... #\u0026gt; $ c: num [1:10] 1 0.513 0.437 0.124 0.6 ... #\u0026gt; $ d: num [1:10] 0.8526 0.0558 0.7913 0.1059 0 ...  Notice that the output of map (like lapply) is a list. But we can easily convert it into a data frame:\nmap_norm_df \u0026lt;- map(df, normalize) %\u0026gt;% as_tibble str(map_norm_df) #\u0026gt; tibble [10 × 4] (S3: tbl_df/tbl/data.frame) #\u0026gt; $ a: num [1:10] 0 0.52 0.301 0.441 0.817 ... #\u0026gt; $ b: num [1:10] 0.992 1 0.435 0 0.868 ... #\u0026gt; $ c: num [1:10] 1 0.513 0.437 0.124 0.6 ... #\u0026gt; $ d: num [1:10] 0.8526 0.0558 0.7913 0.1059 0 ...  ","date":1650326400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1650555085,"objectID":"38e00b0c5842cb2265b785b80c52cee9","permalink":"https://biodash.github.io/codeclub/s03e12_functions_in_loops/","publishdate":"2022-04-19T00:00:00Z","relpermalink":"/codeclub/s03e12_functions_in_loops/","section":"codeclub","summary":"In this session of Code Club, we'll see how putting your own functions into loops prevents repeating yourself even more.","tags":null,"title":"Session S03E12: Incorporating your own functions into loops","type":"codeclub"},{"authors":["Michael Broe"],"categories":null,"content":"\n New To Code Club?   First, check out the Code Club Computer Setup instructions, which also has some pointers that might be helpful if you\u0026rsquo;re new to R or RStudio.\n  Please open RStudio before Code Club to test things out \u0026ndash; if you run into issues, join the Zoom call early and we\u0026rsquo;ll troubleshoot.\n   Session Goals  Learn the basic template of a function in R. Learn another way to avoid repetition in your code by creating your own functions. Learn all the advantages of using functions instead of copied code blocks.   We\u0026rsquo;ll be using tibble() from the tidyverse package, so we need to load that first.\nlibrary(tidyverse) #\u0026gt; ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ── #\u0026gt; ✔ ggplot2 3.3.5 ✔ purrr  0.3.4 #\u0026gt; ✔ tibble  3.1.6 ✔ dplyr  1.0.8 #\u0026gt; ✔ tidyr  1.2.0 ✔ stringr 1.4.0 #\u0026gt; ✔ readr  2.1.2 ✔ forcats 0.5.1 #\u0026gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── #\u0026gt; ✖ dplyr::filter() masks stats::filter() #\u0026gt; ✖ dplyr::lag() masks stats::lag()  What is an R function? A good way to understand this is to translate knowledge you already have from math directly into R. \u0026lsquo;Once upon a time\u0026rsquo; you probably met something like this:\n$$y = 2x +3$$\nwhich relates an expression involving $x$ (on the right hand side) to an equivalent value $y$.\nIn mathematics, a function is just a \u0026lsquo;rule\u0026rsquo; that relates inputs to outputs (with certain constraints).\nLater you may have come across this formulation:\n$$f(x) = 2x +3$$\nNow the function has a name f() (not a particularly good one, however).\nYou probably recall that $x$ is called the argument of the function. So how do we translate this into R?\nThe crucial thing here is the R function() operator:\nf \u0026lt;- function(x) \u0026#123; (2 * x) + 3 \u0026#125;  We define the function using this function() operator, and also assign the function to a name (here f) using \u0026lt;-, just like assigning any other value to an object. This means that now f is a function object: just like you create vector objects, or lists, or data frame objects, when you assign them to names. Notice too that in RStudio they appear in the Global Environment in a special \u0026lsquo;Functions\u0026rsquo; section, and clicking on them shows the code of the function. This means that if you have a large file with many functions defined, you don\u0026rsquo;t have to go back searching for the function definition in the code itself.\nTerminology (gotta have it!)\nx here is also called the argument to the function. In this case there is just one, there could be more.\nThe expression inside the curly braces: $(2 * x) + 3$, is called the body of the function.\nHere is the basic template of any function in R:\nname \u0026lt;- function(arg1, arg2...) { \u0026lt;body\u0026gt; } The arguments go inside (\u0026hellip;). The body is the block of code you want to reuse, and it\u0026rsquo;s contained in curly brackets {\u0026hellip;}.\nGiving good names to functions can be tricky. You to don\u0026rsquo;t want to be too explicit, and you don\u0026rsquo;t want to be too terse (f() is too terse, btw). We\u0026rsquo;ll return to this below when we write fancier functions.\nBut now we want to actually use the function to compute an output: this is termed calling the function, by passing in a specific value. That specific value gets assigned to the argument inside the function.\nHere is a trivial example, assigning the value $1$ to the argument:\nf(1) #\u0026gt; [1] 5  So after calling the function in this way x is instantiated to 1, and what is really happening is:\nf \u0026lt;- function(1) { (2 * 1) + 3 } Easy-peasy.\nBut wait! if you simply call a function, its output (which you are probably interested in) just goes away. If you want to save the output of the function, to be used later, you need to assign the output to a variable:\nmy_output \u0026lt;- f(1) my_output #\u0026gt; [1] 5  This variable will now appear in the Values section of your Environment pane in RStudio, and can be reused in your subsequent code.\nBut wait! remember R data-structures? What if we pass in a vector as the argument?\nf(c(1, 2, 3, 4, 5)) #\u0026gt; [1] 5 7 9 11 13  Woohoo! In R, the functions you write yourself are also automagically \u0026lsquo;vectorized\u0026rsquo;: vector in, vector out.\nBut wait! how about a list???\nf(list(1, 2, 3, 4, 5)) #\u0026gt; Error in 2 * x: non-numeric argument to binary operator  Whoops! Looks like R is vectorized over vectors :). If we want to process our new function over a list we\u0026rsquo;ll have to do something more fancy, like a for-loop. We\u0026rsquo;ll see how to incorporate our own functions into for-loops, building on previous sessions, next week.\nBreakout rooms, write your own functions Exercise 1 Write a function from scratch that computes the square of a single input. Give it a sensible name, and run it on some test input numbers to make sure it works.\nMake sure to look in the Functions pane of the RStudio Environment tab to check it is assigned correctly.\nIs your function vectorized? Run it on a simple vector argument to check.\n  Hints (click here)  The relevant R exponentiation operator is `**`.    Solution (click here)  square \u0026lt;- function(x)\u0026#123; x ** 2 \u0026#125; square(2) #\u0026gt; [1] 4 square(c(1, 2, 3, 4)) #\u0026gt; [1] 1 4 9 16    Exercise 2 The function in Exercise 1 has a single argument, the base, and raises it to the exponent 2.\nWrite a function with two arguments, which includes both a base and exponent and try it out.\nIs your function vectorized? Run it on a simple base vector to check.\nBonus: is it vectorized on both the base and the exponent?\n  Solution (click here)  exponential \u0026lt;- function(b, e)\u0026#123; b ** e \u0026#125; exponential(2, 10) #\u0026gt; [1] 1024 exponential(c(1, 2, 3, 4), 10) #\u0026gt; [1] 1 1024 59049 1048576 exponential(c(1, 2, 3, 4), c(1, 2, 3, 4)) #\u0026gt; [1] 1 4 27 256 exponential(c(1, 2, 3, 4), c(1, 2)) #\u0026gt; [1] 1 4 3 16    Exercise 3 R does not have a built-in function for calculating the coefficient of variation, aka the RSD (relative standard deviation). This is defined as the ratio of the standard deviation to the mean.\nCreate a function that computes this, and test it on a couple of vectors.\n  Hints (click here)  The relevant R built-in functions are sd() and mean(). The function should have one argument, which is assumed to be a vector.    Solution (click here)  coefficient_of_variation \u0026lt;- function(v)\u0026#123; sd(v)/mean(v) \u0026#125; coefficient_of_variation(1:10) #\u0026gt; [1] 0.5504819 coefficient_of_variation(15:175) #\u0026gt; [1] 0.4907454     Why write functions? (Spoiler alert, DRY\u0026hellip;) Copying your code is not good The first motivation for writing a function is when you find yourself cut-and-pasting code blocks with slight alterations each time.\nSay we have the following toy tidyverse data frame, where each column is a vector of 10 random numbers from a normal distribution, with mean = 0 and sd = 1 (the defaults for rnorm):\ndf \u0026lt;- tibble( a = rnorm(10), b = rnorm(10), c = rnorm(10), d = rnorm(10) ) df #\u0026gt; # A tibble: 10 × 4 #\u0026gt; a b c d #\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 -1.88 0.727 -0.468 1.27  #\u0026gt; 2 0.0399 -0.789 -2.41 0.376 #\u0026gt; 3 -0.410 1.86 1.05 -0.618 #\u0026gt; 4 -0.266 -0.204 -0.715 0.279 #\u0026gt; 5 0.244 -0.0673 0.884 -0.340 #\u0026gt; 6 0.0645 -1.09 0.152 -0.840 #\u0026gt; 7 0.749 -0.494 0.681 0.341 #\u0026gt; 8 -1.35 0.208 0.0776 2.15  #\u0026gt; 9 -1.17 0.0883 -0.829 -0.174 #\u0026gt; 10 0.733 1.05 0.808 0.190  In previous Code Clubs we\u0026rsquo;ve seen how you can apply a built-in function like mean to each column using a for loop or lapply. But say we wanted to do something a bit fancier that is not part of core R. For example, we can normalize the values in a column so they range from 0 to 1 using the following code block:\n(df$a - min(df$a)) / (max(df$a) - min(df$a)) #\u0026gt; [1] 0.0000000 0.7297073 0.5582684 0.6132731 0.8073051 0.7390605 1.0000000 #\u0026gt; [8] 0.1988908 0.2699569 0.9939179  This code is a literal translation of the mathematical formula for normalization:\n$$z_{i} = \\frac{x_{i} - min(x)}{max(x)-min(x)}$$\nOK, so how can we do this for each column? Here is a first attempt:\ndf$a \u0026lt;- (df$a - min(df$a)) / (max(df$a) - min(df$a)) df$b \u0026lt;- (df$b - min(df$a)) / (max(df$b) - min(df$b)) df$c \u0026lt;- (df$c - min(df$c)) / (max(df$c) - min(df$c)) df$d \u0026lt;- (df$d - min(df$d)) / (max(df$d) - min(df$d)) df #\u0026gt; # A tibble: 10 × 4 #\u0026gt; a b c d #\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 0 0.246 0.562 0.706  #\u0026gt; 2 0.730 -0.267 0 0.407  #\u0026gt; 3 0.558 0.630 1 0.0746 #\u0026gt; 4 0.613 -0.0691 0.490 0.375  #\u0026gt; 5 0.807 -0.0228 0.952 0.168  #\u0026gt; 6 0.739 -0.370 0.741 0  #\u0026gt; 7 1 -0.167 0.894 0.396  #\u0026gt; 8 0.199 0.0705 0.719 1  #\u0026gt; 9 0.270 0.0299 0.458 0.223  #\u0026gt; 10 0.994 0.356 0.930 0.345  This works, but it caused me mental anguish to type it out. Even with cut and paste! All those manual textual substitutions!! And manual data entry is prone to mistakes, especially repetitive tasks like this. And say you had 1,000 columns\u0026hellip;\nAnd it didn\u0026rsquo;t work!! Honestly, I swear that mistake was totally real: I didn\u0026rsquo;t notice it until I looked at the output. Can you spot the mistake?\nIt turns out R has a range function that returns the minimum and maximum of a vector, which somewhat simplifies the coding:\nrange(df$a)  The result is a vector like c(-1.2129504, 2.1011248) (it varies run to run, since the columns values are random) which we can then index, and so we only do the min/max computation once for each column, instead of three times, so we get the following block of code for each column:\nrng \u0026lt;- range(df$a) (df$a - rng[1]) / (rng[2] - rng[1])  Does this help?\nrng \u0026lt;- range(df$a) df$a \u0026lt;- (df$a - rng[1]) / (rng[2] - rng[1]) rng \u0026lt;- range(df$b) df$b \u0026lt;- (df$b - rng[1]) / (rng[2] - rng[1]) rng \u0026lt;- range(df$c) df$c \u0026lt;- (df$c - rng[1]) / (rng[2] - rng[1]) rng \u0026lt;- range(df$d) df$d \u0026lt;- (df$d - rng[1]) / (rng[2] - rng[1])  Still pretty horrible, and arguably worse since we add a line for each column.\nHow can we distill this into a function to avoid all that repetition?\nEncapsulation of code in a function The secret to function writing is abstracting the constant from the variable. (Using the range function does throw into sharper relief what is constant and what is varying at least.) The constant part is the body of the function: the template or boiler-plate you use over and over again. The variable parts are the arguments of the function.\nHere\u0026rsquo;s what it looks like in this case:\nnormalize \u0026lt;- function(x) \u0026#123; rng \u0026lt;- range(x) (x - rng[1]) / (rng[2] - rng[1]) \u0026#125;  Pretty cool, right? Here normalize is the descriptive name we give the function.\nIn the current case the argument of our function is a column vector pulled from the data frame. But we can potentially use this function on any vector, so let\u0026rsquo;s not be too specific. The more generally you can write your function, the more useful it will be.\ntest_vec \u0026lt;- c(3, 7, pi, 8.657, 80) normalize(test_vec) #\u0026gt; [1] 0.000000000 0.051948052 0.001838866 0.073467532 1.000000000  A couple of things to note:\n  Including that extra line rng \u0026lt;- range(x) is no longer a problem, since we just type it once. If you are typing things out over and over you might prefer brevity. When you write a function, you should prefer clarity. It\u0026rsquo;s good practice to break the the function down into logical steps, and name them properly. It\u0026rsquo;s much easier for others to \u0026lsquo;read\u0026rsquo; your function, and much easier for you when you come back to it in a couple of years. This is the principle of making your code \u0026lsquo;self-annotated\u0026rsquo;.\n  Functions should be simple, clear, and do one thing well. You create programs by combining simple functions in a modular manner.\n  Our original horrible code can now be rewritten as:\ndf$a \u0026lt;- normalize(df$a) df$b \u0026lt;- normalize(df$b) df$c \u0026lt;- normalize(df$c) df$d \u0026lt;- normalize(df$d)  Which is an improvement, but the real power comes from the fact that we can use our new function in for loops and apply statements. We\u0026rsquo;ll see how to do that next week.\nBy writing our own functions, we\u0026rsquo;ve effectively extended what R can do. And this is all that packages are: libraries of new functions that extend the capabilities of base R. In fact, if there are functions you design for your particular subject area and find yourself using all the time, you can make your own package and load it, and all your favorite functions will be right there (but that\u0026rsquo;s for another day\u0026hellip;)\nExercise 4: Functions are not just for arithmetic! The fastq file format for DNA sequencing uses a letter/punctuation code for the quality of the base called at each position (the fourth line below) which is in one-to-one relationship to the bases in the second line:\n@SIM:1:FCX:1:15:6329:1045 1:N:0:2 TCGCACTCAACGCCCTGCATATGACAAGACAGAATC + \u0026lt;\u0026gt;;##=\u0026gt;\u0026lt;9=AAAAAAAAAA9#:\u0026lt;#\u0026lt;;\u0026lt;\u0026lt;\u0026lt;????#=  To translate a letter code into a numerical phred quality score we have to do two things: (i) translate the character to an integer using the ASCII code look up table (ii) subtract 33 from that value (!). (High scores are good).\nFor the first step, R has a built-in function that converts a character into an integer according to that table, for example:\nutf8ToInt(\"!\") #\u0026gt; [1] 33  Write a function phred_score() that computes the phred score for any character. Check that it returns 0 for \u0026ldquo;!\u0026rdquo;.\nIs this function vectorized? Apply your function to our example string:\n\u0026lt;\u0026gt;;##=\u0026gt;\u0026lt;9=AAAAAAAAAA9#:\u0026lt;#\u0026lt;;\u0026lt;\u0026lt;\u0026lt;????#=\nto convert it to a vector of phred quality scores.\n  Hints (click here)  \nRemember when you pass the value to this function it has to be an R character string, which needs to be surrounded by quotes.\n   Solution (click here)  phred_score \u0026lt;- function(character)\u0026#123; utf8ToInt(character) - 33 \u0026#125; phred_score(\"!\") #\u0026gt; [1] 0 phred_score(\"\u0026lt;\u0026gt;;##=\u0026gt;\u0026lt;9=AAAAAAAAAA9#:\u0026lt;#\u0026lt;;\u0026lt;\u0026lt;\u0026lt;????#=\") #\u0026gt; [1] 27 29 26 2 2 28 29 27 24 28 32 32 32 32 32 32 32 32 32 32 24 2 25 27 2 #\u0026gt; [26] 27 26 27 27 27 30 30 30 30 2 28  Note: \u0026ldquo;!\u0026rdquo; is the first printing character in the ASCII table. The characters 0 through 32 were used historically to control the behavior of teleprinters: \u0026ldquo;the original ASCII specification included 33 non-printing control codes which originated with Teletype machines; most of these are now obsolete\u0026rdquo;. If the ASCII table started with \u0026ldquo;!\u0026rdquo; instead of NUL we wouldn\u0026rsquo;t need the correction of subtracting 33, and \u0026ldquo;!\u0026rdquo; would translate to 0.    ","date":1648425600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648654483,"objectID":"ed61ad1f762318a38328edb0246ac7c1","permalink":"https://biodash.github.io/codeclub/s03e11_functions/","publishdate":"2022-03-28T00:00:00Z","relpermalink":"/codeclub/s03e11_functions/","section":"codeclub","summary":"In this session of Code Club, we'll look at how to avoid repeating yourself in another way by writing your own functions.","tags":null,"title":"Session S03E11: Writing your own Functions","type":"codeclub"},{"authors":["Mike Sovic"],"categories":null,"content":" Session Goals  List and differentiate between some useful purrr functions. Compare purrr functions to the apply() functions from last session. Use purrr functions as alternatives to loops.   Highlights From Last Session In the previous session, we explored some functions from the apply() family of functions, which provided alternatives to writing loops to iteratively apply some function to the elements of a data structure such as a vector, list, or data frame. Let\u0026rsquo;s start with a simple list with three entries\u0026hellip;\nGenerate An Example List our_list \u0026lt;- list(A = 1:10, B = 11:20, C = 21:30) our_list #\u0026gt; $A #\u0026gt; [1] 1 2 3 4 5 6 7 8 9 10 #\u0026gt;  #\u0026gt; $B #\u0026gt; [1] 11 12 13 14 15 16 17 18 19 20 #\u0026gt;  #\u0026gt; $C #\u0026gt; [1] 21 22 23 24 25 26 27 28 29 30   We could use a loop to calculate the mean for each of the entries.\nCalculate Mean For Each List Entry With A Loop results_loop \u0026lt;- list() for (i in 1:length(our_list)) \u0026#123; results_loop[i] \u0026lt;- mean(our_list[[i]], na.rm = TRUE) \u0026#125; results_loop #\u0026gt; [[1]] #\u0026gt; [1] 5.5 #\u0026gt;  #\u0026gt; [[2]] #\u0026gt; [1] 15.5 #\u0026gt;  #\u0026gt; [[3]] #\u0026gt; [1] 25.5   Like we saw last week, lapply() gave us an alternative way to do the same thing, and with simpler and clearer code\u0026hellip;\nCalculate Mean For Each Variable With lapply() res_lapply \u0026lt;- lapply(our_list, mean, na.rm = TRUE) res_lapply #\u0026gt; $A #\u0026gt; [1] 5.5 #\u0026gt;  #\u0026gt; $B #\u0026gt; [1] 15.5 #\u0026gt;  #\u0026gt; $C #\u0026gt; [1] 25.5   I mentioned in the previous session that when working with the apply() functions, it\u0026rsquo;s important to think about the structure/type of data going in to the function, and also that getting returned by the function. We saw that the lapply() example above returned a list, and can confirm that with\u0026hellip;\nclass(res_lapply) #\u0026gt; [1] \"list\"   We also saw that sapply() was very similar to lapply(), but instead of a list being returned, the results were condensed down to a vector - specifically in this case, a numeric vector\u0026hellip;\nres_sapply \u0026lt;- sapply(our_list, mean, na.rm = TRUE) res_sapply #\u0026gt; A B C  #\u0026gt; 5.5 15.5 25.5 class(res_sapply) #\u0026gt; [1] \"numeric\"   map() Functions From purrr In this session, we\u0026rsquo;re going to look at some of the map() functions from the purrr package, which is part of the tidyverse. In some cases, these functions return the same results as their apply() analogues. As an example, compare the map() function to lapply()\u0026hellip;\nlibrary(tidyverse) res_map \u0026lt;- map(our_list, mean, na.rm = TRUE) res_map #\u0026gt; $A #\u0026gt; [1] 5.5 #\u0026gt;  #\u0026gt; $B #\u0026gt; [1] 15.5 #\u0026gt;  #\u0026gt; $C #\u0026gt; [1] 25.5   So why might you want to use the map() functions? There are two primary reasons\u0026hellip;\n The syntax/names of the map() functions might be easier to understand and make for clearer code. The map() family provides additional functionality that might at best be cumbersome to achieve with the base R approaches. We won\u0026rsquo;t get to this in this session, but see the imap() function for one example.  Ultimately, much of the value in using these map() functions gets realized when you start writing your own custom functions, which is something we haven\u0026rsquo;t done yet, but will be doing soon. For now, we\u0026rsquo;ll work with some fairly basic examples just to get introduced to some of the syntax and usage of these functions. There\u0026rsquo;s a purrr cheatsheet that you might find helpful available at https://raw.githubusercontent.com/rstudio/cheatsheets/main/purrr.pdf\nWe\u0026rsquo;ll start working with map() in the first breakout session\u0026hellip;\nBreakout Exercises 1 Like with the first breakout exercise from last week, below I\u0026rsquo;m pulling out a subset of the numeric variables available in the penguins data frame and reformating them into a list named pens_list that we\u0026rsquo;ll use to practice with map() functions.\nlibrary(tidyverse) library(palmerpenguins) pens_list \u0026lt;- select_if(penguins, is.numeric) %\u0026gt;% select(-year) %\u0026gt;% as.list() str(pens_list) #\u0026gt; List of 4 #\u0026gt; $ bill_length_mm : num [1:344] 39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ... #\u0026gt; $ bill_depth_mm : num [1:344] 18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ... #\u0026gt; $ flipper_length_mm: int [1:344] 181 186 195 NA 193 190 181 195 193 190 ... #\u0026gt; $ body_mass_g : int [1:344] 3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ...    Use map() to calculate the mean value for each of the variables/entries in pens_list. What type of results are returned (i.e. is it a list, vector, data frame, etc)?\n  Hints (click here)  \nApply the mean() function with map(). Remember there are NA\u0026rsquo;s in the data - see the help for mean() for dealing with those. You can view the result, or try class() to get the type of object returned.\n   Solution (click here)  res_map \u0026lt;- map(pens_list, mean, na.rm = TRUE) res_map #\u0026gt; $bill_length_mm #\u0026gt; [1] 43.92193 #\u0026gt;  #\u0026gt; $bill_depth_mm #\u0026gt; [1] 17.15117 #\u0026gt;  #\u0026gt; $flipper_length_mm #\u0026gt; [1] 200.9152 #\u0026gt;  #\u0026gt; $body_mass_g #\u0026gt; [1] 4201.754 class(res_map) #\u0026gt; [1] \"list\"       As we saw last week, and in the example above, sapply() is similar to lapply(), but returns results as a vector instead of as a list. Take a look at the help for map() and find a function that will return a vector of doubles (numerics) and apply it to the pens_list object like you just did with map(). Then find another function that will return the results as a data frame.\n  Hints (click here)  \nTry the map_dbl() and map_dfc() functions.\n   Solution (click here)  map_vec \u0026lt;- map_dbl(pens_list, mean, na.rm = TRUE) map_vec #\u0026gt; bill_length_mm bill_depth_mm flipper_length_mm body_mass_g  #\u0026gt; 43.92193 17.15117 200.91520 4201.75439 map_df \u0026lt;- map_dfc(pens_list, mean, na.rm = TRUE) map_df #\u0026gt; # A tibble: 1 x 4 #\u0026gt; bill_length_mm bill_depth_mm flipper_length_mm body_mass_g #\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 43.9 17.2 201. 4202.      Remember that with map(), the input is a single list or vector, and the function is applied to each element of the list of vector. A number of variants of the map() function are available that define the type of output that gets returned.\nmap2() While map() allowed us to apply some function to the elements of a single list or vector, map2() lets us apply some operation to paired elements from two lists (or vectors).\nour_list1 \u0026lt;- list(1:10) our_list1 #\u0026gt; [[1]] #\u0026gt; [1] 1 2 3 4 5 6 7 8 9 10 our_list2 \u0026lt;- list(11:20) our_list2 #\u0026gt; [[1]] #\u0026gt; [1] 11 12 13 14 15 16 17 18 19 20   Here\u0026rsquo;s we\u0026rsquo;ll use map2() to get the sums of corresponding pairs of elements from the two lists\u0026hellip;\nmap2(our_list1, our_list2, ~ .x + .y) #\u0026gt; [[1]] #\u0026gt; [1] 12 14 16 18 20 22 24 26 28 30   The tilde (~) in the third argument indicates a formula that will be converted to a function and applied. It\u0026rsquo;s actually kind of a short-hand way to write and apply a custom function, but since we haven\u0026rsquo;t gotten in to writing our own functions yet (though that\u0026rsquo;s coming soon!), for now, just remember that the function passed to map2() has to take two arguments, which are defined as \u0026lsquo;.x\u0026rsquo; and \u0026lsquo;.y\u0026rsquo; (elements in corresponding positions of the first and second lists or vectors, respectively) in each iteration of the function.\nBreakout Exercises 2 Below are two vectors containing bill-measurement data from the penguins data frame (bill length and bill depth).\nbill_length_mm \u0026lt;- pens_list$bill_length_mm bill_depth_mm \u0026lt;- pens_list$bill_depth_mm    Use map2() to calculate the bill ratio for each penguin (length/depth). Output the result as a vector containing doubles (numerics), and save it as the object bill_ratios.\n  Hints (click here)  \nApply the map2_dbl() function and use the third argument to define a formula that divides the length value by the width value.\n   Solution (click here)  bill_ratios \u0026lt;- map2_dbl(bill_length_mm, bill_depth_mm, ~ .x / .y)       How many of the penguins in the dataset have bill ratios greater than 3?\n  Hints (click here)  \nLogicals in R can be interpreted as 1/0 (TRUE/FALSE). Try using sum() to sum over the results of a logical expression to get the number of ratios \u0026gt; 3. Alternatively, you could index bill_ratios to retain just the values \u0026gt; 3, and then get the length of that vector. Remember that there are NA\u0026rsquo;s mixed in - how does this affect each of these two approaches?\n   Solution (click here)  sum(bill_ratios \u0026gt; 3, na.rm = TRUE) #\u0026gt; [1] 109 # OR bill_ratios[bill_ratios \u0026gt; 3] %\u0026gt;% na.omit() %\u0026gt;% length() #\u0026gt; [1] 109      Bonus  Here\u0026rsquo;s one more dataset that\u0026rsquo;s based on the penguins data (though much of it is made up). It represents measurements taken for each of three penguins over three years. List1 has data for year 1, List2 has data for year 2, and List3 has data for year 3.\nyr1_list \u0026lt;- list(\"Pen_1\" = c(\"bill_length_mm\" = 39.1, \"bill_depth_mm\" = 18.7, \"flipper_length_mm\" = 181, \"body_mass_g\" = 3750), \"Pen_2\" = c(\"bill_length_mm\" = 39.5, \"bill_depth_mm\" = 17.4, \"flipper_length_mm\" = 186, \"body_mass_g\" = 3800), \"Pen_3\" = c(\"bill_length_mm\" = 40.3, \"bill_depth_mm\" = 18, \"flipper_length_mm\" = 195, \"body_mass_g\" = 3250)) yr2_list \u0026lt;- list(\"Pen_1\" = c(\"bill_length_mm\" = 39.8, \"bill_depth_mm\" = 18.9, \"flipper_length_mm\" = 184, \"body_mass_g\" = 3767), \"Pen_2\" = c(\"bill_length_mm\" = 38.7, \"bill_depth_mm\" = 17.2, \"flipper_length_mm\" = 186, \"body_mass_g\" = 3745), \"Pen_3\" = c(\"bill_length_mm\" = 40.7, \"bill_depth_mm\" = 18.6, \"flipper_length_mm\" = 217, \"body_mass_g\" = 3470)) yr3_list \u0026lt;- list(\"Pen_1\" = c(\"bill_length_mm\" = 40.2, \"bill_depth_mm\" = 19.3, \"flipper_length_mm\" = 188, \"body_mass_g\" = 3790), \"Pen_2\" = c(\"bill_length_mm\" = 38.4, \"bill_depth_mm\" = 17.0, \"flipper_length_mm\" = 187, \"body_mass_g\" = 3710), \"Pen_3\" = c(\"bill_length_mm\" = 40.9, \"bill_depth_mm\" = 18.9, \"flipper_length_mm\" = 228, \"body_mass_g\" = 3493))   Try calculating the average value for each of the variables for each penguin over the three years. Output the results as a data frame. Note you\u0026rsquo;ll need an new map() function that we haven\u0026rsquo;t used yet. Take a look at the help for map2(), or the purrr cheatsheet, to find a similar function that works on more than two lists (takes functions with 3 or more required arguments).\n  Hints (click here)  The pmap() can be used here. It has some similarities to map2(), but instead of applying to 2 lists, pmap() works with 3 or more. Notice the names of the lists the function will be applied to are given as a single argument (a list). Provide a formula to calculate the mean. Unlike the map2() function, which only works on functions that take 2 arguments (denoted \u0026lsquo;.x\u0026rsquo; and \u0026lsquo;.y\u0026rsquo;), the number of arguments passed to the function used in pmap() can be three or more, and they are denoted \u0026lsquo;..1\u0026rsquo;, \u0026lsquo;..2\u0026rsquo;, \u0026lsquo;..3\u0026rsquo;, etc.\n   Solution (click here)  pmap_dfr(list(yr1_list, yr2_list, yr3_list), ~ (..1 + ..2 + ..3)/3) #\u0026gt; # A tibble: 3 x 4 #\u0026gt; bill_length_mm bill_depth_mm flipper_length_mm body_mass_g #\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 39.7 19.0 184. 3769  #\u0026gt; 2 38.9 17.2 186. 3752. #\u0026gt; 3 40.6 18.5 213. 3404.      ","date":1647820800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648128528,"objectID":"66691d52638ba796da77fb4c76b126fe","permalink":"https://biodash.github.io/codeclub/s03e10_purrr/","publishdate":"2022-03-21T00:00:00Z","relpermalink":"/codeclub/s03e10_purrr/","section":"codeclub","summary":"In this session of Code Club, we'll consider some functions from the purrr package, which can be used as efficient alternatives to for loops and the `apply()` functions we explored in the previous session.","tags":null,"title":"Session S03E10: Functional Programming With purrr::map() Functions","type":"codeclub"},{"authors":["Mike Sovic"],"categories":null,"content":" Session Goals  Continue practicing with loops in R. Describe how the apply() functions relate to loops in R. Use apply functions as alternatives to loops. Identify the input and output formats associated with different apply() functions.   Highlights From Recent Sessions In the past several sessions, we\u0026rsquo;ve talked about several things that have relevance to today\u0026rsquo;s discussion of the apply() functions. Here\u0026rsquo;s a quick review\u0026hellip;\nData Structures And Indexing There are several widely-used data structures in R. They include vectors, lists, and data frames. As Michael Broe showed in a recent session on data structures, each of these can be indexed, which means we can pull out one or more specific elements from those structures.\nVectors Vectors have one dimension (can be characterized by their length), and all the elements of any vector in R have to be of the same class. They are often created with the c() (combine) function\u0026hellip;\n#Create some vectors num_vector1 \u0026lt;- 1:10 num_vector1 #\u0026gt; [1] 1 2 3 4 5 6 7 8 9 10 class(num_vector1) #\u0026gt; [1] \"integer\" num_vector2 \u0026lt;- c(1,2,6,10) num_vector2 #\u0026gt; [1] 1 2 6 10 class(num_vector2) #\u0026gt; [1] \"numeric\" log_vector \u0026lt;- c(TRUE, FALSE, TRUE, FALSE) log_vector #\u0026gt; [1] TRUE FALSE TRUE FALSE class(log_vector) #\u0026gt; [1] \"logical\" #Index a vector num_vector2[c(1,3)] #\u0026gt; [1] 1 6 num_vector2[log_vector] #\u0026gt; [1] 1 6   Lists #Create a list my_list \u0026lt;- list(\"num_vec1\" = num_vector1, \"num_vec2\" = num_vector2, \"log_vec\" = c(TRUE, FALSE, TRUE, FALSE)) #View the list my_list #\u0026gt; $num_vec1 #\u0026gt; [1] 1 2 3 4 5 6 7 8 9 10 #\u0026gt;  #\u0026gt; $num_vec2 #\u0026gt; [1] 1 2 6 10 #\u0026gt;  #\u0026gt; $log_vec #\u0026gt; [1] TRUE FALSE TRUE FALSE #Try some indexing my_list[2] #\u0026gt; $num_vec2 #\u0026gt; [1] 1 2 6 10 my_list[[2]] #\u0026gt; [1] 1 2 6 10 my_list$num_vec2 #\u0026gt; [1] 1 2 6 10   Data Frames #Create a data frame my_df \u0026lt;- data.frame(\"num_vec\" = num_vector2, \"log_vec\" = c(TRUE, FALSE, TRUE, FALSE)) #View the data frame my_df #\u0026gt; num_vec log_vec #\u0026gt; 1 1 TRUE #\u0026gt; 2 2 FALSE #\u0026gt; 3 6 TRUE #\u0026gt; 4 10 FALSE #OR my_df \u0026lt;- as.data.frame(my_list[c(2,3)]) my_df #\u0026gt; num_vec2 log_vec #\u0026gt; 1 1 TRUE #\u0026gt; 2 2 FALSE #\u0026gt; 3 6 TRUE #\u0026gt; 4 10 FALSE #Index the data frame my_df[2] #\u0026gt; log_vec #\u0026gt; 1 TRUE #\u0026gt; 2 FALSE #\u0026gt; 3 TRUE #\u0026gt; 4 FALSE my_df[[2]] #\u0026gt; [1] TRUE FALSE TRUE FALSE my_df$log_vec #\u0026gt; [1] TRUE FALSE TRUE FALSE   Loops As Jelmer demonstrated in last week\u0026rsquo;s session, loops allow you to iteratively apply some task(s) to a series of inputs. In the simple loop below, we take each of the three values (1,3,6), print a statement with the original value, then negate the value and print another statement with the updated value\u0026hellip;\nfor (x in c(1,3,6)) \u0026#123; print(paste0(\"Input value is \", x)) x \u0026lt;- -x print(paste0(\"Negated value is \", x)) \u0026#125; #\u0026gt; [1] \"Input value is 1\" #\u0026gt; [1] \"Negated value is -1\" #\u0026gt; [1] \"Input value is 3\" #\u0026gt; [1] \"Negated value is -3\" #\u0026gt; [1] \"Input value is 6\" #\u0026gt; [1] \"Negated value is -6\"   Functions We use functions abundantly in R. Even the simple examples above used multiple functions, including c(), which combined items into a vector, class(), which returned the type, or class of an object, and paste0(), which allowed us to stitch together character vectors and objects into a single expression. Functions typically accept (and often require) arguments - pieces of information that are provided inside the parentheses that may provide input for the function or details that modify its behavior. As a simple example, setting the na.rm argument in the mean() function provides a mean for all values in a vector after removing any that are NA. Otherwise, the mean is returned as \u0026ldquo;NA\u0026rdquo;\u0026hellip;\nvalues \u0026lt;- c(1:5, NA, 7:10) values #\u0026gt; [1] 1 2 3 4 5 NA 7 8 9 10 mean(values) #\u0026gt; [1] NA mean(values, na.rm = TRUE) #\u0026gt; [1] 5.444444   Functionals In contrast to traditional arguments like the na.rm above, some functions accept other functions as arguments - these are sometimes called functionals. In this session we\u0026rsquo;ll look at some of the functionals in the apply() group. These provide alternatives to writing loops by allowing us to iteratively apply some function over structures like lists or data frames. They include\u0026hellip;\n apply() - apply some function to the margins (rows or columns) of a rectangular object (i.e. matrix or data frame) lapply() - apply some function to each element of a list sapply() - similar to lapply(), but provides output in a different format mapply() - apply a function to multiple lists  Key to understanding how and when to use each of these is thinking about the structure of the data going in and the structure of the results that get returned. We\u0026rsquo;ll start with lapply().\nlapply() lapply() allows you to iteratively apply a function to items in a list, and by default, returns a list of results with the same number of entries as the input had. The only required arguments are the list the function will be applied to and the function itself. Keep in mind that these apply() functions are alternatives to loops. We\u0026rsquo;ll try calculating means with both the loop approach and the apply() approach on the simple_list example below\u0026hellip;\nsimple_list \u0026lt;- list(1:10, 11:15, 16:30) simple_list #\u0026gt; [[1]] #\u0026gt; [1] 1 2 3 4 5 6 7 8 9 10 #\u0026gt;  #\u0026gt; [[2]] #\u0026gt; [1] 11 12 13 14 15 #\u0026gt;  #\u0026gt; [[3]] #\u0026gt; [1] 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30   Calculate Means With A Loop num_entries \u0026lt;- length(simple_list) results_list_loop \u0026lt;- list() for (i in 1:num_entries) \u0026#123; current_mean \u0026lt;- mean(simple_list[[i]]) results_list_loop[i] \u0026lt;- current_mean \u0026#125; results_list_loop #\u0026gt; [[1]] #\u0026gt; [1] 5.5 #\u0026gt;  #\u0026gt; [[2]] #\u0026gt; [1] 13 #\u0026gt;  #\u0026gt; [[3]] #\u0026gt; [1] 23   Calculate Means With lapply() results_list_apply \u0026lt;- lapply(simple_list, mean) results_list_apply #\u0026gt; [[1]] #\u0026gt; [1] 5.5 #\u0026gt;  #\u0026gt; [[2]] #\u0026gt; [1] 13 #\u0026gt;  #\u0026gt; [[3]] #\u0026gt; [1] 23   Notice we can use a lot less code with lapply() to get the same result as with the for loop.\nGive lapply() a try in a Breakout Room\u0026hellip;\nBreakout Exercises 1 As we\u0026rsquo;ve talked about before, lists and data frames are closely related data structures in R - data frames are a special type of list in which all the entries are of the same size, and so they can be neatly organized into a rectangular row/column structure. When data fit that rectangular pattern, it\u0026rsquo;s easy to switch them between lists and data frames.\nThe code below pulls out the columns of the penguins data frame that are numeric and reformats them into a list named pens_list, which we\u0026rsquo;re previewing with the str() function.\nlibrary(tidyverse) library(palmerpenguins) pens_list \u0026lt;- select_if(penguins, is.numeric) %\u0026gt;% as.list() str(pens_list) #\u0026gt; List of 5 #\u0026gt; $ bill_length_mm : num [1:344] 39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ... #\u0026gt; $ bill_depth_mm : num [1:344] 18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ... #\u0026gt; $ flipper_length_mm: int [1:344] 181 186 195 NA 193 190 181 195 193 190 ... #\u0026gt; $ body_mass_g : int [1:344] 3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ... #\u0026gt; $ year : int [1:344] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ...    Calculate the median value for each of the variables/entries in pens_list.\n  Hints (click here)  \nYou can write a loop to do this, or, preferably, use lapply(). You\u0026rsquo;ll need one additional argument (na.rm) for the median() function - see the mean() example above, or check the help for the median() and lapply() functions for more details.\n   Solution (click here)  # loop option results_loop \u0026lt;- list() for (i in 1:length(pens_list)) \u0026#123; results_loop[i] \u0026lt;- median(pens_list[[i]], na.rm = TRUE) \u0026#125; results_loop #\u0026gt; [[1]] #\u0026gt; [1] 44.45 #\u0026gt;  #\u0026gt; [[2]] #\u0026gt; [1] 17.3 #\u0026gt;  #\u0026gt; [[3]] #\u0026gt; [1] 197 #\u0026gt;  #\u0026gt; [[4]] #\u0026gt; [1] 4050 #\u0026gt;  #\u0026gt; [[5]] #\u0026gt; [1] 2008 #lapply option lapply(pens_list, median, na.rm = TRUE) #\u0026gt; $bill_length_mm #\u0026gt; [1] 44.45 #\u0026gt;  #\u0026gt; $bill_depth_mm #\u0026gt; [1] 17.3 #\u0026gt;  #\u0026gt; $flipper_length_mm #\u0026gt; [1] 197 #\u0026gt;  #\u0026gt; $body_mass_g #\u0026gt; [1] 4050 #\u0026gt;  #\u0026gt; $year #\u0026gt; [1] 2008       You might have noticed that one of the columns is year. We don\u0026rsquo;t really need to get the median for that, so use lapply() to calculate the medians again, but this time only do it for the first 4 columns..\n  Hints (click here)  \nIndex the list in the lapply() function with square brackets to apply the function to just the first 4 entries.\n   Solution (click here)  #lapply option lapply(pens_list[1:4], median, na.rm = TRUE) #\u0026gt; $bill_length_mm #\u0026gt; [1] 44.45 #\u0026gt;  #\u0026gt; $bill_depth_mm #\u0026gt; [1] 17.3 #\u0026gt;  #\u0026gt; $flipper_length_mm #\u0026gt; [1] 197 #\u0026gt;  #\u0026gt; $body_mass_g #\u0026gt; [1] 4050       Try the same code again, but this time run it with sapply() instead of lapply(). What\u0026rsquo;s the difference in these two functions?\n  Hints (click here)  \nSimply replace lapply() from the previous exercise with sapply().\n   Solution (click here)  sapply(pens_list[1:4], median, na.rm = TRUE) #\u0026gt; bill_length_mm bill_depth_mm flipper_length_mm body_mass_g  #\u0026gt; 44.45 17.30 197.00 4050.00      apply() lapply() allowed us to apply a function to separate entries in a list. apply() does something similar, but applies the function to the margins (rows or columns) of objects with two dimensions like data frames or matrices.\nLet\u0026rsquo;s start with a simple matrix\u0026hellip;\nsimple_mat \u0026lt;- matrix(1:15, nrow = 3) simple_mat #\u0026gt; [,1] [,2] [,3] [,4] [,5] #\u0026gt; [1,] 1 4 7 10 13 #\u0026gt; [2,] 2 5 8 11 14 #\u0026gt; [3,] 3 6 9 12 15   Now we\u0026rsquo;ll use apply() to get means for entries in simple_mat. Like lapply(), apply() requires that we provide arguments to define the object the function will be applied to and the function itself. But since with apply() the function can either be applied to the rows or columns, we need a third argument to specify which we want. This is done with either a \u0026lsquo;1\u0026rsquo; for rows or a \u0026lsquo;2\u0026rsquo; for columns\u0026hellip;\nGet The Mean For Each Column apply(simple_mat, 2, mean) #\u0026gt; [1] 2 5 8 11 14   Get The Mean For Each Row apply(simple_mat, 1, mean) #\u0026gt; [1] 7 8 9   Breakout Exercises 2 The code below will download a dataframe that contains average monthly temperature data for 282 US locations from 1981-2010, reformat it a bit to make it easier to work with, and store it as the object temp_data.\ntemp_data \u0026lt;- read_csv('https://raw.githubusercontent.com/biodash/biodash.github.io/master/assets/data/temperature/city_temp_data_noaa.csv') %\u0026gt;% unite(\"Location\", City, State, sep = \" \") %\u0026gt;% column_to_rownames(\"Location\")    Preview temp_data. How is it structured? What do the rows and columns represent?\n  Hints (click here)  \nUse head() or glimpse() to preview the dataset.\n   Solution (click here)  head(temp_data) #\u0026gt; JAN FEB MAR APR MAY JUN JUL AUG SEP OCT NOV DEC #\u0026gt; BIRMINGHAM AP AL 53.8 58.4 66.7 74.4 81.5 87.7 90.8 90.6 85.1 75.3 65.4 55.9 #\u0026gt; HUNTSVILLE AL 51.2 55.9 64.9 73.6 81.3 88.2 90.7 90.9 85.0 74.6 63.7 53.5 #\u0026gt; MOBILE AL 60.0 63.2 69.8 76.1 83.0 88.2 90.4 90.5 87.3 79.4 70.3 61.9 #\u0026gt; MONTGOMERY AL 57.4 61.8 69.7 76.6 84.0 89.8 92.1 91.9 87.3 78.3 69.0 59.6 #\u0026gt; ANCHORAGE AK 23.1 26.6 33.9 44.5 56.0 62.8 65.4 63.5 55.1 40.5 27.8 24.8 #\u0026gt; ANNETTE AK 41.6 42.7 44.9 50.2 56.3 61.1 64.3 64.7 59.3 51.6 44.6 41.5       Now calculate the mean temperature for each month. Based on the locations sampled, what month is the warmest overall? The coldest?\n  Hints (click here)  \nUse apply() to calculate the means for each column (columns are designated with \u0026lsquo;2\u0026rsquo;).\n   Solution (click here)  apply(temp_data, 2, mean) #\u0026gt; JAN FEB MAR APR MAY JUN JUL AUG  #\u0026gt; 44.35709 48.03156 55.75035 64.83936 73.33227 80.73582 84.88865 83.81418  #\u0026gt; SEP OCT NOV DEC  #\u0026gt; 77.35922 66.87234 55.59610 46.22589       Now calculate the mean temperature for each location. Which location has the warmest annual temperature? The coldest? Since there are a lot of results to sort through, consider using indexing to extract the warmest and coldest temperatures.\n  Hints (click here)  \nUse apply() to calculate the means for each row (rows are designated with \u0026lsquo;1\u0026rsquo;). Save the results to an object, and then use logical indexing in combination with the max() function to pull out the entry with the maximum value or min() to pull out the minimum value.\n   Solution (click here)  row_means \u0026lt;- apply(temp_data, 1, mean) row_means[row_means == max(row_means)] #\u0026gt; POHNPEI-CAROLINE IS. PC  #\u0026gt; 88.175 row_means[row_means == min(row_means)] #\u0026gt; BARROW AK  #\u0026gt; 17.18333       How many locations have a mean temp \u0026gt; 75F?\n  Hints (click here)  \nUse indexing like in the previous exercise. You can print the results, or use the length function to get the number returned.\n   Solution (click here)  row_means[row_means \u0026gt; 75] %\u0026gt;% length() #\u0026gt; [1] 68      Bonus  How many states or territories have at least one city in the dataset with a mean temp \u0026gt; 75F?\n  Hints (click here)  \nThe states or territories are given by the last 2 characters in the row names of the data frame (which become the names of the vector elements in the results of apply()). Extract the set of names, use a regular expression to pull out the last two characters from each (consider stringr::str_rep, or gsub()), then unique them to get each one that\u0026rsquo;s represented and find the length of that vector.\n   Solution (click here)  loc_names \u0026lt;- row_means[row_means \u0026gt; 75] %\u0026gt;% names() states \u0026lt;- stringr::str_replace(loc_names, \"(.+)(..)\", \"\\\\2\" ) unique_states \u0026lt;- unique(states) unique_states #\u0026gt; [1] \"AL\" \"AZ\" \"CA\" \"FL\" \"GA\" \"HI\" \"LA\" \"MS\" \"NV\" \"NM\" \"SC\" \"TX\" \"PC\" \"PR\" length(unique_states) #\u0026gt; [1] 14       Purrr: An Alternative (Tidy) Approach To apply() Functions There are tidy alternatives to the apply functions - they\u0026rsquo;re part of the purrr package, which we\u0026rsquo;ll explore in the next session. In the meantime, if you want a preview, you can find details on purrr here.\n ","date":1646697600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646937915,"objectID":"20465b33bf133673f38ef0b91ee031c0","permalink":"https://biodash.github.io/codeclub/s03e09_apply/","publishdate":"2022-03-08T00:00:00Z","relpermalink":"/codeclub/s03e09_apply/","section":"codeclub","summary":"In this session of Code Club, we'll consider the `apply()` family of functions, which can often be used as efficient alternatives to writing some of the loops we worked with in the previous session.","tags":null,"title":"Session S03E09: Functional Programming With Apply() functions","type":"codeclub"},{"authors":["Jelmer Poelstra"],"categories":null,"content":"\n Housekeeping New to Code Club? Check out the Code Club Computer Setup instructions, which also has pointers for if you\u0026rsquo;re new to R or RStudio.\nSession goals Today, you will learn:\n The basics of for loops. How to print and save output from for loops How to use if statements  R packages we will use ## This will _install_ the packages only if you don't already have them: if (!require(palmerpenguins)) install.packages(\"palmerpenguins\") if (!require(tidyverse)) install.packages(\"tidyverse\") if (!require(glue)) install.packages(\"glue\") ## This will _load_ the packages: library(palmerpenguins) library(tidyverse) library(glue)   I \u0026ndash; Introduction Last week, we learned that we should avoid copy-pasting our code when we want to repeat operations.\nWe also focused on one alternative approach: making use of R\u0026rsquo;s vectorization capabilities. When passed a vector, most functions will automatically be performed separately for all elements in the vector, and you don\u0026rsquo;t need to explicitly iterate over (e.g., loop over) these elements:\nc(2.5, 4.1, 7, 3.6) * 0.45 #\u0026gt; [1] 1.125 1.845 3.150 1.620  However, vectorization can only be applied to a specific set of problems. A more universal solution when you need to repeat operations is iteration. The two main iteration approaches in R involve loops and functionals.\nToday, we will focus on loops and over the next two weeks, Mike Sovic will teach us about functionals.\nIn R, functionals like apply and map are often preferred over loops because they are more elegant and succinct than loops \u0026ndash; but this can also make them harder to understand.\nLoops are a good place to start learning to use iteration because:\n  They make the iteration aspect explicit and are therefore more intuitive than functionals.\n  They can easily accommodate longer blocks of code without the need to also write your own function.\n  They are a near-universal feature in programming languages and are a very common sight in languages like Python and Bash, so they are good to familiarize yourself with.\n  Today, we will talk about the most common type of loop: the for loop. (Other types of loops in R are while loops and repeat loops \u0026ndash; I honestly never use those.)\n II \u0026ndash; for loop basics for loops iterate over a collection, such as a vector, allowing you to perform one or more actions for each element in the collection, one element at a time.\nThe basic syntax is as follows:\nfor (item in collection) { # ...code to do things for each element, one at a time... } The simple example below will help to understand the syntax:\n(Note that this example is so simple that vectorization would have also worked; we\u0026rsquo;ll move on to more realistic loop examples in the next section.)\n## A loop to print negated values: for (a_number in c(1, 2, 3, 4)) \u0026#123; # We iterate over 1, 2, 3, 4 print(a_number * -1) # Multiply each number by -1 \u0026#125; #\u0026gt; [1] -1 #\u0026gt; [1] -2 #\u0026gt; [1] -3 #\u0026gt; [1] -4  On the first line, we initialized the for loop, telling it to assign each item in the vector to the variable a_number, one at a time.\nThe code inside the loop (which is inside the {} braces and is indented) will run as many times as there are elements in the collection. Each time it runs, a_number contains a single value from the vector.\nIn other words, what you are instructing R to do is:\n# Iteration 1 a_number \u0026lt;- 1 print(a_number * -1) #\u0026gt; [1] -1 # Iteration 2 a_number \u0026lt;- 2 print(a_number * -1) #\u0026gt; [1] -2 # Iteration 3 a_number \u0026lt;- 3 print(a_number * -1) #\u0026gt; [1] -3 # Iteration 4 a_number \u0026lt;- 4 print(a_number * -1) #\u0026gt; [1] -4  ## Just printing the same loop for reference: for (a_number in c(1, 2, 3, 4)) \u0026#123; # We iterate over 1, 2, 3, 4 print(a_number * -1) # Multiply each number by -1 \u0026#125;    The name given to the variable that will contain one item at a time, here a_number, is an arbitrary choice, just like when you assign variables the regular way (a_number \u0026lt;- 1).\n  The collection is usually a vector or a list: one that already exists or which you create on the fly in the loop initialization line, like we did above with c(1, 2, 3, 4).\n  for, the parentheses (), in, and the curly braces {} are all fixed elements of for loops. Only the braces can be omitted in some cases, as we\u0026rsquo;ll see below.\n  The variable in the loop As mentioned, the variable name that we assign to is arbitrary: we could use anything, as long as we reference it with the same name inside the loop:\n## Example 1 with a different variable name: \"positive_number\" for (positive_number in c(1, 2, 3, 4)) \u0026#123; print(positive_number * -1) \u0026#125; #\u0026gt; [1] -1 #\u0026gt; [1] -2 #\u0026gt; [1] -3 #\u0026gt; [1] -4 ## Example 2 with a different variable name: \"i\" for (i in c(1, 2, 3, 4)) \u0026#123; print(i * -1) \u0026#125; #\u0026gt; [1] -1 #\u0026gt; [1] -2 #\u0026gt; [1] -3 #\u0026gt; [1] -4  Note also that we actually don\u0026rsquo;t have to use the variable we are looping over: we could also use a for loop as a roundabout way to simply repeat something as many times as there are values in our collection:\nfor (i in c(1, 2, 3, 4)) \u0026#123; print(\"Yes!\") # Print \"Yes!\" in each of our four iterations  \u0026#125; #\u0026gt; [1] \"Yes!\" #\u0026gt; [1] \"Yes!\" #\u0026gt; [1] \"Yes!\" #\u0026gt; [1] \"Yes!\"  Printing the output In a loop, we need to explicitly use the print() function if we want to print something to screen \u0026ndash; nothing will be printed if we omit this:\nfor (i in 1:4) \u0026#123; i * -1 \u0026#125;  In practice, you might often want to store the result in a variable rather than just printing it to screen, and we\u0026rsquo;ll see how to do that in the next section.\n A note on syntax: the curly braces are not strictly necessary for one-liners like this:\nfor (i in c(1, 2, 3, 4)) print(i * -1) #\u0026gt; [1] -1 #\u0026gt; [1] -2 #\u0026gt; [1] -3 #\u0026gt; [1] -4     Breakout Rooms I Copy and run this code to get set up! ## This will _install_ the packages only if you don't already have them: if (!require(palmerpenguins)) install.packages(\"palmerpenguins\") if (!require(tidyverse)) install.packages(\"tidyverse\") if (!require(glue)) install.packages(\"glue\") ## This will _load_ the packages: library(palmerpenguins) library(tidyverse) library(glue)    Exercise 1: First loops Loop over the numbers 5 through 10; and inside the loop, simply print each number to screen.\n(Of course, feel free to look at the examples above for reference \u0026ndash; but type out the loop to get the syntax under your fingers.)\n  Solution (click here)  for (i in 5:10) \u0026#123; print(i) \u0026#125; #\u0026gt; [1] 5 #\u0026gt; [1] 6 #\u0026gt; [1] 7 #\u0026gt; [1] 8 #\u0026gt; [1] 9 #\u0026gt; [1] 10   Loop over the first 8 penguin bill depth values (column bill_depth_mm in the penguins dataframe), and in each iteration of the loop, add 100 to the focal bill depth. Print the results to screen.\n  Hints on extracting the bill depths (click here)    One way to extract the bill_depth_mm column as a vector is using penguins$bill_depth_mm. (Another is penguins %\u0026gt;% pull(bill_depth_mm).)\n  One way to take the first 8 values of a vector my_vec is using my_vec[1:8]. (Another is my_vec %\u0026gt;% head(8).)\n     Solution (click here)  for (bill_dep in penguins$bill_depth_mm[1:8]) \u0026#123; print(bill_dep + 100) \u0026#125; #\u0026gt; [1] 118.7 #\u0026gt; [1] 117.4 #\u0026gt; [1] 118 #\u0026gt; [1] NA #\u0026gt; [1] 119.3 #\u0026gt; [1] 120.6 #\u0026gt; [1] 117.8 #\u0026gt; [1] 119.6      Exercise 2: Keeping track of the iteration number Create a vector with the first names of everyone in your breakout room group (e.g. first_names \u0026lt;- c(\u0026quot;Phillida\u0026quot;, \u0026quot;Ethan\u0026quot;, \u0026quot;Tonci\u0026quot;)), and a second vector with everyone\u0026rsquo;s last names.\nNext, use a loop to print your breakout room\u0026rsquo;s equivalent of the following:\nThe name of person 1 is Phillida Maas The name of person 2 is Ethan Hope The name of person 3 is Tonci Elwes    Hint 1 (click here)    To get the numbers (person 1 etc) and the corresponding first and last names, you\u0026rsquo;ll want to loop over a vector of numbers (indices), in this case 1:3. You can then extract the corresponding names inside the loop by indexing with this number (as in first_names[1]).\n  Ideally, you will determine the number of names in your room with code instead of just counting them manually \u0026ndash; you can do so using the length() function. That way, you could use the exact same code if you had a different number of people in your group.\n     Hint 2: code with blanks (click here)  Use the following code as a template, replacing the ... blanks:\nfirst_names \u0026lt;- c(\"Phillida\", \"Ethan\", \"Tonci\") last_names \u0026lt;- c(\"Maas\", \"Hope\", \"Elwes\") n_names \u0026lt;- ... # get the number of first or last names for (... in 1:...) \u0026#123; first_name \u0026lt;- ... last_name \u0026lt;- ... string \u0026lt;- glue(\"The name of person \u0026#123;...\u0026#125; is \u0026#123;...\u0026#125; \u0026#123;...\u0026#125;\") print(string) \u0026#125;     Solution (click here)  first_names \u0026lt;- c(\"Phillida\", \"Ethan\", \"Tonci\") last_names \u0026lt;- c(\"Maas\", \"Hope\", \"Elwes\") n_names \u0026lt;- length(first_names) # get the number of first or last names for (index in 1:n_names) \u0026#123; first_name \u0026lt;- first_names[index] last_name \u0026lt;- last_names[index] string \u0026lt;- glue(\"The name of person \u0026#123;index\u0026#125; is \u0026#123;first_name\u0026#125; \u0026#123;last_name\u0026#125;\") print(string) \u0026#125; #\u0026gt; The name of person 1 is Phillida Maas #\u0026gt; The name of person 2 is Ethan Hope #\u0026gt; The name of person 3 is Tonci Elwes      III \u0026ndash; Plots or files as output In the examples and exercises so far, we have simply printed some text output to screen. While this can be useful, in practice, you might often be interested in doing one of the following:\n  Printing a plot in each iteration of the loop\n  Saving results or a plot to file in each iteration of the loop\n  Creating a single R object (e.g. a dataframe) across the entire loop: in each iteration, you add one element (e.g. a row) to the object.\n  The latter scenario is especially verbose to do with a loop and moreover, the most intuitive way of doing that is unnecessarily slow (which is a key reason why loops in R have the reputation of being slow). I explain the way to store all loop output in one object in the bonus materials below but when you need to do this, a functional approach is particularly worth considering \u0026ndash; we\u0026rsquo;ll learn how to do so in the next few weeks.\nInstead, let\u0026rsquo;s take a look at creating plots and saving files in loops. We\u0026rsquo;ll also take this opportunity to explicitly see how we change a bit of code that does something once in order to use it in a loop.\nSay that we want to make a scatterplot of bill lengths vs. bill depths in all penguin species in the penguins dataframe. We\u0026rsquo;ll start with some code to make a scatterplot for one of the three species:\n## Select just the data for the focal species: one_penguin_df \u0026lt;- penguins %\u0026gt;% filter(species == \"Gentoo\") %\u0026gt;% # Select only rows with Gentoo penguins drop_na() # Remove rows with NAs ## Create the plot: ggplot(one_penguin_df) + geom_point(aes(x = bill_length_mm, y = bill_depth_mm, color = sex)) + labs(title = \"Gentoo\") + theme_minimal()   How can we adapt this code to run it for all three penguin species with a loop?\nSince we will loop over the species, we\u0026rsquo;ll start by saving the species names in a vector:\nall_penguin_species \u0026lt;- unique(penguins$species)  Next, we write the loop, looping over our vector of species names.\nThe code inside the loop is going to be nearly identical to the code above, except that we use the variable focal_species instead of the literal string \u0026ldquo;Gentoo\u0026rdquo; (that way, we make a plot for each species and not three plots for Gentoo Penguins):\nfor (focal_species in all_penguin_species) \u0026#123; ## Select just the data for the focal species: one_penguin_df \u0026lt;- penguins %\u0026gt;% filter(species == focal_species) %\u0026gt;% # Using the `focal_species` variable drop_na() ## Create the plot: p \u0026lt;- ggplot(one_penguin_df) + geom_point(aes(x = bill_length_mm, y = bill_depth_mm, color = sex)) + labs(title = focal_species) + # Using the `focal_species` variable theme_minimal() ## Print the plot: print(p) \u0026#125;   Note that the code above would be exactly the same regardless of whether we had 3 or all 18 species of penguins in the world in this dataframe.\nNow, let\u0026rsquo;s see an example where instead of just printing the plots, we wanted to save each plot in an appropriately named file:\nfor (focal_species in all_penguin_species) \u0026#123; ## Select just the data for the focal species: one_penguin_df \u0026lt;- penguins %\u0026gt;% filter(species == focal_species) %\u0026gt;% drop_na() ## Create the plot: p \u0026lt;- ggplot(one_penguin_df) + geom_point(aes(x = bill_length_mm, y = bill_depth_mm, color = sex)) + labs(title = focal_species) + theme_minimal() ## Save the plot filename \u0026lt;- glue(\"\u0026#123;focal_species\u0026#125;_bill-len_bill-dp.png\") ggsave(filename, p) # Save the plot to file! \u0026#125;  This would create the following png files:\n# Adelie_bill-len_bill-dp.png # Gentoo_bill-len_bill-dp.png # Chinstrap_bill-len_bill-dp.png  Similarly, it is straightforward to save a text file with results in each iteration of a loop:\nfor (focal_species in all_penguin_species) \u0026#123; ## Select just the data for the focal species: one_penguin_df \u0026lt;- penguins %\u0026gt;% filter(species == focal_species) %\u0026gt;% drop_na() ## Save the results in a tab-separated values (tsv) file: filename \u0026lt;- glue(\"\u0026#123;focal_species\u0026#125;.tsv\") write_tsv(one_penguin_df, filename) # Save the dataframe to file! \u0026#125;  This would create the following text files:\n# Adelie.tsv # Gentoo.tsv # Chinstrap.tsv   IV \u0026ndash; if statements if statements are similar in syntax to for loops, and are also considered a \u0026ldquo;control flow\u0026rdquo; structure. But their purpose is different from loops: instead of iterating, if statements do something once and they only do it when a condition is fulfilled.\nFor instance, we may want to check in a script whether a certain directory (folder) exists on our computer, and if it doesn\u0026rsquo;t, then we create the directory:\n## `!dir.exists()` will be `TRUE` if the directory doesn't already exist ## `!` inverts a logical, so the below says \"If the directory does _not_ exist\" if (!dir.exists(\"important_directory\")) \u0026#123; print(\"Creating new directory\") dir.create(\"important_directory\") \u0026#125; else \u0026#123; print(\"Dir already exists\") \u0026#125;  Inside the parentheses () after if, we should define a test that evaluates to either TRUE or FALSE. If the test evaluates to TRUE, whatever is inside the curly braces {} will be executed, and if it is FALSE, what is inside the curly braces will be ignored.\nWe can optionally add an else clause: what to do if the test evaluated to FALSE. (And for more complicated cases, else if clauses can add additional tests and actions.)\nif statements are commonly combined with for loops \u0026ndash; we may want to only execute the functions in our loop for items in our collection that fulfill a certain condition:\nfor (one_number in 1:10) \u0026#123; if (one_number \u0026gt; 7) \u0026#123; # Only `TRUE` for numbers \u0026gt;7 print(one_number) # Hence, this is only executed for numbers \u0026gt;7 \u0026#125; \u0026#125; #\u0026gt; [1] 8 #\u0026gt; [1] 9 #\u0026gt; [1] 10   Like in for loops, braces can be omitted in if statements in one-liners:\nfor (one_number in 1:10) if (one_number \u0026gt; 7) print(one_number) #\u0026gt; [1] 8 #\u0026gt; [1] 9 #\u0026gt; [1] 10     Breakout Rooms II  Exercise 3: A plotting loop Using a loop and the penguins dataframe, produce a separate scatterplot for each island with bill length vs. body weight, using different colors for the species and different shapes for the sexes.\nPrint each plot and save each plot to file.\n  Hints (click here)    The loop plotting example we went through above have almost the same code that you\u0026rsquo;ll need here, so take another look at that if you\u0026rsquo;re stuck.\n  Islands are in the island column and penguin sexes are in the sex column.\n  Use the shape aesthetic in ggplot2 to get different shapes for the sexes.\n     Solution (click here)  all_islands \u0026lt;- unique(penguins$island) for (focal_island in all_islands) \u0026#123; ## Select just the data for the focal island: one_island_df \u0026lt;- penguins %\u0026gt;% filter(island == focal_island) %\u0026gt;% drop_na() ## Create the plot: p \u0026lt;- ggplot(one_island_df) + geom_point(aes(x = bill_length_mm, y = bill_depth_mm, color = species, shape = sex)) + labs(title = focal_island) + theme_minimal() ## Save and print the plot filename \u0026lt;- glue(\"\u0026#123;focal_island\u0026#125;_bill-len_bill-dp.png\") ggsave(filename, p) print(p) \u0026#125;       Exercise 4: if in a loop In the previous exercise, you should have noticed that one island only has a single species of penguin.\nLet\u0026rsquo;s say that we don\u0026rsquo;t want to create this plot for islands with only a single species. Modify your loop from the previous exercise to include an if statement such that you will only make plots for islands with multiple species of penguins.\n  Hints (click here)  You\u0026rsquo;ll want to count the number of distinct species in the dataframe after filtering it to contain penguins for one island only. One way to do that is using length(unique(...)).\n   Solution (click here)  all_islands \u0026lt;- unique(penguins$island) for (focal_island in all_islands) \u0026#123; ## Select just the data for the focal island: one_island_df \u0026lt;- penguins %\u0026gt;% filter(island == focal_island) %\u0026gt;% drop_na() ## Check how many species there are: n_species \u0026lt;- length(unique(one_island_df$species)) if (n_species \u0026gt; 1) \u0026#123; ## Create the plot: p \u0026lt;- ggplot(one_island_df) + geom_point(aes(x = bill_length_mm, y = bill_depth_mm, color = species, shape = sex)) + labs(title = focal_island) + theme_minimal() ## Save and print the plot filename \u0026lt;- glue(\"\u0026#123;focal_island\u0026#125;_bill-len_bill-dp.png\") ggsave(filename, p) print(p) \u0026#125; \u0026#125;    Bonus: Add an else clause to print a message to screen that no plot will be created for the focal island.\n  Solution (click here)  all_islands \u0026lt;- unique(penguins$island) for (focal_island in all_islands) \u0026#123; ## Select just the data for the focal species: one_island_df \u0026lt;- penguins %\u0026gt;% filter(island == focal_island) %\u0026gt;% drop_na() ## Check how many species there are n_species \u0026lt;- length(unique(one_island_df$species)) if (n_species \u0026gt; 1) \u0026#123; ## Create the plot: p \u0026lt;- ggplot(one_island_df) + geom_point(aes(x = bill_length_mm, y = bill_depth_mm, color = species, shape = sex)) + labs(title = focal_island) + theme_minimal() ## Save and print the plot filename \u0026lt;- glue(\"\u0026#123;focal_island\u0026#125;_bill-len_bill-dp.png\") ggsave(filename, p) print(p) \u0026#125; else \u0026#123; print(glue(\"Not creating a plot for \u0026#123;focal_island\u0026#125;\")) \u0026#125; \u0026#125; #\u0026gt; Not creating a plot for Torgersen       Bonus Material Test operators and functions Common test operators and functions include:\n == and != to test for (in)equality for numbers and characters alike \u0026gt;, \u0026lt;, \u0026gt;= and \u0026lt;= for numeric comparisons %in% to test for \u0026ldquo;group membership\u0026rdquo; (see examples below) is.na() and is.null() to test for NA and NULL values, respectively Any test can be negated (inverted) with an exclamation mark !  Some examples:\n8 != 8 #\u0026gt; [1] FALSE 8 %in% c(5, 8, 10) # 8 is contained in the vector 5, 8, 10 #\u0026gt; [1] TRUE 8 %in% c(10:20) # 8 is not contained in the vector 10:20 #\u0026gt; [1] FALSE \"Adelie\" %in% c(\"Gentoo\", \"Adelie\") #\u0026gt; [1] TRUE !is.na(8) #\u0026gt; [1] TRUE  You can also combine tests with \u0026amp; (logical and) and | (logical or):\n# With `\u0026amp;`, TRUE is returned if both tests are TRUE: 8 == 8 \u0026amp; !is.na(8) #\u0026gt; [1] TRUE 8 == 8 \u0026amp; is.na(8) #\u0026gt; [1] FALSE # With `|`, TRUE is returned if at least one of the tests is TRUE:  8 == 8 | !is.na(8) #\u0026gt; [1] TRUE 8 == 8 | is.na(8) #\u0026gt; [1] TRUE 8 != 8 | is.na(8) #\u0026gt; [1] FALSE  Storing loop output If we want the output to be saved in an object of some kind, we need to explicitly make an assignment in each iteration of the loop. This is where we need to start paying attention to the design of our loop. Unless computational speed is of no concern, you should avoid growing an object in each iteration of the loop.\nFor example, you might be inclined to do the following if you wanted to compute and store the medians of each column in a data frame:\n## We initialize a vector in which we collect the column medians: column_medians \u0026lt;- vector() for (column_number in 3:6) \u0026#123; ## We extract one column using \"dataframe_name[[column_number]]\": column_median \u0026lt;- median(penguins[[column_number]], na.rm = TRUE) ## We add the single-column median to our vector of medians: column_medians \u0026lt;- c(column_medians, column_median) \u0026#125; column_medians #\u0026gt; [1] 44.45 17.30 197.00 4050.00  Similarly, if you were working with a data frame, you may be tempted to add a column (with cbind()) or a row (with rbind()) to the data frame in each iteration of the loop.\nThe problem with these approaches is that R has to create an entirely new object in each iteration of the loop, because the object\u0026rsquo;s memory requirements keep increasing.\nInstead, before you start the loop, you\u0026rsquo;ll want to give the final vector (here, column_medians) the appropriate size:\ncolumn_medians \u0026lt;- vector(length = length(3:6)) for (column_number in 3:6) \u0026#123; column_median \u0026lt;- median(penguins[[column_number]], na.rm = TRUE) column_medians[column_number] \u0026lt;- column_median \u0026#125; column_medians #\u0026gt; [1] 0.00 0.00 44.45 17.30 197.00 4050.00  Note that for very small problems, such as the example above, there will not be a noticeable difference in computing time between pre-assigning a properly sized object versus growing an object inside the loop. However, it is still good to get into the habit of pre-assigning an object of the right size.\nSome summary guidelines for loops, especially when you are working with large datasets and speed is an issue:\n Don\u0026rsquo;t use a loop when you can instead use vectorized operations. When you write a loop, avoid doing things inside the loop that don\u0026rsquo;t need to be repeated. Don\u0026rsquo;t grow objects inside the loop. Instead, pre-assign an object large enough to contain all output of the loop and fill it in inside the loop.  Further reading   The iteration chapter in Hadley Wickham\u0026rsquo;s R for Data Science (2017). Doesn\u0026rsquo;t really cover if statements.\n  Software Carpentry Control Flow lesson. Starts out with if statements.\n  ","date":1646265600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646345365,"objectID":"120e4ca0582d7e38dafd21cc62eb78ec","permalink":"https://biodash.github.io/codeclub/s03e08_loops/","publishdate":"2022-03-03T00:00:00Z","relpermalink":"/codeclub/s03e08_loops/","section":"codeclub","summary":"In this second session on strategies for repeating operations without copy-pasting your code, we will focus on for loops.","tags":["codeclub","iteration"],"title":"S03E08: For Loops and If Statements","type":"codeclub"},{"authors":["Jelmer Poelstra"],"categories":null,"content":"\n Housekeeping New to Code Club? Check out the Code Club Computer Setup instructions, which also has pointers for if you\u0026rsquo;re new to R or RStudio.\nSession goals Today, you will learn:\n That you should avoid copy-pasting your code Which alternatives exist in R What vectorization is and how to make use of it  R packages we will use We will use palmerpenguins for its penguins dataframe, the tidyverse for dataframe manipulation and plotting, and glue\u0026rsquo;s glue function to paste strings.\n## This will _install_ the packages only if you don't already have them: if (!require(palmerpenguins)) install.packages(\"palmerpenguins\") if (!require(tidyverse)) install.packages(\"tidyverse\") if (!require(glue)) install.packages(\"glue\") ## This will _load_ the packages: library(palmerpenguins) library(tidyverse) library(glue)   I \u0026ndash; Avoid copy-pasting code Don\u0026rsquo;t Repeat Yourself Sometimes, you have a bit of code, and you need to repeat the operations in that code almost exactly.\nThis can apply to anywhere from a single line to dozens of lines of code. For instance, you may want to rerun a statistical model with different parameter values, or repeat an analysis for different batches or subsets of samples. In the context of our trusty penguins dataset, we may want to repeat an analysis for each of the 4 morphological measurements taken for each penguin.\nYour first instinct is perhaps to copy-paste your code several times, and make the necessary slight adjustments in each instance. There are problems with this approach, including:\n  You will end up with a lot of code, reducing clarity and making it more error-prone\n  Making changes to the parts of the code shared by all blocks becomes challenging.\n  Avoiding such code repetition is where the programming mantra \u0026ldquo;Don\u0026rsquo;t Repeat Yourself\u0026rdquo; (\u0026ldquo;DRY\u0026rdquo;) comes from.\nAlternatives So, what are the alternatives?\nIn R, two key approaches that allow you to avoid copy-pasting your code both involve iteration to repeat a procedure, and do so either:\n  Using a loop\n  Using \u0026ldquo;functional programming\u0026rdquo;: apply a function multiple times with special functions (\u0026ldquo;functionals\u0026rdquo;) from the base R apply-family or purrr\u0026rsquo;s map-family.\n  Loops are especially useful if you have a whole block of code that needs to be rerun, while functionals are easier to apply when you need to rerun a single function call.\nYou can additionally avoid code repetition by:\n  Writing your own functions (using arguments to make them flexible)\n  In simple cases, making use of R\u0026rsquo;s vectorization capabilities.\n  These approaches are clearer, less error-prone, and more flexible than copy-pasting code. They can also be combined.\nWe will tackle all of these approaches in Code Club in the upcoming weeks, starting with vectorization this week.\nBut first, an iteration example Below, I will give a quick example of each of the two iteration approaches: a loop and a functional. Hopefully, this will be illustrative even if you don\u0026rsquo;t understand all the details: come back in the next few weeks to learn more about it!\nSay that we wanted to compute the mean for each of the 4 measurements taken for each penguin: bill length, bill depth, flipper length, and body mass.\nhead(penguins) #\u0026gt; # A tibble: 6 × 8 #\u0026gt; species island bill_length_mm bill_depth_mm flipper_length_… body_mass_g sex  #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; #\u0026gt; 1 Adelie Torge… 39.1 18.7 181 3750 male  #\u0026gt; 2 Adelie Torge… 39.5 17.4 186 3800 fema… #\u0026gt; 3 Adelie Torge… 40.3 18 195 3250 fema… #\u0026gt; 4 Adelie Torge… NA NA NA NA NA  #\u0026gt; 5 Adelie Torge… 36.7 19.3 193 3450 fema… #\u0026gt; 6 Adelie Torge… 39.3 20.6 190 3650 male  #\u0026gt; # … with 1 more variable: year \u0026lt;int\u0026gt;  First, let\u0026rsquo;s see how we can do this for one measurement:\n## We extract a vector of bill lengths from the penguins dataframe with `$` ## Note that any NAs would cause the mean to be NA without na.rm=TRUE mean(penguins$bill_length_mm, na.rm = TRUE) #\u0026gt; [1] 43.92193 ## Among other options, we could also extract this 3rd column using `[[`: mean(penguins[[3]], na.rm = TRUE) #\u0026gt; [1] 43.92193  (For an overview of base R data frame indexing, see the bottom of the the page.)\nIf we would simply repeat this procedure using the first syntax four times, we would write:\nmean(penguins$bill_length_mm, na.rm = TRUE) #\u0026gt; [1] 43.92193 mean(penguins$bill_depth_mm, na.rm = TRUE) #\u0026gt; [1] 17.15117 mean(penguins$flipper_length_mm, na.rm = TRUE) #\u0026gt; [1] 200.9152 mean(penguins$body_mass_g, na.rm = TRUE) #\u0026gt; [1] 4201.754  But that is a bit repetitive. And it would get especially repetitive if we had 20 different measurements. Or if, instead of just computing the mean, we wanted to perform an analysis consisting of multiple steps.\nHow would using iteration in a case like this look like?\n  With a for loop:\n## The columns we are interested in are columns 3 through 6 (3:6) ## We can extract each column with the `[[...]]` notation we saw last week for (column_index in 3:6) \u0026#123; column_mean \u0026lt;- mean(penguins[[column_index]], na.rm = TRUE) print(column_mean) \u0026#125; #\u0026gt; [1] 43.92193 #\u0026gt; [1] 17.15117 #\u0026gt; [1] 200.9152 #\u0026gt; [1] 4201.754    With purrr\u0026rsquo;s map() function:\npenguins %\u0026gt;% select(3:6) %\u0026gt;% map(mean, na.rm = TRUE) #\u0026gt; $bill_length_mm #\u0026gt; [1] 43.92193 #\u0026gt;  #\u0026gt; $bill_depth_mm #\u0026gt; [1] 17.15117 #\u0026gt;  #\u0026gt; $flipper_length_mm #\u0026gt; [1] 200.9152 #\u0026gt;  #\u0026gt; $body_mass_g #\u0026gt; [1] 4201.754     In this simple example where we are working with a dataframe, a specialized dplyr approach with across() also works:\npenguins %\u0026gt;% summarise(across(3:6, mean, na.rm = TRUE)) #\u0026gt; # A tibble: 1 × 4 #\u0026gt; bill_length_mm bill_depth_mm flipper_length_mm body_mass_g #\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 43.9 17.2 201. 4202.    What about vectorization? While iteration using loops or functionals is very useful, in R, we don\u0026rsquo;t need to use these strategies as much as in other languages. The main reason for this is that R often makes use of vectorization.\nTo illustrate vectorization, we\u0026rsquo;ll work with a vector of bill lengths that we extract from the penguins dataframe (though as we\u0026rsquo;ll see later, all of this works in dataframes, too):\n## Remove rows with NAs: penguins_noNA \u0026lt;- drop_na(penguins) ## Extract a column with `$`, then take the first 10 values:  bill_len \u0026lt;- penguins_noNA$bill_length_mm[1:10] bill_len #\u0026gt; [1] 39.1 39.5 40.3 36.7 39.3 38.9 39.2 41.1 38.6 34.6  Say that we wanted to convert each value in the bill_len vector from millimeters to inches. Would we need to multiply each individual value by 0.0393701?\nbill_len[1] * 0.0393701 #\u0026gt; [1] 1.539371 bill_len[2] * 0.0393701 #\u0026gt; [1] 1.555119 bill_len[3] * 0.0393701 #\u0026gt; [1] 1.586615 # And so on...  Or should we resort to a loop or a map()-type function here? Fortunately, none of this is necessary! You may already know that in R, you can simply do:\nbill_len * 0.0393701 #\u0026gt; [1] 1.539371 1.555119 1.586615 1.444883 1.547245 1.531497 1.543308 1.618111 #\u0026gt; [9] 1.519686 1.362205  Similarly, say that we wanted to log-transform every value in the vector, then we can just use the log() function once for the entire vector:\nlog(bill_len) #\u0026gt; [1] 3.666122 3.676301 3.696351 3.602777 3.671225 3.660994 3.668677 3.716008 #\u0026gt; [9] 3.653252 3.543854  If you knew about this, perhaps you didn\u0026rsquo;t even think of much of it? Actually, it is worth dwelling on this capability, which is called vectorization and is a pretty unique feature of the R language. In many other languages, you would in fact write a loop to transform each individual value.\nSo let\u0026rsquo;s learn a bit more about vectorization.\n II \u0026ndash; Vectorization patterns A vector and a \u0026ldquo;scalar\u0026rdquo; When we multiplied the value 0.0393701 with the vector bill_len, 0.0393701 was automatically recycled as many times as needed to be multiplied with each individual value in the bill_len vector.\nbill_len #\u0026gt; [1] 39.1 39.5 40.3 36.7 39.3 38.9 39.2 41.1 38.6 34.6 bill_len * 0.0393701 #\u0026gt; [1] 1.539371 1.555119 1.586615 1.444883 1.547245 1.531497 1.543308 1.618111 #\u0026gt; [9] 1.519686 1.362205  (A single value like 0.0393701 is often called a \u0026ldquo;scalar\u0026rdquo; or a variable, but in R it is really a vector of length 1.)\nAs mentioned, you would write a loop to do this in many other languages, and in fact, under the hood, R also uses a loop to do this!\nVectorization is very useful for two reasons:\n  You don\u0026rsquo;t have to write the loop (or another iteration construct), which saves you a fair bit of typing and makes the code clearer.\n  The under-the-hood-loop is being executed much faster than a loop that you would write with R code, because it is written in C/C++.\n  Vectors of equal length We can also use vectorized operations when both vectors contain multiple items. For instance, say we want to get the ratio of bill length to bill depth for each penguin:\n## Like above with bill length, we create a vector with 10 bill depths: bill_dp \u0026lt;- penguins_noNA$bill_depth_mm[1:10] ## We compute the ratio: bill_len / bill_dp #\u0026gt; [1] 2.090909 2.270115 2.238889 1.901554 1.907767 2.185393 2.000000 2.335227 #\u0026gt; [9] 1.820755 1.639810 bill_len #\u0026gt; [1] 39.1 39.5 40.3 36.7 39.3 38.9 39.2 41.1 38.6 34.6 bill_dp #\u0026gt; [1] 18.7 17.4 18.0 19.3 20.6 17.8 19.6 17.6 21.2 21.1  What happened here is that the first value is bill_len was divided by the first value in bill_dp, the second value in bill_len by the second value in bill_dp, and so forth.\nThis also works directly for the columns of a data frame:\nbill_ratio \u0026lt;- penguins$bill_length_mm / penguins$bill_depth_mm head(bill_ratio) #\u0026gt; [1] 2.090909 2.270115 2.238889 NA 1.901554 1.907767  In the above examples, both vectors had the same length. In Exercise 1, you\u0026rsquo;ll see that vectorization also works with two vectors with multiple values that differ in length.\nVectorized functions Above, we already briefly saw that we can simply pass a vector to the log() function and it will compute the log for each of them and return a vector of the same length.\nSo, the log() function works the same regardless of whether you pass a single value or a vector with multiple values:\nlog(21) #\u0026gt; [1] 3.044522 log(bill_len) #\u0026gt; [1] 3.666122 3.676301 3.696351 3.602777 3.671225 3.660994 3.668677 3.716008 #\u0026gt; [9] 3.653252 3.543854  Because in R, a single value like 21 is really a vector of length 1, this behavior makes sense.\nJust remember that for most functions, you do really need to pass a vector and not just a sequence of numbers:\n## This way, log() thinks you are passing 3 separate arguments: log(10, 15, 20) #\u0026gt; Error in log(10, 15, 20): unused argument (20) ## Now, you pass 1 argument which is a vector created with `c()` log(c(10, 15, 20)) #\u0026gt; [1] 2.302585 2.708050 2.995732  There are many other vectorized functions that will transform each value in a vector, such as round() (rounding numbers) and abs() (taking absolute numbers).\nOther vectorized functions summarize a vector into a single value, such as sum() and mean().\n Breakout Rooms I   Code to get set up (click here)  ## This will _install_ the packages only if you don't already have them: if (!require(palmerpenguins)) install.packages(\"palmerpenguins\") if (!require(tidyverse)) install.packages(\"tidyverse\") if (!require(glue)) install.packages(\"glue\") ## Load the packages we will use library(palmerpenguins) library(tidyverse) library(glue) ## Create a vector of bill lengths penguins_noNA \u0026lt;- drop_na(penguins) bill_len \u0026lt;- penguins_noNA$bill_length_mm[1:10]    Exercise 1: Unequal length Vectorization also works when two vectors with multiple elements do not have the same length. For instance, in the example below, we divide the first value by 10, the second by 100, the third again by 10, and so on:\nbill_len / c(10, 100) #\u0026gt; [1] 3.910 0.395 4.030 0.367 3.930 0.389 3.920 0.411 3.860 0.346  Given the length of bill_len (which is 10), do you see any issues if you would divide by a vector of length 3? Try it out and see what happens.\n  Hints (click here)  While 10 is a multiple of 2, it is not a multiple of 3. This means that the shorter vector will not be recycled in its entirety the last time around.\n   Solution (click here)  R will perform the operation but issue a warning about it:\nbill_len / c(10, 100, 1000) #\u0026gt; Warning in bill_len/c(10, 100, 1000): longer object length is not a multiple of shorter object length #\u0026gt; [1] 3.9100 0.3950 0.0403 3.6700 0.3930 0.0389 3.9200 0.4110 0.0386 3.4600   Negate every other value in the bill_len vector.\n  Hints (click here)    Negation means turning a positive value into a negative value and vice versa (e.g. 3 =\u0026gt; -3 and -15 =\u0026gt; 15).\n  You can leave the other values unchanged simply by multiplying them by 1.\n     Solution (click here)  bill_len * c(1, -1) #\u0026gt; [1] 39.1 -39.5 40.3 -36.7 39.3 -38.9 39.2 -41.1 38.6 -34.6      Exercise 2: Strings The glue function from the package of the same name allows you to combine literal strings with values or strings contained in R objects. For instance:\nisland \u0026lt;- \"Biscoe\" glue(\"The island of \u0026#123;island\u0026#125;\") #\u0026gt; The island of Biscoe  So, you combine both literal strings and R objects in a single quoted string, and access the values of R objects using braces {}.\nExtract the names of the three islands contained in the penguins dataframe, and save them in an vector called islands.\n  Hints (click here)  Use the unique() function to get a \u0026ldquo;deduplicated\u0026rdquo; vector of islands, i.e. with one entry per island.\n   Solution (click here)  islands \u0026lt;- unique(penguins$island) islands #\u0026gt; [1] Torgersen Biscoe Dream  #\u0026gt; Levels: Biscoe Dream Torgersen ## Or tidyverse style: islands \u0026lt;- penguins %\u0026gt;% distinct(island) %\u0026gt;% pull(island)  Note: it is fine that islands is still a factor, like the island column in penguins was.\n Make use of vectorization to print each island\u0026rsquo;s name like so:\n#\u0026gt; The island of Torgersen #\u0026gt; The island of Biscoe #\u0026gt; The island of Dream    Solution (click here)  glue(\"Island of \u0026#123;islands\u0026#125;\") #\u0026gt; Island of Torgersen #\u0026gt; Island of Biscoe #\u0026gt; Island of Dream      III \u0026ndash; Vectorization with logical indices We can also use vectorized solutions when we want to operate only on elements that satisfy a certain condition. To do so, we make use of R\u0026rsquo;s ability to index a vector with a logical vector.\nLet\u0026rsquo;s say we don\u0026rsquo;t trust any bill length measurement of over 40 mm, and we want to remove those from our vector.\nFirst, we need to know that statements with a comparison operator like \u0026gt;, \u0026lt;, or == will test each value and return a logical vector with the results.\nFor example:\n# The resulting vector contains TRUE or FALSE for each entry in the original vector: bill_len == 39.1 #\u0026gt; [1] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE bill_len #\u0026gt; [1] 39.1 39.5 40.3 36.7 39.3 38.9 39.2 41.1 38.6 34.6  Or, going back to our example with values \u0026gt;40:\nbill_len \u0026gt; 40 #\u0026gt; [1] FALSE FALSE TRUE FALSE FALSE FALSE FALSE TRUE FALSE FALSE  When we index the original vector with such a logical vector (sometimes referred to as a mask), we only get the TRUEs, i.e. values \u0026gt;40:\nbill_len[bill_len \u0026gt; 40] #\u0026gt; [1] 40.3 41.1  This is very succinct and powerful!\nWith a similar strategy, you can also retain all elements of the vector but manipulate some of them:\n## We create a separate vector so we don't change the original one: bill_len_ed \u0026lt;- bill_len ## Only change values \u0026gt; 40: bill_len_ed[bill_len \u0026gt; 40] \u0026lt;- bill_len_ed[bill_len \u0026gt; 40] - 100 bill_len_ed #\u0026gt; [1] 39.1 39.5 -59.7 36.7 39.3 38.9 39.2 -58.9 38.6 34.6  But for those kinds of operations, the vectorized ifelse() function is easier and clearer:\n# ifelse(test, return-this-if-true, return-this-if-false) bill_len_ed \u0026lt;- ifelse(test = bill_len \u0026gt; 40, yes = bill_len - 100, no = bill_len) bill_len_ed #\u0026gt; [1] 39.1 39.5 -59.7 36.7 39.3 38.9 39.2 -58.9 38.6 34.6   When creating logical vectors, the any() and all() functions are very handy.\nFor instance, say we had a vector of p-values:\npvals \u0026lt;- c(0.06, 0.048, 0.01, 0.73)  To check whether any of the p-values are significant:\nany(pvals \u0026lt; 0.05) #\u0026gt; [1] TRUE  To check whether all of the p-values are significant:\nall(pvals \u0026lt; 0.05) #\u0026gt; [1] FALSE  Moreover, because TRUE corresponds to 1 and FALSE to 0, you can also directly count the number of elements that satisfy a condition:\nsum(pvals \u0026lt; 0.05) #\u0026gt; [1] 2     Breakout Rooms II  Exercise 3: Logical vectors Create a vector bill_len_NA where all values \u0026gt; 40 have been turned into NAs.\n  Solution (click here)  ## Using logical vector subsetting: bill_len_NA \u0026lt;- bill_len bill_len_NA[bill_len_NA \u0026gt; 40] \u0026lt;- NA bill_len_NA #\u0026gt; [1] 39.1 39.5 NA 36.7 39.3 38.9 39.2 NA 38.6 34.6 ## Or, using `ifelse()`: bill_len_NA \u0026lt;- ifelse(bill_len \u0026gt; 40, NA, bill_len) bill_len_NA #\u0026gt; [1] 39.1 39.5 NA 36.7 39.3 38.9 39.2 NA 38.6 34.6   Remove all NAs from bill_len_NA. (If you don\u0026rsquo;t know the function to identify NAs in a vector, take a look at the Hints.)\n  Hints (click here)    The function is.na() will check which values in a vector are NAs: it returns a logical vector with TRUEs for NA values and FALSEs for non-NA values.\n  Since you want to remove NA values, you need to negate the output of the is.na() function when subsetting. You can negate logical tests in R with a !. So, !is.na() would have TRUE for non-NA values, which would allow you to keep them.\n     Solution (click here)  bill_len_NA[!is.na(bill_len_NA)] #\u0026gt; [1] 39.1 39.5 36.7 39.3 38.9 39.2 38.6 34.6      Exercise 4: ifelse() plot With ggplot, make a geom_point() plot of bill_length_mm versus bill_depth_mm only for Gentoo Penguins. In this plot, highlight penguins with a bill length to bill depth ratio larger than 3.5 by giving those points a different color.\n(Don\u0026rsquo;t hesitate to look at the Hints if you\u0026rsquo;re not sure how to approach this.)\n  Hints 1  (click here)   Create a new dataframe with:  Just Gentoo penguins (use the function filter()) No NAs (use the function drop_na()) A new column with a logical vector indicating whether the bill length to bill depth ratio is \u0026gt;3.5 (e.g., use the mutate() function with an ifelse() statement).   When creating the plot, assign the new column to the color aesthetic.  An alternative: you don\u0026rsquo;t even need to create the logical-vector-column, you could also directly map the color aesthetic to a logical expression!\n   Hints 2  (click here)   Here is some example skeleton code for the data processing:  ... \u0026lt;- penguins %\u0026gt;% ## Only retain rows for 1 penguin species: filter(species == ...) %\u0026gt;% ## Remove rows with NAs: ... %\u0026gt;% ## Create a new column `ratio` with a logical vector: mutate(ratio = ifelse(..., \"\u0026gt; 3.5\", \"\u0026lt; 3.5\"))   Here is some example skeleton code for the plot:  ggplot(...) + ## Use the new column with the logical vector for the `color` aesthetic geom_point(aes(x = ..., y = ..., color = ...))     Solution (click here)  ## Create the new dataframe gent \u0026lt;- penguins %\u0026gt;% filter(species == \"Gentoo\") %\u0026gt;% drop_na() %\u0026gt;% mutate( ratio = ifelse(bill_length_mm / bill_depth_mm \u0026gt; 3.5, \"\u0026gt; 3.5\", \"\u0026lt; 3.5\") ) ## Make the plot: ggplot(gent) + geom_point(aes(x = bill_length_mm, y = bill_depth_mm, color = ratio))   Or include the comparison directly in the ggplot call (!):\n## Create the new dataframe without making a new variable gent \u0026lt;- penguins %\u0026gt;% filter(species == \"Gentoo\") %\u0026gt;% drop_na() ## Make the plot and include the logical expression in the `aes()` call: ggplot(gent) + geom_point(aes(x = bill_length_mm, y = bill_depth_mm, color = bill_length_mm / bill_depth_mm \u0026gt; 3.5)) + labs(color = \"Bill length ratio \u0026gt; 3.5\")       Bonus Matrix vectorization We can also perform vectorized operations on entire matrices. With the following matrix:\n## We use the \"sample\" function to get 25 random values between 1 and a 100, ## and put those in a 5*5 matrix: mat \u0026lt;- matrix(sample(1:100, 25), nrow = 5, ncol = 5) mat #\u0026gt; [,1] [,2] [,3] [,4] [,5] #\u0026gt; [1,] 53 47 52 31 16 #\u0026gt; [2,] 13 83 4 37 34 #\u0026gt; [3,] 17 89 40 30 20 #\u0026gt; [4,] 60 81 98 66 90 #\u0026gt; [5,] 36 58 91 19 82  \u0026hellip;we could multiple all values by 10 or get the square of each value simply as follows:\nmat * 10 #\u0026gt; [,1] [,2] [,3] [,4] [,5] #\u0026gt; [1,] 530 470 520 310 160 #\u0026gt; [2,] 130 830 40 370 340 #\u0026gt; [3,] 170 890 400 300 200 #\u0026gt; [4,] 600 810 980 660 900 #\u0026gt; [5,] 360 580 910 190 820 mat * mat #\u0026gt; [,1] [,2] [,3] [,4] [,5] #\u0026gt; [1,] 2809 2209 2704 961 256 #\u0026gt; [2,] 169 6889 16 1369 1156 #\u0026gt; [3,] 289 7921 1600 900 400 #\u0026gt; [4,] 3600 6561 9604 4356 8100 #\u0026gt; [5,] 1296 3364 8281 361 6724  Base R data frame indexing Extract a column as a vector:\n## By name: penguins$species penguins[[\"species\"]] ## By index (column number): penguins[[1]]  Extract one or more columns as a data frame using [row, column] notation,\nwith a leading comma ([, column]) meaning all rows:\n## By name: penguins[, \"species\"] penguins[, c(\"species\", \"island\")] ## By index (column numbers): penguins[, 1] penguins[, c(1, 2)]   Subset rows by a condition, with a trailing comma ([row, ]) meaning all columns:\npenguins[penguins$species == \"Adelie\", ] penguins[penguins$bill_length_mm \u0026gt; 40, ]  ","date":1645574400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645661363,"objectID":"0403b68b4d512bb7704a52031b695b1b","permalink":"https://biodash.github.io/codeclub/s03e07_vectorization/","publishdate":"2022-02-23T00:00:00Z","relpermalink":"/codeclub/s03e07_vectorization/","section":"codeclub","summary":"In this first session on strategies for repeating operations without copy-pasting your code, we will focus on vectorization.","tags":["codeclub","iteration"],"title":"S03E07: Avoid Copy-pasting Code - Intro and Vectorization","type":"codeclub"},{"authors":["Michael Broe"],"categories":null,"content":" New To Code Club?   First, check out the Code Club Computer Setup instructions, which also has some pointers that might be helpful if you\u0026rsquo;re new to R or RStudio.\n  Please open RStudio before Code Club to test things out \u0026ndash; if you run into issues, join the Zoom call early and we\u0026rsquo;ll troubleshoot.\n   Session Goals  Learn the uses of base-R\u0026rsquo;s three subsetting operators: [ ], [[ ]], and $. Learn how the behavior of these operators varies depending on the data structure you are subsetting (vector, list, or data frame). Learn the value of the str() command. Learn how these base-R operators relate to tidyverse commands.   In our previous set of Code Clubs on stats packages, we\u0026rsquo;ve encountered data structures of various kinds. Here we put the data structure material all in one place, and put it in a wider R context. We\u0026rsquo;ll move below and beyond the tidyverse to get an overview of accessing various kinds of data structure in R.\nIntro: What is \u0026lsquo;subsetting\u0026rsquo; anyway? Subsetting (also known as indexing) is simply a way of using base-R syntax to extract specific pieces of a data structure.\nWe\u0026rsquo;ve already seen two dplyr verbs that perform this kind of operation: filter (to extract specific rows) and select (to extract specific columns).\nBut these are tidyverse commands, and only work with data frames.\nR has two more basic data structures, from which everything else is built, vectors and lists, and for these we need different subsetting operators (dplyr functions won\u0026rsquo;t work). We\u0026rsquo;ll also see that data frames are just a special kind of list, and that base-R subsetting operators also work for these, which can often be useful and efficient.\nIn your R experience you will almost certainly come across both code and output which does not adhere to tidyverse conventions. We have already come across outputs which are not data frames in SO3E01: t-tests and S03E02: ANOVA which we will revisit below.\nSince the behavior of these operators depends on the actual data structure you are working with, it\u0026rsquo;s useful when experimenting to use them in conjunction with the str() function, which compactly displays the internal structure of an any R object. A knowledge of the make-up of these data structures is also important when you come to write your own loops, iterations, and functions.\nThe most important distinction between vectors and lists is within vectors every value must be of the same type: for example, all characters, or all integers, etc. Inside lists, you can mix values of any type.\nIn addition, a list is best thought of as a general purpose container, which can contain not just mixed values, but also entire vectors of any type.\nSince we\u0026rsquo;ll be comparing base-R with tidyverse functions, we need to load the tidyverse, and we\u0026rsquo;ll also be using the palmerpenguins package to reproduce our previous ANOVA results:\nlibrary(tidyverse) library(palmerpenguins)   Vectors A vector is absolutely the most basic data structure in R. Every value in a vector must be of the same type.\nThere are four basic types of vector: integer, double, character, and logical. Vectors are created by hand with the c() (combine, concatenate) function. We can check the type with the typeof() operator. This is totally redundant if you just created the vector yourself, but when you are debugging code, or creating a vector using an expression, you might want to check exactly what type of vector is being used:\nvec_dbl \u0026lt;- c(1, 2, 3, 4, 5) vec_dbl #\u0026gt; [1] 1 2 3 4 5 typeof(vec_dbl) #\u0026gt; [1] \"double\"  vec_seq \u0026lt;- seq(1, 5) vec_seq #\u0026gt; [1] 1 2 3 4 5 typeof(vec_seq) #\u0026gt; [1] \"integer\"  vec_which \u0026lt;- 1:5 vec_which #\u0026gt; [1] 1 2 3 4 5 typeof(vec_which) #\u0026gt; [1] \"integer\"  vec_chr \u0026lt;- c(\"a\", \"b\", \"c\", \"d\", \"e\") vec_chr #\u0026gt; [1] \"a\" \"b\" \"c\" \"d\" \"e\" typeof(vec_chr) #\u0026gt; [1] \"character\"  Vectors have an insanely simple structure:\nstr(vec_dbl) #\u0026gt; num [1:5] 1 2 3 4 5  str() also displays the type, and RStudio displays the result of str() in the Values pane. (Note that \u0026lsquo;double\u0026rsquo; and \u0026lsquo;num(eric)\u0026rsquo; mean exactly the same thing in R.)\nFor such a simple structure, there are a surprisingly large number of ways to subset a vector. We\u0026rsquo;ll just look a a small sample here, and use the following example:\nx \u0026lt;- c(2.1, 4.2, 3.3, 5.4) x #\u0026gt; [1] 2.1 4.2 3.3 5.4 str(x) #\u0026gt; num [1:4] 2.1 4.2 3.3 5.4  (Notice for this example we are using a pedagocial trick, where the number after the decimal point indicates the position (index) of the value before the decimal point).\nPositive integers return elements at the specified positions. Any expression that evaluates to a vector of positions can be used as the index. The index operator is [ ]:\nx[3] #\u0026gt; [1] 3.3 x[c(3, 1)] #\u0026gt; [1] 3.3 2.1 x[2:4] #\u0026gt; [1] 4.2 3.3 5.4  Negative integers exclude elements at the specified positions:\nx[-3] #\u0026gt; [1] 2.1 4.2 5.4 x[c(-3, -1)] #\u0026gt; [1] 4.2 5.4  The bottom line here is that each value in a vector has an implicit index (position), and we can use that index to pull out values of interest. This can be extremely useful when writing for-loops that move through a vector accessing one value at a time.\nAttributes. One of the unusual features of R as opposed to other programming languages is that you can assign metadata of various kinds to the elements of vectors (and lists). For example, we can assign a name to each element, and then use a character vector as the index expression:\ny \u0026lt;- c(\"a\" = 2.1, \"b\" = 4.2, \"c\" = 3.3, \"d\" = 5.4) str(y) #\u0026gt; Named num [1:4] 2.1 4.2 3.3 5.4 #\u0026gt; - attr(*, \"names\")= chr [1:4] \"a\" \"b\" \"c\" \"d\" y[c(\"d\", \"c\", \"a\")] #\u0026gt; d c a  #\u0026gt; 5.4 3.3 2.1  The str() command now shows us that we now have a \u0026lsquo;Named\u0026rsquo; numeric vector, and that we have a \u0026ldquo;names\u0026rdquo; attribute, which is itself a character vector.\nExercise 1a Consider the words \u0026ldquo;yellow\u0026rdquo;, \u0026ldquo;red\u0026rdquo;, and \u0026ldquo;green\u0026rdquo;.\nCreate a numeric vector called \u0026ldquo;lengths\u0026rdquo; which simply shows the length of the words (in that order).\nLook at its structure using str().\nExtract the first and last elements of this vector, indexing by position.\n  Solution (click here)  lengths \u0026lt;- c(6, 3, 5) str(lengths) #\u0026gt; num [1:3] 6 3 5 lengths[c(1, 3)] #\u0026gt; [1] 6 5    Exercise 1b Now create a second vector called \u0026ldquo;named_lengths\u0026rdquo;, with the same word-lengths, but now also using a corresponding names attribute: \u0026ldquo;yellow\u0026rdquo;, \u0026ldquo;red\u0026rdquo; and \u0026ldquo;green\u0026rdquo;.\nLook at its structure using str().\nAgain, extract the first and last elements, but now using a character vector as the index.\n  Solution (click here)  named_lengths \u0026lt;- c(\"yellow\" = 6, \"red\" = 3, \"green\" = 5) str(named_lengths) #\u0026gt; Named num [1:3] 6 3 5 #\u0026gt; - attr(*, \"names\")= chr [1:3] \"yellow\" \"red\" \"green\" named_lengths[c(\"yellow\", \"green\")] #\u0026gt; yellow green  #\u0026gt; 6 5     Lists There are two main differences between vectors and lists: (i) lists can contain elements of different types; and (ii) lists can contain entire vectors as elements (and even other lists: which is why lists are sometimes referred to as recursive: it can be lists of lists of lists, \u0026lsquo;all the way down\u0026rsquo;. This is a topic for another day!).\nLet\u0026rsquo;s directly compare the structure of a list of numbers to a vector of numbers. Just as we create vectors by hand with the c() function, we create lists with the list() function.\nl \u0026lt;- list(2.1, 4.2, 3.3, 5.4) l #\u0026gt; [[1]] #\u0026gt; [1] 2.1 #\u0026gt;  #\u0026gt; [[2]] #\u0026gt; [1] 4.2 #\u0026gt;  #\u0026gt; [[3]] #\u0026gt; [1] 3.3 #\u0026gt;  #\u0026gt; [[4]] #\u0026gt; [1] 5.4 str(l) #\u0026gt; List of 4 #\u0026gt; $ : num 2.1 #\u0026gt; $ : num 4.2 #\u0026gt; $ : num 3.3 #\u0026gt; $ : num 5.4  Notice the difference between printing a list (all those brackets!!) and using the str() command, which is much more compact and readable.\nWhat if we mix values of different types?\nl_mixed \u0026lt;- list(2.1, 2L, T, \"a\") l_mixed #\u0026gt; [[1]] #\u0026gt; [1] 2.1 #\u0026gt;  #\u0026gt; [[2]] #\u0026gt; [1] 2 #\u0026gt;  #\u0026gt; [[3]] #\u0026gt; [1] TRUE #\u0026gt;  #\u0026gt; [[4]] #\u0026gt; [1] \"a\" str(l_mixed) #\u0026gt; List of 4 #\u0026gt; $ : num 2.1 #\u0026gt; $ : int 2 #\u0026gt; $ : logi TRUE #\u0026gt; $ : chr \"a\"  Things get more interesting when we create a list of vectors:\nmixed_vectors \u0026lt;- list(c(\"kim\", \"sandy\", \"lee\"), c(23, 21, 26)) mixed_vectors #\u0026gt; [[1]] #\u0026gt; [1] \"kim\" \"sandy\" \"lee\"  #\u0026gt;  #\u0026gt; [[2]] #\u0026gt; [1] 23 21 26 str(mixed_vectors) #\u0026gt; List of 2 #\u0026gt; $ : chr [1:3] \"kim\" \"sandy\" \"lee\" #\u0026gt; $ : num [1:3] 23 21 26  In these examples we see the appearance of a new subsetting operator [[ ]], in addition to [ ]. How do they differ? Let\u0026rsquo;s experiment, focussing on the second element of the list c(23, 21, 26). Let\u0026rsquo;s try to pull out that vector using the [2] notation (it is the second element after all).\nmixed_vectors_subset_a \u0026lt;- mixed_vectors[2] mixed_vectors_subset_a #\u0026gt; [[1]] #\u0026gt; [1] 23 21 26 str(mixed_vectors_subset_a) #\u0026gt; List of 1 #\u0026gt; $ : num [1:3] 23 21 26  This does not pull out the vector!! Instead, it returns a sublist which contains that vector as the only element (we\u0026rsquo;ll see why R does this below\u0026hellip;).\nSo how to we get our hands on the actual vector? This is where the [[ ]] operator comes in:\nmixed_vectors_subset_b \u0026lt;- mixed_vectors[[2]] mixed_vectors_subset_b #\u0026gt; [1] 23 21 26 str(mixed_vectors_subset_b) #\u0026gt; num [1:3] 23 21 26  We see that the behavior of the [ ] operator is very different for lists: it selects the element(s) you request, but always still wrapped inside a list. It \u0026lsquo;shrinks\u0026rsquo; the original list. The [[ ]] operator on the other hand \u0026lsquo;drills-down\u0026rsquo; and just returns the \u0026lsquo;un-listed\u0026rsquo; vector in that position.\nData frames The reason R does things this way is because data frames are so central to the language. Let\u0026rsquo;s build a data frame from the ground up to see how it works.\nBasically the \u0026ldquo;inside\u0026rdquo; of a data frame is just a list with name attributes:\nattr_list \u0026lt;- list(name = c(\"kim\", \"sandy\", \"lee\"), age = c(23, 21, 26)) attr_list #\u0026gt; $name #\u0026gt; [1] \"kim\" \"sandy\" \"lee\"  #\u0026gt;  #\u0026gt; $age #\u0026gt; [1] 23 21 26 str(attr_list) #\u0026gt; List of 2 #\u0026gt; $ name: chr [1:3] \"kim\" \"sandy\" \"lee\" #\u0026gt; $ age : num [1:3] 23 21 26  Notice that instead of str() displaying $ : ... for each entry, we now see attributes $ name: ... , $ age: ... for each entry. Also note that all those double [[ ]] notations have disappeared when we print. This should give you a clue that $age, for example, is a kind of alias for [[2]].\nFinally, we can \u0026lsquo;wrap\u0026rsquo; this into an official data frame structure:\nmy_df \u0026lt;- as.data.frame(attr_list) my_df #\u0026gt; name age #\u0026gt; 1 kim 23 #\u0026gt; 2 sandy 21 #\u0026gt; 3 lee 26 str(my_df) #\u0026gt; 'data.frame': 3 obs. of 2 variables: #\u0026gt; $ name: chr \"kim\" \"sandy\" \"lee\" #\u0026gt; $ age : num 23 21 26  Or wrap it into a tidyverse tibble:\nmy_tibble \u0026lt;- as_tibble(attr_list) my_tibble #\u0026gt; # A tibble: 3 × 2 #\u0026gt; name age #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 kim 23 #\u0026gt; 2 sandy 21 #\u0026gt; 3 lee 26 str(my_tibble) #\u0026gt; tibble [3 × 2] (S3: tbl_df/tbl/data.frame) #\u0026gt; $ name: chr [1:3] \"kim\" \"sandy\" \"lee\" #\u0026gt; $ age : num [1:3] 23 21 26  So a data frame is basically a list (intepreted as columns), with a names attribute for the columns (interpreted as headers). And with the extra condition that all the columns are of the same length, so it\u0026rsquo;s rectangular. So we should be able to use our standard list subsetting operators on it:\ncol_2 \u0026lt;- my_df[2] str(col_2) #\u0026gt; 'data.frame': 3 obs. of 1 variable: #\u0026gt; $ age: num 23 21 26  Since a data frame is a list, subsetting using [ ] returns the specified column still inside a data frame. What about [[ ]]?\nvec_2 \u0026lt;- my_df[[2]] str(vec_2) #\u0026gt; num [1:3] 23 21 26  Using [[ ]] pulls out the data vector from the column.\nJust like vectors, we can also subset a data frame by the name attribute, instead of by position:\ncol_2_by_name \u0026lt;- my_df[\"age\"] str(col_2_by_name) #\u0026gt; 'data.frame': 3 obs. of 1 variable: #\u0026gt; $ age: num 23 21 26  vec_2_by_name \u0026lt;- my_df[[\"age\"]] str(vec_2_by_name) #\u0026gt; num [1:3] 23 21 26  Finally my_df$age is simply a shorthand for my_df[[\u0026quot;age\u0026quot;]] without the [[ ]] and the \u0026quot; \u0026quot;:\nvec_2_by_dollar_name \u0026lt;- my_df$age str(vec_2_by_dollar_name) #\u0026gt; num [1:3] 23 21 26  Direct comparison with tidyverse functions The dplyr command select() over a data frame is exactly analogous to the single bracket operator my_df[\u0026quot;age\u0026quot;]. It returns a data frame with a single column:\ncol_2_select \u0026lt;- select(my_df, \"age\") col_2_select #\u0026gt; age #\u0026gt; 1 23 #\u0026gt; 2 21 #\u0026gt; 3 26 str(col_2_select) #\u0026gt; 'data.frame': 3 obs. of 1 variable: #\u0026gt; $ age: num 23 21 26  The dplyr command pull() over a data frame is exactly analogous to the double bracket operator my_df[[\u0026quot;age\u0026quot;]]. It returns the data vector inside that column:\ncol_2_pull \u0026lt;- pull(my_df, \"age\") col_2_pull #\u0026gt; [1] 23 21 26 str(col_2_pull) #\u0026gt; num [1:3] 23 21 26  The \u0026lsquo;problem\u0026rsquo; with these dplyr functions is that they require a data frame as input, and we recently saw in S03E01: t-tests that the statistical t-test output was not a data frame:\npop1 \u0026lt;- rnorm(n = 20, mean = 10, sd = 3) pop2 \u0026lt;- rnorm(n = 20, mean = 10, sd = 3) tresult \u0026lt;- t.test(x = pop1, y = pop2) str(tresult) #\u0026gt; List of 10 #\u0026gt; $ statistic : Named num 0.32 #\u0026gt; ..- attr(*, \"names\")= chr \"t\" #\u0026gt; $ parameter : Named num 37.9 #\u0026gt; ..- attr(*, \"names\")= chr \"df\" #\u0026gt; $ p.value : num 0.751 #\u0026gt; $ conf.int : num [1:2] -1.77 2.44 #\u0026gt; ..- attr(*, \"conf.level\")= num 0.95 #\u0026gt; $ estimate : Named num [1:2] 10.17 9.84 #\u0026gt; ..- attr(*, \"names\")= chr [1:2] \"mean of x\" \"mean of y\" #\u0026gt; $ null.value : Named num 0 #\u0026gt; ..- attr(*, \"names\")= chr \"difference in means\" #\u0026gt; $ stderr : num 1.04 #\u0026gt; $ alternative: chr \"two.sided\" #\u0026gt; $ method : chr \"Welch Two Sample t-test\" #\u0026gt; $ data.name : chr \"pop1 and pop2\" #\u0026gt; - attr(*, \"class\")= chr \"htest\"  This is not a data frame, but an \u0026lsquo;htest\u0026rsquo; class object. Further, it cannot be converted to a data frame in the usual way:\nas.data.frame(tresult) Error in as.data.frame.default(tresult): cannot coerce class ‘\u0026#34;htest\u0026#34;’ to a data.frame This is precisely why the tidyverse developed the broom::tidy() function, which works with legacy base-R outputs, and converts them to data frames. But if you have lots of t-tests, the overhead of converting all the outputs using broom, then using dplyr functions to access data, can be inefficient and overkill.\nThe t-test output is not a data frame, but it is a named list, so we can subset it directly. For example, to pull out the p.value we can do either:\ntresult[[3]] #\u0026gt; [1] 0.7507211  or\ntresult$p.value #\u0026gt; [1] 0.7507211  which is really much simpler than going through broom. In addition, we can get extra granularity very quickly using this notation. Say we want the lower bound of the confidence interval. We can \u0026lsquo;stack\u0026rsquo; indexes:\ntresult[[4]][[1]] #\u0026gt; [1] -1.774624  or\ntresult$conf.int[[1]] #\u0026gt; [1] -1.774624  This is saying \u0026lsquo;give me the 4th element (or the conf.int element), and then give me the 1st element of that\u0026rsquo;.\n Exercise 2 Reuse the t.test() code above, run str on the output, and extract the stderr value using both the $ and [[ ]] indexing approaches.\n  Solution (click here)  tresult$stderr #\u0026gt; [1] 1.041051  tresult[[7]] #\u0026gt; [1] 1.041051     Exercise 3 When we ran our first ANOVA, we never actually looked at the data structure that was produced.\nRun the following code and inspect the output.\nbill_length_anova \u0026lt;- aov(data = penguins %\u0026gt;% drop_na(), bill_length_mm ~ species + sex + species*sex) str(bill_length_anova) Aieee!\nWhat happens when you try to turn this into a data frame, using as.data.frame(bill_length_anova)?\nNow you can see why broom:tidy() is so useful! To remind yourselves what the tidied version looks like, run the code:\ntidy_anova \u0026lt;- broom::tidy(bill_length_anova) tidy_anova But we can still extract values from this data structure directly: you just have to work out where to look\u0026hellip;\nSee if you can extract the total residual df from this data structure using the $ notation.\n  Solution (click here)  bill_length_anova \u0026lt;- aov(data = penguins %\u0026gt;% drop_na(), bill_length_mm ~ species + sex + species*sex) str(bill_length_anova) #\u0026gt; List of 13 #\u0026gt; $ coefficients : Named num [1:6] 37.26 9.32 8.31 3.13 1.39 ... #\u0026gt; ..- attr(*, \"names\")= chr [1:6] \"(Intercept)\" \"speciesChinstrap\" \"speciesGentoo\" \"sexmale\" ... #\u0026gt; $ residuals : Named num [1:333] -1.29 2.242 3.042 -0.558 -1.09 ... #\u0026gt; ..- attr(*, \"names\")= chr [1:333] \"1\" \"2\" \"3\" \"4\" ... #\u0026gt; $ effects : Named num [1:333] -802.79 44.75 70.8 33.7 3.82 ... #\u0026gt; ..- attr(*, \"names\")= chr [1:333] \"(Intercept)\" \"speciesChinstrap\" \"speciesGentoo\" \"sexmale\" ... #\u0026gt; $ rank : int 6 #\u0026gt; $ fitted.values: Named num [1:333] 40.4 37.3 37.3 37.3 40.4 ... #\u0026gt; ..- attr(*, \"names\")= chr [1:333] \"1\" \"2\" \"3\" \"4\" ... #\u0026gt; $ assign : int [1:6] 0 1 1 2 3 3 #\u0026gt; $ qr :List of 5 #\u0026gt; ..$ qr : num [1:333, 1:6] -18.2483 0.0548 0.0548 0.0548 0.0548 ... #\u0026gt; .. ..- attr(*, \"dimnames\")=List of 2 #\u0026gt; .. .. ..$ : chr [1:333] \"1\" \"2\" \"3\" \"4\" ... #\u0026gt; .. .. ..$ : chr [1:6] \"(Intercept)\" \"speciesChinstrap\" \"speciesGentoo\" \"sexmale\" ... #\u0026gt; .. ..- attr(*, \"assign\")= int [1:6] 0 1 1 2 3 3 #\u0026gt; .. ..- attr(*, \"contrasts\")=List of 2 #\u0026gt; .. .. ..$ species: chr \"contr.treatment\" #\u0026gt; .. .. ..$ sex : chr \"contr.treatment\" #\u0026gt; ..$ qraux: num [1:6] 1.05 1.03 1.05 1.05 1.03 ... #\u0026gt; ..$ pivot: int [1:6] 1 2 3 4 5 6 #\u0026gt; ..$ tol : num 1e-07 #\u0026gt; ..$ rank : int 6 #\u0026gt; ..- attr(*, \"class\")= chr \"qr\" #\u0026gt; $ df.residual : int 327 #\u0026gt; $ contrasts :List of 2 #\u0026gt; ..$ species: chr \"contr.treatment\" #\u0026gt; ..$ sex : chr \"contr.treatment\" #\u0026gt; $ xlevels :List of 2 #\u0026gt; ..$ species: chr [1:3] \"Adelie\" \"Chinstrap\" \"Gentoo\" #\u0026gt; ..$ sex : chr [1:2] \"female\" \"male\" #\u0026gt; $ call : language aov(formula = bill_length_mm ~ species + sex + species * sex, data = penguins %\u0026gt;% drop_na()) #\u0026gt; $ terms :Classes 'terms', 'formula' language bill_length_mm ~ species + sex + species * sex #\u0026gt; .. ..- attr(*, \"variables\")= language list(bill_length_mm, species, sex) #\u0026gt; .. ..- attr(*, \"factors\")= int [1:3, 1:3] 0 1 0 0 0 1 0 1 1 #\u0026gt; .. .. ..- attr(*, \"dimnames\")=List of 2 #\u0026gt; .. .. .. ..$ : chr [1:3] \"bill_length_mm\" \"species\" \"sex\" #\u0026gt; .. .. .. ..$ : chr [1:3] \"species\" \"sex\" \"species:sex\" #\u0026gt; .. ..- attr(*, \"term.labels\")= chr [1:3] \"species\" \"sex\" \"species:sex\" #\u0026gt; .. ..- attr(*, \"order\")= int [1:3] 1 1 2 #\u0026gt; .. ..- attr(*, \"intercept\")= int 1 #\u0026gt; .. ..- attr(*, \"response\")= int 1 #\u0026gt; .. ..- attr(*, \".Environment\")=\u0026lt;environment: R_GlobalEnv\u0026gt;  #\u0026gt; .. ..- attr(*, \"predvars\")= language list(bill_length_mm, species, sex) #\u0026gt; .. ..- attr(*, \"dataClasses\")= Named chr [1:3] \"numeric\" \"factor\" \"factor\" #\u0026gt; .. .. ..- attr(*, \"names\")= chr [1:3] \"bill_length_mm\" \"species\" \"sex\" #\u0026gt; $ model :'data.frame': 333 obs. of 3 variables: #\u0026gt; ..$ bill_length_mm: num [1:333] 39.1 39.5 40.3 36.7 39.3 38.9 39.2 41.1 38.6 34.6 ... #\u0026gt; ..$ species : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ... #\u0026gt; ..$ sex : Factor w/ 2 levels \"female\",\"male\": 2 1 1 1 2 1 2 1 2 2 ... #\u0026gt; ..- attr(*, \"terms\")=Classes 'terms', 'formula' language bill_length_mm ~ species + sex + species * sex #\u0026gt; .. .. ..- attr(*, \"variables\")= language list(bill_length_mm, species, sex) #\u0026gt; .. .. ..- attr(*, \"factors\")= int [1:3, 1:3] 0 1 0 0 0 1 0 1 1 #\u0026gt; .. .. .. ..- attr(*, \"dimnames\")=List of 2 #\u0026gt; .. .. .. .. ..$ : chr [1:3] \"bill_length_mm\" \"species\" \"sex\" #\u0026gt; .. .. .. .. ..$ : chr [1:3] \"species\" \"sex\" \"species:sex\" #\u0026gt; .. .. ..- attr(*, \"term.labels\")= chr [1:3] \"species\" \"sex\" \"species:sex\" #\u0026gt; .. .. ..- attr(*, \"order\")= int [1:3] 1 1 2 #\u0026gt; .. .. ..- attr(*, \"intercept\")= int 1 #\u0026gt; .. .. ..- attr(*, \"response\")= int 1 #\u0026gt; .. .. ..- attr(*, \".Environment\")=\u0026lt;environment: R_GlobalEnv\u0026gt;  #\u0026gt; .. .. ..- attr(*, \"predvars\")= language list(bill_length_mm, species, sex) #\u0026gt; .. .. ..- attr(*, \"dataClasses\")= Named chr [1:3] \"numeric\" \"factor\" \"factor\" #\u0026gt; .. .. .. ..- attr(*, \"names\")= chr [1:3] \"bill_length_mm\" \"species\" \"sex\" #\u0026gt; - attr(*, \"class\")= chr [1:2] \"aov\" \"lm\"  as.data.frame(bill_length_anova) Error in as.data.frame.default(bill_length_anova) : cannot coerce class ‘c(\u0026#34;aov\u0026#34;, \u0026#34;lm\u0026#34;)’ to a data.frame tidy_anova \u0026lt;- broom::tidy(bill_length_anova) tidy_anova #\u0026gt; # A tibble: 4 × 6 #\u0026gt; term df sumsq meansq statistic p.value #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 species 2 7015. 3508. 654. 5.03e-115 #\u0026gt; 2 sex 1 1136. 1136. 212. 2.42e- 37 #\u0026gt; 3 species:sex 2 24.5 12.2 2.28 1.03e- 1 #\u0026gt; 4 Residuals 327 1753. 5.36 NA NA   bill_length_anova$df.residual #\u0026gt; [1] 327     \n","date":1644883200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645110349,"objectID":"f12ec554199ec77277f0cb3075ea2d99","permalink":"https://biodash.github.io/codeclub/s03e06_datastructures/","publishdate":"2022-02-15T00:00:00Z","relpermalink":"/codeclub/s03e06_datastructures/","section":"codeclub","summary":"In this session of Code Club, we'll move below and beyond the tidyverse to get an overview of accessing various kinds of data structure in R.","tags":null,"title":"S03E06: Data structures and subsetting","type":"codeclub"},{"authors":["Stephen Opiyo"],"categories":null,"content":" Housekeeping New to Code Club? Check out the Code Club Computer Setup instructions, which also has pointers for if you\u0026rsquo;re new to R or RStudio. A few related Code Club sessions include:\n S03E01: T-tests S03E02: ANOVA S03E03: PCA S03E04: ANOVA part II  What will we go over today  We will introduce correlation. Learn using the corrr() package.  R packages we will use  corrr \u0026ndash; for correlation analysis palmerpenguins \u0026ndash; for the penguins data tidyverse \u0026ndash; for data wrangling   1 - What is correlation? Correlation is a statistical method used to assess a linear association between two continuous variables. It is measured by a statistic called the correlation coefficient, which represents the strength of the linear association between the variables in question. It is a dimensionless quantity that takes a value in the range −1 to +1. A correlation coefficient of zero indicates that no linear relationship exists between two continuous variables, and a correlation coefficient of −1 or +1 indicates a perfect linear relationship. (The stronger the correlation, the closer the correlation coefficient comes to −1 and +1.)\nPositive coefficient: If the coefficient is a positive number, the variables are positively related (i.e., as the value of one variable goes up, the value of the other also tends to do so).\nNegative coefficient: If the coefficient is a negative number, the variables are inversely related (i.e., as the value of one variable goes up, the value of the other tends to go down).\nTypes of correlation coefficients: There are two main types of correlation coefficients, Pearson\u0026rsquo;s correlation coefficient and Spearman\u0026rsquo;s rank correlation coefficient. The correct usage of correlation coefficient type depends on the types of variables being studied.\n  Pearson\u0026rsquo;s correlation coefficient: Pearson\u0026rsquo;s correlation coefficient is denoted as ϱ for a population parameter and as r for a sample statistic. It is used when both variables being studied are normally distributed.\n  Spearman\u0026rsquo;s rank correlation coefficient: Spearman\u0026rsquo;s rank correlation coefficient is denoted as ϱs for a population parameter and as rs for a sample statistic. It is appropriate when one or both variables are skewed or ordinal.\n  Rule of thumb for interpreting the size of a correlation coefficient\n .90 to 1.00 (−.90 to −1.00) \u0026ndash; Very high positive (negative) correlation .70 to .90 (−.70 to −.90) \u0026ndash; High positive (negative) correlation .50 to .70 (−.50 to −.70) \u0026ndash; Moderate positive (negative) correlation .30 to .50 (−.30 to −.50) \u0026ndash; Low positive (negative) correlation .00 to .30 (.00 to −.30) \u0026ndash; Negligible correlation   2 - The corrr package and our data The corrr package is a tool for exploring correlations. It makes it easy to perform routine tasks when exploring correlation matrices such as ignoring the diagonal, focusing on the correlations of certain variables against others, or rearranging and visualizing the matrix in terms of the strength of the correlations. The corrr package exists within the Comprehensive R Archive Network, or CRAN.\nLet\u0026rsquo;s install it \u0026ndash; we only need to do this once:\ninstall.packages(\"corrr\")  To use the corrr package, we need to load it up using library(). We also need to load the tidyverse since we will be using it later:\nlibrary(corrr) library(tidyverse) #\u0026gt; ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ── #\u0026gt; ✔ ggplot2 3.3.5 ✔ purrr  0.3.4 #\u0026gt; ✔ tibble  3.1.6 ✔ dplyr  1.0.7 #\u0026gt; ✔ tidyr  1.2.0 ✔ stringr 1.4.0 #\u0026gt; ✔ readr  2.1.2 ✔ forcats 0.5.1 #\u0026gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── #\u0026gt; ✖ dplyr::filter() masks stats::filter() #\u0026gt; ✖ dplyr::lag() masks stats::lag()   Let\u0026rsquo;s get set up and grab some data to work with.\nWe will use the same dataset palmerpenguins used in the previous weeks.\nIf you didn\u0026rsquo;t install this package previously, please do so now:\ninstall.packages(\"palmerpenguins\")  Then, to use the package, we need to use the function library() to load it:\nlibrary(palmerpenguins)  The data we will use today is a dataframe called penguins, which we reference after loading the package. We will look at the structure of the data:\n# look at the first 10 rows, all columns head(penguins, 10) #\u0026gt; # A tibble: 10 × 8 #\u0026gt; species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 Adelie Torgersen 39.1 18.7 181 3750 #\u0026gt; 2 Adelie Torgersen 39.5 17.4 186 3800 #\u0026gt; 3 Adelie Torgersen 40.3 18 195 3250 #\u0026gt; 4 Adelie Torgersen NA NA NA NA #\u0026gt; 5 Adelie Torgersen 36.7 19.3 193 3450 #\u0026gt; 6 Adelie Torgersen 39.3 20.6 190 3650 #\u0026gt; 7 Adelie Torgersen 38.9 17.8 181 3625 #\u0026gt; 8 Adelie Torgersen 39.2 19.6 195 4675 #\u0026gt; 9 Adelie Torgersen 34.1 18.1 193 3475 #\u0026gt; 10 Adelie Torgersen 42 20.2 190 4250 #\u0026gt; # … with 2 more variables: sex \u0026lt;fct\u0026gt;, year \u0026lt;int\u0026gt; # check the structure of penguins_data # glimpse() which is a part of dplyr functions  # similarly to str() and can be used interchangeably glimpse(penguins) #\u0026gt; Rows: 344 #\u0026gt; Columns: 8 #\u0026gt; $ species \u0026lt;fct\u0026gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel… #\u0026gt; $ island \u0026lt;fct\u0026gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse… #\u0026gt; $ bill_length_mm \u0026lt;dbl\u0026gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, … #\u0026gt; $ bill_depth_mm \u0026lt;dbl\u0026gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, … #\u0026gt; $ flipper_length_mm \u0026lt;int\u0026gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186… #\u0026gt; $ body_mass_g \u0026lt;int\u0026gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, … #\u0026gt; $ sex \u0026lt;fct\u0026gt; male, female, female, NA, female, male, female, male… #\u0026gt; $ year \u0026lt;int\u0026gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…  Okay, now we have a sense of what the penguins dataset is.\n 2 - Basic usage of the corrr package Now we want to use the corrr package to correlate variables in the penguins dataframe. The corrr package uses the correlate() function and returns correlation results in a tibble output.\nNow let\u0026rsquo;s select variables from the penguins dataframe using the pipe operator %\u0026gt;% and the select() function. We then specify the name of the variables that we want to select. In this example, we are selecting variables bill_length_mm, bill_depth_mm, flipper_length_mm, and body_mass_g.\nThen, we will compute correlations among the variables:\npenguins_cor \u0026lt;- penguins %\u0026gt;% # Take penguins_data select(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g) %\u0026gt;% correlate() # Select variables and calculate their correlations #\u0026gt;  #\u0026gt; Correlation method: 'pearson' #\u0026gt; Missing treated using: 'pairwise.complete.obs' penguins_cor # Correlation results in tibble #\u0026gt; # A tibble: 4 × 5 #\u0026gt; term bill_length_mm bill_depth_mm flipper_length_mm body_mass_g #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 bill_length_mm NA -0.235 0.656 0.595 #\u0026gt; 2 bill_depth_mm -0.235 NA -0.584 -0.472 #\u0026gt; 3 flipper_length_mm 0.656 -0.584 NA 0.871 #\u0026gt; 4 body_mass_g 0.595 -0.472 0.871 NA  The output of the correlate() function can be piped (%\u0026gt;%) to:\n The shave() and rearrange() functions for internal changes The focus(), and stretch() functions to reshape the structure And the rplot(), fashion(), and network_plot() for visualizations.  Let us start with internal changes.\n The shave() function removes values of the upper or lower triangle and sets them to NA. The rearrange() function arranges the columns and rows based on correlation strengths.  We can apply the shave() function to remove the top triangle:\npenguins_cor %\u0026gt;% # Take penguins_cor results shave() # Remove the upper triangle #\u0026gt; # A tibble: 4 × 5 #\u0026gt; term bill_length_mm bill_depth_mm flipper_length_mm body_mass_g #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 bill_length_mm NA NA NA NA #\u0026gt; 2 bill_depth_mm -0.235 NA NA NA #\u0026gt; 3 flipper_length_mm 0.656 -0.584 NA NA #\u0026gt; 4 body_mass_g 0.595 -0.472 0.871 NA  You can see that the values in the upper triangle have been replaced by NAs. Now let us remove NAs by using the fashion() function:\npenguins_cor %\u0026gt;% # Take penguins_cor results shave %\u0026gt;% # Remove the upper triangle fashion() # Remove NAs #\u0026gt; term bill_length_mm bill_depth_mm flipper_length_mm body_mass_g #\u0026gt; 1 bill_length_mm  #\u0026gt; 2 bill_depth_mm -.24  #\u0026gt; 3 flipper_length_mm .66 -.58  #\u0026gt; 4 body_mass_g .60 -.47 .87  You can see that the NAs have been removed, and we have a clean tibble (dataframe).\nNow, let\u0026rsquo;s rearrange columns based on correlation strengths:\npenguins_cor %\u0026gt;% # Take penguins_cor results rearrange %\u0026gt;% # Rearrange based on correlation strengths shave %\u0026gt;% # Remove the upper triangle fashion() # Remove NAs #\u0026gt; term flipper_length_mm body_mass_g bill_length_mm bill_depth_mm #\u0026gt; 1 flipper_length_mm  #\u0026gt; 2 body_mass_g .87  #\u0026gt; 3 bill_length_mm .66 .60  #\u0026gt; 4 bill_depth_mm -.58 -.47 -.24  You can see that the values are arranged based on correlation strengths.\n  Breakout session 1   From the penguins dataframe, create a new dataset called penguins_biscoe by selecting only the penguins from the island of Biscoe.\n  In the penguins_biscoe dataframe, what are the correlations between bill_length_mm, bill_depth_mm, flipper_length_mm, and year?\n  Remove the upper triangle and NAs from the results, and arrange based on correlation strengths.\n    Hints (click here)  Use the filter() function to only select penguins from the island of Biscoe.\n   Solution (click here)  Select the penguins from the island of Biscoe:\npenguins_biscoe \u0026lt;- penguins %\u0026gt;% # Save results in new object filter(island == \"Biscoe\") # Select data from only Biscoe island   Calculate correlation of the variables in penguins_biscoe:\nbiscoe_cor \u0026lt;- penguins_biscoe %\u0026gt;% # select bill_length_mm, bill_depth_mm, flipper_length_mm, and year select(bill_length_mm, bill_depth_mm, flipper_length_mm, year) %\u0026gt;% correlate() # Calculate correlation #\u0026gt;  #\u0026gt; Correlation method: 'pearson' #\u0026gt; Missing treated using: 'pairwise.complete.obs' biscoe_cor #\u0026gt; # A tibble: 4 × 5 #\u0026gt; term bill_length_mm bill_depth_mm flipper_length_mm year #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 bill_length_mm NA -0.444 0.866 0.0968 #\u0026gt; 2 bill_depth_mm -0.444 NA -0.579 0.138  #\u0026gt; 3 flipper_length_mm 0.866 -0.579 NA 0.105  #\u0026gt; 4 year 0.0968 0.138 0.105 NA  To remove the upper triangle and rearrange the results:\nbiscoe_cor %\u0026gt;% # Take biscoe_cor results rearrange() %\u0026gt;% # Rearrange based on correlation strengths shave() %\u0026gt;% # Remove the upper triangle fashion() # Remove NAs #\u0026gt; term flipper_length_mm bill_length_mm year bill_depth_mm #\u0026gt; 1 flipper_length_mm  #\u0026gt; 2 bill_length_mm .87  #\u0026gt; 3 year .11 .10  #\u0026gt; 4 bill_depth_mm -.58 -.44 .14      3 - Reshaping and visualizations The output of the correlate() function can also be piped to:\n The focus() and stretch() functions to reshape the structure And the rplot(), and network_plot() for visualizations.  Reshape structure:\n The focus() function select columns or rows based on the variable specified. The stretch() converts correction results from a tibble into a long format.  Visualizations:\n The rplot() function plots correlation results The network_plot() function plots a point for each variable, joined by paths for correlations  Let use the stretch() function to convert our correlation results penguins_cor into a long format:\npenguins_cor %\u0026gt;% stretch() #\u0026gt; # A tibble: 16 × 3 #\u0026gt; x y r #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 bill_length_mm bill_length_mm NA  #\u0026gt; 2 bill_length_mm bill_depth_mm -0.235 #\u0026gt; 3 bill_length_mm flipper_length_mm 0.656 #\u0026gt; 4 bill_length_mm body_mass_g 0.595 #\u0026gt; 5 bill_depth_mm bill_length_mm -0.235 #\u0026gt; 6 bill_depth_mm bill_depth_mm NA  #\u0026gt; 7 bill_depth_mm flipper_length_mm -0.584 #\u0026gt; 8 bill_depth_mm body_mass_g -0.472 #\u0026gt; 9 flipper_length_mm bill_length_mm 0.656 #\u0026gt; 10 flipper_length_mm bill_depth_mm -0.584 #\u0026gt; 11 flipper_length_mm flipper_length_mm NA  #\u0026gt; 12 flipper_length_mm body_mass_g 0.871 #\u0026gt; 13 body_mass_g bill_length_mm 0.595 #\u0026gt; 14 body_mass_g bill_depth_mm -0.472 #\u0026gt; 15 body_mass_g flipper_length_mm 0.871 #\u0026gt; 16 body_mass_g body_mass_g NA  You can see that the results are coverted into a long format.\nWe can also select a column we are interested in. Let us select only correlation between \u0026ldquo;bill_depth_mm\u0026rdquo; and the rest of the variables.\npenguins_cor %\u0026gt;% focus(\"bill_depth_mm\") #\u0026gt; # A tibble: 3 × 2 #\u0026gt; term bill_depth_mm #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 bill_length_mm -0.235 #\u0026gt; 2 flipper_length_mm -0.584 #\u0026gt; 3 body_mass_g -0.472  You can see that we selected only the correlations between bill_depth_mm and other variables.\nWe can now visualize the results:\npenguins_cor %\u0026gt;% rearrange() %\u0026gt;% # Rearrange based on correlation strengths rplot() # Plot #\u0026gt; Don't know how to automatically pick scale for object of type noquote. Defaulting to continuous.   We plotted based on correlation strengths.\n  Breakout session 2 Use the dataframe (penguins_biscoe) you created in Breakout session 1 to:\n  Calculate correlations among the variables, and present the results in a long format\n  Select the correlations that include year\n  Plot a correlation graph based on correlation strengths\n    Solution (click here)  biscoe_cor \u0026lt;- penguins_biscoe %\u0026gt;% select(bill_length_mm, bill_depth_mm, flipper_length_mm, year) %\u0026gt;% correlate() #\u0026gt;  #\u0026gt; Correlation method: 'pearson' #\u0026gt; Missing treated using: 'pairwise.complete.obs' biscoe_cor %\u0026gt;% stretch() #\u0026gt; # A tibble: 16 × 3 #\u0026gt; x y r #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 bill_length_mm bill_length_mm NA  #\u0026gt; 2 bill_length_mm bill_depth_mm -0.444  #\u0026gt; 3 bill_length_mm flipper_length_mm 0.866  #\u0026gt; 4 bill_length_mm year 0.0968 #\u0026gt; 5 bill_depth_mm bill_length_mm -0.444  #\u0026gt; 6 bill_depth_mm bill_depth_mm NA  #\u0026gt; 7 bill_depth_mm flipper_length_mm -0.579  #\u0026gt; 8 bill_depth_mm year 0.138  #\u0026gt; 9 flipper_length_mm bill_length_mm 0.866  #\u0026gt; 10 flipper_length_mm bill_depth_mm -0.579  #\u0026gt; 11 flipper_length_mm flipper_length_mm NA  #\u0026gt; 12 flipper_length_mm year 0.105  #\u0026gt; 13 year bill_length_mm 0.0968 #\u0026gt; 14 year bill_depth_mm 0.138  #\u0026gt; 15 year flipper_length_mm 0.105  #\u0026gt; 16 year year NA  You can see that results are in a long format.\nbiscoe_cor %\u0026gt;% focus(\"year\") #\u0026gt; # A tibble: 3 × 2 #\u0026gt; term year #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 bill_length_mm 0.0968 #\u0026gt; 2 bill_depth_mm 0.138  #\u0026gt; 3 flipper_length_mm 0.105  Correlations with year:\nbiscoe_cor %\u0026gt;% rearrange() %\u0026gt;% rplot() #\u0026gt; Don't know how to automatically pick scale for object of type noquote. Defaulting to continuous.      ","date":1644451200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1644513254,"objectID":"afd1c119b3906a1edd0d18826835dbd5","permalink":"https://biodash.github.io/codeclub/s03e05_correlation/","publishdate":"2022-02-10T00:00:00Z","relpermalink":"/codeclub/s03e05_correlation/","section":"codeclub","summary":"During this second-to-last session in our series on basic stats in R, Stephen will show how to run correlation analyses in R.","tags":null,"title":"S03E05: Introduction to correlation","type":"codeclub"},{"authors":["Jelmer Poelstra"],"categories":null,"content":"  A PCA of genetic variation among Europeans, from Novembre et al 2008: \"Genes mirror geography within Europe\"    Housekeeping New to Code Club? Check out the Code Club Computer Setup instructions, which also has pointers for if you\u0026rsquo;re new to R or RStudio. A few related Code Club sessions include:\n S03E01: T-tests S03E02: ANOVA S02E06: Intro to ggplot2, part I S02E07: Intro to ggplot2, part II  Session goals   Learn how to perform a PCA in R using the prcomp() function.\n  Understand what is represented by the different components of the output.\n  Learn about three kinds of plots commonly used to visualize PCA results, and how to create them.\n   R packages we will use  palmerpenguins \u0026ndash; A data package containing the data we will explore tidyverse \u0026ndash; A metapackage that includes ggplot2 which we\u0026rsquo;ll use for plotting, access to the %\u0026gt;% pipe, etc. broom \u0026ndash; We\u0026rsquo;ll again use the tidy() function in broom to create tidy dataframes from untidy statistical function output glue \u0026ndash; For pasting strings with variables factoextra \u0026ndash; For easily creating a PCA biplot (and other PCA plots)    Getting set up If you plan to just listen during the first part, you can wait until the Breakout Rooms to do the following. Also, instead of copying-and-pasting code, you could download this R script with today\u0026rsquo;s code.\n  Open a new R script in RStudio (File =\u0026gt; New File =\u0026gt; R Script)\n  Save the script, as something along the lines of codeclub_S03E03_PCA.R\n  Copy the following code into the script, for installing packages where needed:\n## `require(glue)` returns FALSE if glue isn't installed; therefore, ## these lines will only try to install packages that aren't already installed. if (!require(palmerpenguins)) install.packages(\"palmerpenguins\") if (!require(tidyverse)) install.packages(\"tidyverse\") if (!require(broom)) install.packages(\"broom\") if (!require(glue)) install.packages(\"glue\") if (!require(factoextra)) install.packages(\"factoextra\")    Also copy the following code to load the packages into your R session:\nlibrary(palmerpenguins) library(tidyverse) library(broom) library(glue) library(factoextra)    Run the code above in the R console.\n   1 - A brief intro to PCA Principal Component Analysis (PCA) is a popular method that creates \u0026ldquo;summary variables\u0026rdquo; (Principal Components) which represent as much of the information as possible from a high-dimensional dataset.\nA high-dimensional dataset is a dataset with measurements for many variables, such as expression levels for thousands of genes.\nPCA and similar methods like PCoA and nMDS (see box below) are also called \u0026ldquo;dimension reduction\u0026rdquo; or \u0026ldquo;ordination\u0026rdquo; methods, and can be classified as a type of unsupervised learning.\nPCA is most commonly used for exploratory data visualization to see overall patterns in datasets, though you could also use the resulting Principal Components as response variables in a statistical model.\n Glossary  Principal Components (PCs) \u0026ndash; the summary variables that a PCA produces. Loadings (rotations) \u0026ndash; Loadings apply to the original variables. They are the contributions of variables to PCs, which form the \u0026ldquo;recipes\u0026rdquo; used to create the PCs. Scores (coordinates) \u0026ndash; Scores apply to the samples. These scores, for each PC, are coordinates that can be used to create a score plot which is the \u0026ldquo;classic\u0026rdquo; PCA plot. Eigenvalue \u0026ndash; The variance (amount of variation) explained by a PC.     Similar ordination methods Besides PCA, other commonly used ordination methods that are also unconstrained (i.e., with no response variable) include the following:\n  Principal Coordinate Analysis (PCoA) is also known as Metric Multidimensional Scaling (MDS / mMDS). PCoA allows you to use distance measures other than Euclidean distance and can be run e.g. with stats::cmdscale().\n  Non-metric Multidimensional Scaling (nMDS) is a non-metric method with quite different inner workings from PCA and PCoA that is especially suitable when your distance values are imprecise. It can be run e.g. with vegan::metaMDS().\n  If you\u0026rsquo;re struggling to pick a suitable ordination approach for your data, take a look at Table 1 in Nguyen \u0026amp; Holmes 2019.\n   2 - prcomp(), scaling, and centering To perform a PCA analysis in R, there are two functions that can be used without the need to load any packages: prcomp() and princomp().\n(Like last week\u0026rsquo;s aov() function, these functions are in the stats package, which is loaded into your R session by default. More PCA functions are available in other packages but these tend to be very similar and/or simply wrap the two base R functions.)\nWe will use prcomp(), which is preferred among these two due to its slightly better accuracy1.\nTwo important data pre-processing steps\u0026hellip; \u0026hellip;need to be done for many PCA analyses. Luckily, these can be done alongside the PCA computation in a single call to prcomp():\nCentering the data \u0026ndash; Centering the data around the origin (subtracting the mean of variables) is basically always advisable and is controlled by the center argument of prcomp(), which is set to TRUE by default.\nScaling the data \u0026ndash; Standardizing the standard deviation across the variables in the data (i.e., scaling) is advisable when variables are in different units or on different scales but is generally not recommended when all variables are of the same type and in the same units (e.g., gene counts2). Whether or not to scale the data is controlled by the scale. argument of prcomp(), which is set to FALSE by default.\n 3 - Our first PCA As a simple example, we want to run a PCA summarizing the four numerical measurements taken for each penguin (bill length, bill depth, flipper length, and body mass) in the palmerpenguins dataset.\nFirst, we\u0026rsquo;ll subset the penguins dataframe to:\n  Remove rows with NAs (prcomp() will return an error if any of our variables contain NAs)\n  Select only the columns that we want to include in the PCA\n  ## Remove rows with NAs penguins_noNA \u0026lt;- drop_na(penguins) ## Select columns penguins_for_pca \u0026lt;- penguins_noNA %\u0026gt;% select(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g)  Let\u0026rsquo;s take a look at the resulting dataframe:\nhead(penguins_for_pca) #\u0026gt; # A tibble: 6 × 4 #\u0026gt; bill_length_mm bill_depth_mm flipper_length_mm body_mass_g #\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 39.1 18.7 181 3750 #\u0026gt; 2 39.5 17.4 186 3800 #\u0026gt; 3 40.3 18 195 3250 #\u0026gt; 4 36.7 19.3 193 3450 #\u0026gt; 5 39.3 20.6 190 3650 #\u0026gt; 6 38.9 17.8 181 3625 dim(penguins_for_pca) #\u0026gt; [1] 333 4  Run the PCA! Now, we are ready to run the PCA:\npca \u0026lt;- prcomp(penguins_for_pca, scale = TRUE) # (Because `center = TRUE` by default, we don't have to include that.)  Scaling is desirable here because as we saw above, the variables we use in our PCA are in different units (mm and g).\n  More on scaling (click here)  Because our variables are in different units, standard deviations for those variables may differ dramatically. This would lead the PCA to put more weight on variables with a higher standard deviation, which we don\u0026rsquo;t want if those differences are merely a consequence of different units.\nIf we check the standard deviations in our dataset, we can indeed see large differences:\nmap(penguins_for_pca, sd) #\u0026gt; $bill_length_mm #\u0026gt; [1] 5.468668 #\u0026gt;  #\u0026gt; $bill_depth_mm #\u0026gt; [1] 1.969235 #\u0026gt;  #\u0026gt; $flipper_length_mm #\u0026gt; [1] 14.01577 #\u0026gt;  #\u0026gt; $body_mass_g #\u0026gt; [1] 805.2158    4 - Exploring the output I Like with objects returned by the statistical tests we saw in the previous weeks, the object returned by prcomp() is not just a dataframe or even a regular list\u0026hellip;\nclass(pca) #\u0026gt; [1] \"prcomp\"  \u0026hellip; and trying to print the object to screen will only give you a summary of sorts:\npca #\u0026gt; Standard deviations (1, .., p=4): #\u0026gt; [1] 1.6569115 0.8821095 0.6071594 0.3284579 #\u0026gt;  #\u0026gt; Rotation (n x k) = (4 x 4): #\u0026gt; PC1 PC2 PC3 PC4 #\u0026gt; bill_length_mm 0.4537532 -0.60019490 -0.6424951 0.1451695 #\u0026gt; bill_depth_mm -0.3990472 -0.79616951 0.4258004 -0.1599044 #\u0026gt; flipper_length_mm 0.5768250 -0.00578817 0.2360952 -0.7819837 #\u0026gt; body_mass_g 0.5496747 -0.07646366 0.5917374 0.5846861  Like we saw last week with aov(), we can get a more useful summary of the results with the summary() function:\nsummary(pca) #\u0026gt; Importance of components: #\u0026gt; PC1 PC2 PC3 PC4 #\u0026gt; Standard deviation 1.6569 0.8821 0.60716 0.32846 #\u0026gt; Proportion of Variance 0.6863 0.1945 0.09216 0.02697 #\u0026gt; Cumulative Proportion 0.6863 0.8809 0.97303 1.00000  This shows us the \u0026ldquo;importance\u0026rdquo; of the 4 principal components that our PCA returned, i.e. the amount of variation they explain.\nSeeing all elements with str() These summaries are nice and all, but like we saw in previous weeks, they don\u0026rsquo;t make it obvious where and how to access all the information contained in the object.\nRunning the str() function is a good start for getting to the raw contents of the object, even though the information printed isn\u0026rsquo;t easy to look at:\nstr(pca) #\u0026gt; List of 5 #\u0026gt; $ sdev : num [1:4] 1.657 0.882 0.607 0.328 #\u0026gt; $ rotation: num [1:4, 1:4] 0.454 -0.399 0.577 0.55 -0.6 ... #\u0026gt; ..- attr(*, \"dimnames\")=List of 2 #\u0026gt; .. ..$ : chr [1:4] \"bill_length_mm\" \"bill_depth_mm\" \"flipper_length_mm\" \"body_mass_g\" #\u0026gt; .. ..$ : chr [1:4] \"PC1\" \"PC2\" \"PC3\" \"PC4\" #\u0026gt; $ center : Named num [1:4] 44 17.2 201 4207.1 #\u0026gt; ..- attr(*, \"names\")= chr [1:4] \"bill_length_mm\" \"bill_depth_mm\" \"flipper_length_mm\" \"body_mass_g\" #\u0026gt; $ scale : Named num [1:4] 5.47 1.97 14.02 805.22 #\u0026gt; ..- attr(*, \"names\")= chr [1:4] \"bill_length_mm\" \"bill_depth_mm\" \"flipper_length_mm\" \"body_mass_g\" #\u0026gt; $ x : num [1:333, 1:4] -1.85 -1.31 -1.37 -1.88 -1.92 ... #\u0026gt; ..- attr(*, \"dimnames\")=List of 2 #\u0026gt; .. ..$ : NULL #\u0026gt; .. ..$ : chr [1:4] \"PC1\" \"PC2\" \"PC3\" \"PC4\" #\u0026gt; - attr(*, \"class\")= chr \"prcomp\"  In the first breakout room session, you\u0026rsquo;ll explore the contents of our pca object a bit more.\n Breakout Rooms I  Exercise 1 If you didn\u0026rsquo;t do so already, get set up for the remaining exercises. Either download this R script, open it in RStudio, and run the code, or:\n  Open a new R script in RStudio (File =\u0026gt; New File =\u0026gt; R Script)\n  Save the script, as something along the lines of codeclub_S03E03_PCA.R\n  Copy the following code into the script and then run it in the R console:\n  ## Install packages if needed ## (`require(glue)` returns FALSE if glue isn't installed; therefore, ## these lines will only try to install packages that aren't already installed.) if (!require(palmerpenguins)) install.packages(\"palmerpenguins\") if (!require(tidyverse)) install.packages(\"tidyverse\") if (!require(broom)) install.packages(\"broom\") if (!require(glue)) install.packages(\"glue\") if (!require(factoextra)) install.packages(\"factoextra\") ## Load the packages into your R session library(palmerpenguins) library(tidyverse) library(broom) library(glue) library(factoextra) ## Prep the penguin data for the PCA penguins_noNA \u0026lt;- drop_na(penguins) penguins_for_pca \u0026lt;- penguins_noNA %\u0026gt;% select(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g) ## Run the PCA pca \u0026lt;- prcomp(penguins_for_pca, scale = TRUE)     Exercise 2 How can you access the different components in the List of 5 that is summarized when running str(pca)? For example, say you wanted to see the rotation element in its entirety, how could you do this?\n  Hints (click here)  The $ (dollar sign) operator can be used to access the different elements (as implied by the dollar signs shown in front of the names of the elements).\n   Solution (click here)  To see the rotation element, type pca$rotation:\npca$rotation #\u0026gt; PC1 PC2 PC3 PC4 #\u0026gt; bill_length_mm 0.4537532 -0.60019490 -0.6424951 0.1451695 #\u0026gt; bill_depth_mm -0.3990472 -0.79616951 0.4258004 -0.1599044 #\u0026gt; flipper_length_mm 0.5768250 -0.00578817 0.2360952 -0.7819837 #\u0026gt; body_mass_g 0.5496747 -0.07646366 0.5917374 0.5846861     Exercise 3 (bonus) Take a look at the contents of all five elements in the pca object. Do you have a (rough) understanding of what each represents?\n  Hints (click here)  Take a look at the Glossary.\n   Solution (click here)  All elements of the output are explained in the next section of this page.\n    5 - Exploring the output II Let\u0026rsquo;s take a quick look together at the three most important elements in the object returned by prcomp(), which we named pca:\n  pca$sdev is a vector of standard deviations associated with each principal component (PC), i.e. it is the amount of variation explained by each PC. We also saw this information when running summary(pca) and we\u0026rsquo;ll use it to create the scree plot.\npca$sdev #\u0026gt; [1] 1.6569115 0.8821095 0.6071594 0.3284579    pca$x is the most-used part of the output: a matrix containing the scores (or coordinates) for each sample for each PC, used to create a score plot and part of the biplot.\nhead(pca$x) #\u0026gt; PC1 PC2 PC3 PC4 #\u0026gt; [1,] -1.850808 -0.03202119 0.23454869 0.5276026 #\u0026gt; [2,] -1.314276 0.44286031 0.02742880 0.4011230 #\u0026gt; [3,] -1.374537 0.16098821 -0.18940423 -0.5278675 #\u0026gt; [4,] -1.882455 0.01233268 0.62792772 -0.4721826 #\u0026gt; [5,] -1.917096 -0.81636958 0.69999797 -0.1961213 #\u0026gt; [6,] -1.770356 0.36567266 -0.02841769 0.5046092    pca$rotation is a matrix that contains the loadings for each variable in each PC. These are the \u0026ldquo;recipes\u0026rdquo; for creating each PC, with higher absolute values indicating a larger effect of the variable on the PC. The sign (- or +) matters too: in PC1, larger values of bill_depth_mm lower the PC value, and vice versa for the other three variables. This matrix will be used in creating the biplot.\npca$rotation #\u0026gt; PC1 PC2 PC3 PC4 #\u0026gt; bill_length_mm 0.4537532 -0.60019490 -0.6424951 0.1451695 #\u0026gt; bill_depth_mm -0.3990472 -0.79616951 0.4258004 -0.1599044 #\u0026gt; flipper_length_mm 0.5768250 -0.00578817 0.2360952 -0.7819837 #\u0026gt; body_mass_g 0.5496747 -0.07646366 0.5917374 0.5846861      ...And the remaining two elements (click here)    pca$center is a vector containing the means for each variable, which was subsequently used for centering the data (this would contain just FALSE if the data wasn\u0026rsquo;t centered).\npca$center #\u0026gt; bill_length_mm bill_depth_mm flipper_length_mm body_mass_g  #\u0026gt; 43.99279 17.16486 200.96697 4207.05706    pca$scale similarly is a vector containing the scaling constant for each variable (column) in the data, and would be FALSE if the data wasn\u0026rsquo;t scaled.\npca$scale #\u0026gt; bill_length_mm bill_depth_mm flipper_length_mm body_mass_g  #\u0026gt; 5.468668 1.969235 14.015765 805.215802      6 - Scree plot A \u0026ldquo;scree plot\u0026rdquo;3 is a barplot that shows the amount of variation explained by each PC.\nWe\u0026rsquo;ll make a base R version of this plot (gasp!) because it is so quick to make, and we don\u0026rsquo;t need this figure to be fancy:\nplot(pca)   In this scree plot, we show the variance (i.e. the eigenvalue) associated with each PC (these are the square roots of the standard deviations in pca$sdev.)\n Interpretation This gives us a quick visual overview of the importance of the PCs: PC1 is by far the most important, and PC4 doesn\u0026rsquo;t do much at all. (PCs are always ordered by the amount of variation they explain, with PC1 explaining most.)\n   7 - Score (classic PCA) plot A \u0026ldquo;score plot\u0026rdquo; shows the scores (coordinates) for each sample along two PCs, typically the first two.\nWe\u0026rsquo;re going to need a dataframe to plot. But if we were to broom::tidy() the scores matrix (pca$x), akin to what we\u0026rsquo;ve done with t-test and ANOVA output in previous weeks, we would get a dataframe with all PCs in one column that wouldn\u0026rsquo;t be that easy to plot.\nSo in this case, we\u0026rsquo;ll just manipulate pca$x ourselves \u0026ndash; in particular, we want to add the source penguins_noNA dataframe back to it, which will allow us to color the points by, say, species.\n## Column-bind (= put side-by-side) the scores and the source dataframe pca_scores \u0026lt;- bind_cols(data.frame(pca$x), penguins_noNA) head(pca_scores) #\u0026gt; PC1 PC2 PC3 PC4 species island bill_length_mm #\u0026gt; 1 -1.850808 -0.03202119 0.23454869 0.5276026 Adelie Torgersen 39.1 #\u0026gt; 2 -1.314276 0.44286031 0.02742880 0.4011230 Adelie Torgersen 39.5 #\u0026gt; 3 -1.374537 0.16098821 -0.18940423 -0.5278675 Adelie Torgersen 40.3 #\u0026gt; 4 -1.882455 0.01233268 0.62792772 -0.4721826 Adelie Torgersen 36.7 #\u0026gt; 5 -1.917096 -0.81636958 0.69999797 -0.1961213 Adelie Torgersen 39.3 #\u0026gt; 6 -1.770356 0.36567266 -0.02841769 0.5046092 Adelie Torgersen 38.9 #\u0026gt; bill_depth_mm flipper_length_mm body_mass_g sex year #\u0026gt; 1 18.7 181 3750 male 2007 #\u0026gt; 2 17.4 186 3800 female 2007 #\u0026gt; 3 18.0 195 3250 female 2007 #\u0026gt; 4 19.3 193 3450 female 2007 #\u0026gt; 5 20.6 190 3650 male 2007 #\u0026gt; 6 17.8 181 3625 female 2007  Now we\u0026rsquo;re ready to create the plot:\nscore_plot \u0026lt;- ggplot(pca_scores) + geom_point(aes(x = PC1, y = PC2, color = species)) + theme_classic() score_plot    Interpretation Across these four measurements, Gentoo Penguins can be very clearly distinguished from the other two species, whereas among Adelie and Chinstrap Penguins, there are average differences but they are not fully separable.\n  A better aspect ratio One way to improve our plot is to set the aspect ratio (the proportional relationship between the height and the width) according to the relative percentages of variation explained by the two plotted PCs: because PC1 on the x-axis explains more variation, we want the plot to be wide.\nTo get the percentages in a dataframe, now we will use the tidy() function. But because the output of prcomp() contains multiple elements, we\u0026rsquo;ll have to point tidy() to the $sdev element using the matrix argument (see the docs):\npca_eigen \u0026lt;- tidy(pca, matrix = \"eigenvalues\") pca_eigen #\u0026gt; # A tibble: 4 × 4 #\u0026gt; PC std.dev percent cumulative #\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 1 1.66 0.686 0.686 #\u0026gt; 2 2 0.882 0.195 0.881 #\u0026gt; 3 3 0.607 0.0922 0.973 #\u0026gt; 4 4 0.328 0.0270 1  Now, we\u0026rsquo;ll store the percentages explained by the first two PCs (rounded to one decimal):\n# (Note: pca_eigen$percent contains proportions, not percentages...) PC1_percent \u0026lt;- round(pca_eigen$percent[1] * 100, 1) PC2_percent \u0026lt;- round(pca_eigen$percent[2] * 100, 1) PC1_percent #\u0026gt; [1] 68.6 PC2_percent #\u0026gt; [1] 19.5  Finally, we can modify the aspect ratio, which is expressed as height / width \u0026ndash; and we\u0026rsquo;ll also move the legend to the top, and add the percentages to the axis titles:\nscore_plot \u0026lt;- score_plot + theme(aspect.ratio = PC2_percent / PC1_percent, legend.position = \"top\") + labs(x = glue(\"PC1 (\u0026#123;PC1_percent\u0026#125;%)\"), y = glue(\"PC2 (\u0026#123;PC2_percent\u0026#125;%)\")) score_plot    8 - Biplot A \u0026ldquo;biplot\u0026rdquo; shows the scores of samples for two PCs and the loadings for the original variables along the two PCs.\nBecause biplots are more complicated to make \u0026ldquo;from scratch\u0026rdquo; using ggplot2, we will turn to the package factoextra, which has a convenient function for making biplots, fviz_pca():\nfviz_pca(pca, label = \"var\", # Show labels for variables only habillage = penguins_noNA$species) + # color by / shape by theme(legend.position = \"top\")   While this plot can certainly be improved upon, biplots are by their nature a little unwieldy.\n Interpretation Biplots can be especially useful when you have a modest number of original variables, like here. Some information we can glean from this particular biplot:\n  Flipper length and body mass are highly correlated among individuals, even across species. So flipper length relative to body mass is similar across species.\n  Gentoo penguins are larger and with narrower bills than the other two species.\n     While we made a scree plot with base R and a score plot with \u0026ldquo;base ggplot2\u0026rdquo;, there are also factoextra functions for these and for other PCA plots:\n fviz_eig() \u0026ndash; scree plots fviz_pca_ind() \u0026ndash; score plots fviz_pca_var() \u0026ndash; \u0026ldquo;correlation circles\u0026rdquo;; plots showing loadings only (example). fviz_contrib() \u0026ndash; a barplot with the contribution of variables to 1 PC (example).      Or, to create a biplot from scratch... (click here)  First, let\u0026rsquo;s save the loadings in a dataframe:\npca_loadings \u0026lt;- data.frame(pca$rotation) %\u0026gt;% rownames_to_column(\"var\")  Next, we start with the score plot object score_plot we created above.\nWhat we need to add are the variable loading, which we\u0026rsquo;ll do with geom_segment() to draw arrows, and geom_text() to add text labels near the tips of the arrows:\n## To make the arrows longer (all by the same amount), ## just to improve the visualization, we use a multiplication factor: mult \u0026lt;- 2.5 score_plot + ## geom_segment draws lines geom_segment(data = pca_loadings, ## The lines should start from the origin: aes(x = 0, y = 0, xend = PC1 * mult, yend = PC2 * mult), ## We turn the line into an arrow: arrow = arrow(), ## A gray-tone might work better than black: color = \"grey40\") + geom_text(data = pca_loadings, ## The text labels go at the end of the arrows: aes(x = PC1 * mult, y = PC2 * mult, label = var), ## We left-align (hjust = 0) and lower (vjust = 1) the labels hjust = 0, vjust = 1, ## Again, we use a gray color: color = \"grey40\")     Breakout Rooms II  Exercise 4 Above, we plotted the scores for the first two PCs (PC1 and PC2) in our score plot and biplot. Now, create a biplot with another combination of two PCs.\nTake a look at the help for the fviz_pca() function by typing\n?fviz_pca to find out how you might be able to plot different PCs.\n  Hints (click here)    The axes argument to fviz_pca() controls which axes will be plotted; this argument accepts a vector of two numbers.\n  Do you think it would be worth plotting PC4, which explains \u0026lt;3% of the variation? Would plotting PC3 with one of the PCs we already plotted be informative?\n     Solution (click here)  To plot PC1 \u0026amp; PC3 (which may be a better choice than including PC4 because it explains so little variation):\nfviz_pca(pca, axes = c(1, 3), label = \"var\", habillage = penguins_noNA$species) + theme(legend.position = \"top\")   Behold, now we can distinguish much better between Adelie and Chinstrap Penguins!\n    Exercise 5 Run the PCA for just one of the three penguin species.\nThen, make a biplot of the results, in which you color the points by something else than species, e.g. by sex. (If you want, also make a scree plot and/or a score plot.)\n  Hints (click here)    Use the dplyr function filter() on the penguins_noNA object to select rows corresponding to one penguin species.\n  After that, the code will be nearly identical to that used before; just make sure to refer to the correct objects if you copy-and-paste code.\n     Solution (click here)  This example solution runs a PCA for Gentoo Penguins only.\nFirst, select rows corresponding to our focal penguin species, and run the PCA:\n## (Save this object rather than using one pipeline, ## because you'll need it color the biplot by a factor) onepenguin_noNA \u0026lt;- penguins_noNA %\u0026gt;% filter(species == \"Gentoo\") pca \u0026lt;- onepenguin_noNA %\u0026gt;% select(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g) %\u0026gt;% prcomp(scale = TRUE)  Next, we create the biplot:\nfviz_pca(pca, label = \"var\", habillage = onepenguin_noNA$sex) + theme(legend.position = \"top\")   To create a scree plot:\nplot(pca)   Or a scree plot with fviz_eig():\nfviz_eig(pca)   To create a quick score plot (no aspect ratio manipulation):\nbind_cols(data.frame(pca$x), onepenguin_noNA) %\u0026gt;% ggplot() + geom_point(aes(x = PC1, y = PC2, color = sex)) + theme_classic()   Or a score plot with fviz_pca_ind():\nfviz_pca_ind(pca, geom = \"point\", habillage = onepenguin_noNA$sex)       Exercise 6 (bonus) Make a scree plot of our original PCA results with ggplot2 instead of base R.\n  Hints (click here)    Use the pca_eigen dataframe that we created above for plotting, and use the geom geom_col().\n  Think about what exactly you want to plot on the y-axis. The variance, like in the base R scree plot? Or the proportion/percentage of the variance explained?\n     Solution (click here)  There are a couple of different things that could reasonably be put on the y-axis, but perhaps the clearest option is to put the proportion or percentage of variation (=variance) explained, like below:\nggplot(pca_eigen, aes(x = PC, y = percent)) + geom_col() + labs(y = \"Proportion of the variation explained\") + theme_minimal()   (Note once again that the column in pca_eigen is called percent, but it actually contains proportions.)\n   Further watching \u0026amp; reading  \u0026ldquo;StatQuest\u0026rdquo; videos on:  PCA (22 minutes) PCA follow-up: practical tips (8 minutes) MDS and PCoA   Chapter on Multivariate Analysis from the book \u0026ldquo;Modern Statistics for Modern Biology\u0026rdquo; Nguyen \u0026amp; Holmes 2019: \u0026ldquo;Ten quick tips for effective dimensionality reduction\u0026rdquo;    Crawley 2012 \u0026ndash; \u0026ldquo;The R Book\u0026rdquo; \u0026ndash; pdf \u0026#x21a9;\u0026#xfe0e;\n However, high-throughput sequencing results such as gene counts do need to be normalized by sample sequencing depth (\u0026ldquo;library size\u0026rdquo;) and subjected to a variance stabilizing normalization. \u0026#x21a9;\u0026#xfe0e;\n As \u0026ldquo;The R Book\u0026rdquo; (Crawley 2012) explains: \u0026ldquo;This is called a scree plot in PCA because it is supposed to look like a cliff face on a mountainside (on the left), with a scree slope below it (the tail on the right).\u0026rdquo; \u0026#x21a9;\u0026#xfe0e;\n   ","date":1644278400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1644367765,"objectID":"ac6e352bd77208f6035962a6e9211504","permalink":"https://biodash.github.io/codeclub/s03e03_pca/","publishdate":"2022-02-08T00:00:00Z","relpermalink":"/codeclub/s03e03_pca/","section":"codeclub","summary":"Today, we'll discuss how you can run a Principal Component Analysis (PCA) in R with the `prcomp()` function and create three types of plots from the results: a score plot, a scree plot, and a biplot.","tags":null,"title":"S03E03: Principal Component Analysis (PCA)","type":"codeclub"},{"authors":["Jessica Cooperstone"],"categories":null,"content":" Prep homework Basic computer setup   If you didn\u0026rsquo;t already do this, please follow the Code Club Computer Setup instructions, which also has pointers for if you\u0026rsquo;re new to R or RStudio.\n  If you\u0026rsquo;re able to do so, please open RStudio a bit before Code Club starts \u0026ndash; and in case you run into issues, please join the Zoom call early and we\u0026rsquo;ll help you troubleshoot.\n  New to ggplot? This isn\u0026rsquo;t a ggplot specific session, though we will be using it a bit. Check out the past Code Club sessions covering ggplot2:\n S01E04: intro to ggplot2 S01E05: intro to ggplot2 round 2 S01E10: faceting and animating S02E06: another intro to ggplot2 S02E07: a second intro to ggplot2 round 2 S02E08: combining plots using faceting S02E09: combining plots using faceting and patchwork S02E10: adding statistics to plots S02E11: making interactive plots with plotly  If you\u0026rsquo;ve never used ggplot2 before (or even if you have), you may find this cheat sheet useful.\nAdding statistics to plots We had a previous session S02E10 developed by Daniel Quiroz that covers the package ggpubr and adding statistics to ggplots.\nWe already did t-tests and ANOVA part 1 Mike Sovic covered in code club S03E01 how to run t-tests in R. I covered ANOVA two weeks ago S03E02 and we will be building off that session today.\nGetting an R Markdown   Click here to get an Rmd (optional)  RMarkdown for today # directory  dir.create(\"S03E04\") # directory for our RMarkdown # (\"recursive\" to create two levels at once.) dir.create(\"S03E04/Rmd/\") # save the url location for today's script todays_Rmd \u0026lt;- \"https://raw.githubusercontent.com/biodash/biodash.github.io/master/content/codeclub/S03E04_anova2/anova2.Rmd\" # indicate the name of the new Rmd S03E04_Rmd \u0026lt;- \"S03E04/Rmd/S03E04_anova2.Rmd\" # go get that file!  download.file(url = todays_Rmd, destfile = S03E04_Rmd)     Introduction We have gone through a first pass of running ANOVAs in Code Club a couple weeks ago but didn\u0026rsquo;t have the time to go through all of the content. We are going to re-visit that material today.\nOften people are first introduced to the R programming language when they are wanting to conduct statistical analyses. My experience is that beginners are often able to conduct the analysis they want, and print their results to the console. But, the process of locating and then using the output of their analysis tends to be more complex.\nToday, we are going to go over how to:\n test if our data is suitable for running ANOVA run an ANOVA (parametric) or Kruskal Wallis (non-parametric) test run posthoc tests to understand group differences use the ANOVA data output object as a means to understand R data structure.  If you are looking for a good statistics class, I would recommend Dr. Kristin Mercer\u0026rsquo;s HCS 8887 Experimental Design.\n Load packages, get data We are going to start with our favorite dataset palmerpenguins to provide the input data for our analysis.\nIf you don\u0026rsquo;t have any of the packages below, use install.packages() to download them.\nlibrary(tidyverse) library(palmerpenguins) # for data library(rstatix) # for testing assumptions and running tests library(agricolae) # for post-hoc comparison of groups   1 - Getting acclimated Some words on syntax: the dataset penguins is an object within the palmerpenguins package. If you call the object penguins (after executing library(palmerpenguins)), you will be able to see what is contained within that dataframe.\npenguins #\u0026gt; # A tibble: 344 × 8 #\u0026gt; species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 Adelie Torgersen 39.1 18.7 181 3750 #\u0026gt; 2 Adelie Torgersen 39.5 17.4 186 3800 #\u0026gt; 3 Adelie Torgersen 40.3 18 195 3250 #\u0026gt; 4 Adelie Torgersen NA NA NA NA #\u0026gt; 5 Adelie Torgersen 36.7 19.3 193 3450 #\u0026gt; 6 Adelie Torgersen 39.3 20.6 190 3650 #\u0026gt; 7 Adelie Torgersen 38.9 17.8 181 3625 #\u0026gt; 8 Adelie Torgersen 39.2 19.6 195 4675 #\u0026gt; 9 Adelie Torgersen 34.1 18.1 193 3475 #\u0026gt; 10 Adelie Torgersen 42 20.2 190 4250 #\u0026gt; # … with 334 more rows, and 2 more variables: sex \u0026lt;fct\u0026gt;, year \u0026lt;int\u0026gt;  However, penguins will not be in your environment tab because it is not in your local environment. You can use it without it being in your local environment, but if you are bothered by this, you can save a copy in your local environment such it shows up in that top right pane.\npenguins \u0026lt;- penguins  What is within this dataset?\nglimpse(penguins) #\u0026gt; Rows: 344 #\u0026gt; Columns: 8 #\u0026gt; $ species \u0026lt;fct\u0026gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel… #\u0026gt; $ island \u0026lt;fct\u0026gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse… #\u0026gt; $ bill_length_mm \u0026lt;dbl\u0026gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, … #\u0026gt; $ bill_depth_mm \u0026lt;dbl\u0026gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, … #\u0026gt; $ flipper_length_mm \u0026lt;int\u0026gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186… #\u0026gt; $ body_mass_g \u0026lt;int\u0026gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, … #\u0026gt; $ sex \u0026lt;fct\u0026gt; male, female, female, NA, female, male, female, male… #\u0026gt; $ year \u0026lt;int\u0026gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…  Illustration by Allison Horst\n 2. Testing assumptions I\u0026rsquo;d be remiss if I didn\u0026rsquo;t show you how to test that you aren\u0026rsquo;t violating any of the assumptions needed to conduct an ANOVA. We went over this a little bit back in the session put together by Daniel Quiroz on ggpubr and adding statistical results to ggplots.\nBriefly, in order to use parametric procedures (like ANOVA), we need to be sure our data meets the assumptions for 1) normality and 2) constant variance. This can be done in a few different ways.\nIllustration by Allison Horst\nShapiro-Wilk test for normality We are going to use the Shapiro-Wilk test (using the function shapiro_test() which is in the package rstatix to determine normality, but will do it groupwise. This function is a pipe-friendly wrapper for the function shapiro.test(), which just means you can use it with pipes.\nOur question is:\n Does bill_length_mm vary by species in female penguins?    Illustration by Allison Horst   Caputuring some descriptive statistics penguins %\u0026gt;% filter(sex == \"female\") %\u0026gt;% drop_na() %\u0026gt;% group_by(species) %\u0026gt;% count() #\u0026gt; # A tibble: 3 × 2 #\u0026gt; # Groups: species [3] #\u0026gt; species n #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 Adelie 73 #\u0026gt; 2 Chinstrap 34 #\u0026gt; 3 Gentoo 58  # testing for all female penguins together penguins %\u0026gt;% filter(sex == \"female\") %\u0026gt;% drop_na() %\u0026gt;% rstatix::shapiro_test(bill_length_mm) #\u0026gt; # A tibble: 1 × 3 #\u0026gt; variable statistic p #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 bill_length_mm 0.950 0.0000140 # testing by species penguins %\u0026gt;% drop_na() %\u0026gt;% filter(sex == \"female\") %\u0026gt;% group_by(species) %\u0026gt;% rstatix::shapiro_test(bill_length_mm) #\u0026gt; # A tibble: 3 × 4 #\u0026gt; species variable statistic p #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 Adelie bill_length_mm 0.991 0.895  #\u0026gt; 2 Chinstrap bill_length_mm 0.883 0.00170 #\u0026gt; 3 Gentoo bill_length_mm 0.989 0.895  Note that if we test all the penguins together, it looks like we do not have normal data. If we test by species, we see that two speces have normal data distribution and one (Chinstrap) does not.\nCan we visualize normality in another way?\nLet\u0026rsquo;s quickly make a new dataframe that includes only the female penguins, and drop missing values, so that we don\u0026rsquo;t have to keep including the filter() and drop_na() statements.\nfemale_penguins \u0026lt;- penguins %\u0026gt;% filter(sex == \"female\") %\u0026gt;% drop_na()  Visualizing with a histogram by species.\nfemale_penguins %\u0026gt;% ggplot(aes(x = bill_length_mm)) + geom_histogram() + facet_grid(cols = vars(species)) #\u0026gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.   We can see here too that the Chinstrap penguins look maybe not that normal (and we saw earlier they have the fewest numbers of observations). This is consistent with the results from the Shapiro test.\nLog transforming Would our data look more normal if we log transformed it? Let\u0026rsquo;s see. We can use the function mutate() to create a new column called bill_length_mm_log2 which will have the data from bill_length_mm but log transformed using base 2 (using the base R function log2()).\nIllustration by Allison Horst\nThe syntax of mutate() is like this:\n mutate(new_variable = function(existing_variable))  log_female_penguins \u0026lt;- female_penguins %\u0026gt;% mutate(bill_length_mm_log2 = log2(bill_length_mm))  Testing using shapiro_test() again.\nlog_female_penguins %\u0026gt;% # don't need drop_na() because we already did that group_by(species) %\u0026gt;% shapiro_test(bill_length_mm_log2) #\u0026gt; # A tibble: 3 × 4 #\u0026gt; species variable statistic p #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 Adelie bill_length_mm_log2 0.991 0.868  #\u0026gt; 2 Chinstrap bill_length_mm_log2 0.911 0.00900 #\u0026gt; 3 Gentoo bill_length_mm_log2 0.988 0.841  Still not passing the test for normality. Let\u0026rsquo;s still look at this visually.\nlog_female_penguins %\u0026gt;% ggplot(aes(x = bill_length_mm_log2)) + geom_histogram() + facet_grid(cols = vars(species)) #\u0026gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.   Doesn\u0026rsquo;t look very different from the non-log2 transformed data. Ok, well we tried.\nEqual variance We can test for equal variance using Levene\u0026rsquo;s test, levene_test() which is part of the rstatix package. Again, this is a pipe-friendly wrapper for the function levene.test().\nfemale_penguins %\u0026gt;% levene_test(bill_length_mm ~ species) #\u0026gt; # A tibble: 1 × 4 #\u0026gt; df1 df2 statistic p #\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 2 162 0.819 0.442  Our data meets the assumption for equal variance, but not for normality, so we will need to be sure to select a test that does not have an assumption of normality.\n 3. Kruskal Wallis test The Kruskal Wallis test is the non-parametric version of a one-way ANOVA. This non-parametric test tests whether samples are coming from the same distribution, but uses ranks instead of means.\nWe want to see if there are any differences in bill length (bill_length_mm) in penguins by species. Since our data violates the assumptions of normality, we should do this using a test that does not require normality, and we can use the Kruskal Wallis test. The Kruskal-Wallis test, can be run using the rstatix function kruskal_test().\nFirst let\u0026rsquo;s get some descriptive information about our data.\nfemale_penguins %\u0026gt;% group_by(species) %\u0026gt;% count() #\u0026gt; # A tibble: 3 × 2 #\u0026gt; # Groups: species [3] #\u0026gt; species n #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 Adelie 73 #\u0026gt; 2 Chinstrap 34 #\u0026gt; 3 Gentoo 58  If we want to learn more about the function kruskal_test() we can do so using the code below. The help documentation will show up in the bottom right quadrant of your RStudio.\n?kruskal_test()  We can run a Kruskal-Wallis test by indicating our model.\nbill_length_kruskal \u0026lt;- female_penguins %\u0026gt;% kruskal_test(bill_length_mm ~ species)  The function kruskal_test() already puts the output of the function into a tidy format, so we can simply view it.\nView(bill_length_kruskal)     .y. n statistic df p method     bill_length_mm 165 121.6214 2 0 Kruskal-Wallis     We can also look at our data visually by plotting it, as below.\nfemale_penguins %\u0026gt;% ggplot(aes(x = species, y = bill_length_mm)) + geom_boxplot(outlier.shape = NA) + geom_jitter(width = 0.3)    Breakout rooms 1 We want to know if there are any significant differences in bill_depth_mm by species in male penguins.\nExercise 1  Test your assumptions for normality to determine what would be the appropriate test to do to assess means separation.\n  Hints (click here)  Use the function shapiro_test() to test normality. If your data is non-normal, you can check to see if log transforming it makes it normal.    Solutions (click here)  Shapiro-Wilk Test # create df with male penguins and no NAs male_penguins \u0026lt;- penguins %\u0026gt;% filter(sex == \"male\") %\u0026gt;% drop_na()  # run shapiro test male_penguins %\u0026gt;% group_by(species) %\u0026gt;% rstatix::shapiro_test(bill_depth_mm) #\u0026gt; # A tibble: 3 × 4 #\u0026gt; species variable statistic p #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 Adelie bill_depth_mm 0.964 0.0335 #\u0026gt; 2 Chinstrap bill_depth_mm 0.983 0.863  #\u0026gt; 3 Gentoo bill_depth_mm 0.980 0.401  Visualize\nmale_penguins %\u0026gt;% ggplot(aes(x = bill_depth_mm)) + geom_histogram() + facet_grid(cols = vars(species)) #\u0026gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.   See if log-transforming your data would allow you to use parametric tests.\nlog_male_penguins \u0026lt;- male_penguins %\u0026gt;% mutate(bill_depth_mm_log2 = log2(bill_depth_mm))  Testing using shapiro_test() again.\nlog_male_penguins %\u0026gt;% # don't need drop_na() because we already did that group_by(species) %\u0026gt;% shapiro_test(bill_depth_mm_log2) #\u0026gt; # A tibble: 3 × 4 #\u0026gt; species variable statistic p #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 Adelie bill_depth_mm_log2 0.971 0.0914 #\u0026gt; 2 Chinstrap bill_depth_mm_log2 0.981 0.814  #\u0026gt; 3 Gentoo bill_depth_mm_log2 0.980 0.438  Ok! We could use log2 transformed data\nVisualize.\nlog_male_penguins %\u0026gt;% ggplot(aes(x = bill_depth_mm_log2)) + geom_histogram() + facet_grid(cols = vars(species)) #\u0026gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.       Exercise 2  Test your assumptions for equal variance to determine what would be the appropriate test to do to assess means separation.\n  Hints (click here)  \u0026laquo;\u0026laquo;\u0026laquo;\u0026lt; HEAD You can use the function levene_test() to test for equal variance.\n======= You can use the function [`levene_test()`](https://rpkgs.datanovia.com/rstatix/reference/levene_test.html) to test for equal variance.  b4836320d0dbe7b28303489c3f695280e6d0c2a2    Solutions (click here)  Equal variance male_penguins %\u0026gt;% levene_test(bill_depth_mm ~ species) #\u0026gt; # A tibble: 1 × 4 #\u0026gt; df1 df2 statistic p #\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 2 165 2.30 0.103      Exercise 3  Conduct your Kruskal-Wallis test or ANOVA to see if there is any overall significant effect of species on bill_depth_mm of male penguins.\n  Hints (click here)  Review the information in section 3 of this post. You could also use the package ggpubr.    Solutions (click here)  Kruskal-Wallis bill_depth_kruskal \u0026lt;- male_penguins %\u0026gt;% kruskal_test(bill_depth_mm ~ species)  View(bill_depth_kruskal)     .y. n statistic df p method     bill_depth_mm 168 116.152 2 0 Kruskal-Wallis     ANOVA - to use this you need to be using normal data (here, the log transformed data).\nbill_depth_anova \u0026lt;- aov(data = log_male_penguins, formula = bill_depth_mm_log2 ~ species)  summary(bill_depth_anova) #\u0026gt; Df Sum Sq Mean Sq F value Pr(\u0026gt;F)  #\u0026gt; species 2 3.1221 1.5610 319.8 \u0026lt;2e-16 *** #\u0026gt; Residuals 165 0.8053 0.0049  #\u0026gt; --- #\u0026gt; Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1      4. Posthoc group analysis Now that we\u0026rsquo;ve seen that species are significant effectors of bill_length_mm, our next logical question might be, which species specifically are different from each other? We can determine this by conducting a post-hoc test. We will do our post-hoc analysis using Dunn\u0026rsquo;s test (which is for specifically ranked data) and the function dunn_test() which is a part of rstatix. In the example below, we are using the Benjamini Hochberg method of pvalue adjustment for multiple corrections.\ndunn_bill_length \u0026lt;- female_penguins %\u0026gt;% dunn_test(bill_length_mm ~ species, p.adjust.method = \"BH\") # there are others too  Like we did with t-tests, you can also look at the resulting dunn_test() object (here, dunn_bill_length) in your environment pane.\nView(dunn_bill_length)     .y. group1 group2 n1 n2 statistic p p.adj p.adj.signif     bill_length_mm Adelie Chinstrap 73 34 8.8609532 0.0000000 0.0000000 ****   bill_length_mm Adelie Gentoo 73 58 9.4096815 0.0000000 0.0000000 ****   bill_length_mm Chinstrap Gentoo 34 58 -0.8549426 0.3925829 0.3925829 ns     From this result, we can see that Adelie is significantly different than Chinstrap and Gentoo, but Chinstrap and Gentoo are not significantly different from each other.\nThe structure of this resulting object dunn_bill_length can be determined using the code below.\nstr(dunn_bill_length) #\u0026gt; rstatix_test [3 × 9] (S3: rstatix_test/dunn_test/tbl_df/tbl/data.frame) #\u0026gt; $ .y. : chr [1:3] \"bill_length_mm\" \"bill_length_mm\" \"bill_length_mm\" #\u0026gt; $ group1 : chr [1:3] \"Adelie\" \"Adelie\" \"Chinstrap\" #\u0026gt; $ group2 : chr [1:3] \"Chinstrap\" \"Gentoo\" \"Gentoo\" #\u0026gt; $ n1 : Named int [1:3] 73 73 34 #\u0026gt; ..- attr(*, \"names\")= chr [1:3] \"Adelie\" \"Adelie\" \"Chinstrap\" #\u0026gt; $ n2 : Named int [1:3] 34 58 58 #\u0026gt; ..- attr(*, \"names\")= chr [1:3] \"Chinstrap\" \"Gentoo\" \"Gentoo\" #\u0026gt; $ statistic : num [1:3] 8.861 9.41 -0.855 #\u0026gt; $ p : num [1:3] 7.93e-19 4.98e-21 3.93e-01 #\u0026gt; $ p.adj : num [1:3] 1.19e-18 1.49e-20 3.93e-01 #\u0026gt; $ p.adj.signif: chr [1:3] \"****\" \"****\" \"ns\" #\u0026gt; - attr(*, \"na.action\")= 'omit' Named int 3 #\u0026gt; ..- attr(*, \"names\")= chr \"3\" #\u0026gt; - attr(*, \"args\")=List of 5 #\u0026gt; ..$ data : tibble [165 × 8] (S3: tbl_df/tbl/data.frame) #\u0026gt; .. ..$ species : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ... #\u0026gt; .. ..$ island : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 1 1 ... #\u0026gt; .. ..$ bill_length_mm : num [1:165] 39.5 40.3 36.7 38.9 41.1 36.6 38.7 34.4 37.8 35.9 ... #\u0026gt; .. ..$ bill_depth_mm : num [1:165] 17.4 18 19.3 17.8 17.6 17.8 19 18.4 18.3 19.2 ... #\u0026gt; .. ..$ flipper_length_mm: int [1:165] 186 195 193 181 182 185 195 184 174 189 ... #\u0026gt; .. ..$ body_mass_g : int [1:165] 3800 3250 3450 3625 3200 3700 3450 3325 3400 3800 ... #\u0026gt; .. ..$ sex : Factor w/ 2 levels \"female\",\"male\": 1 1 1 1 1 1 1 1 1 1 ... #\u0026gt; .. ..$ year : int [1:165] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ... #\u0026gt; ..$ formula :Class 'formula' language bill_length_mm ~ species #\u0026gt; .. .. ..- attr(*, \".Environment\")=\u0026lt;environment: 0x55a775e9b4b8\u0026gt;  ======= #\u0026gt; .. .. ..- attr(*, \".Environment\")=\u0026lt;environment: 0x7fb80d5a2188\u0026gt;   b4836320d0dbe7b28303489c3f695280e6d0c2a2 #\u0026gt; ..$ p.adjust.method: chr \"BH\" #\u0026gt; ..$ detailed : logi FALSE #\u0026gt; ..$ method : chr \"dunn_test\"  This df does not have a \u0026lsquo;groups\u0026rsquo; column, but if we want to plot in the same way, we can make a new object which we use for plotting. I\u0026rsquo;m going to show you here how to do this manually.\ndunn_for_plotting \u0026lt;- data.frame(species = c(\"Adelie\", \"Chinstrap\", \"Gentoo\"), groups = c(\"a\", \"b\", \"b\"))   5. Bringing it together in a plot We already looked at a first-pass plot, but let\u0026rsquo;s customize it now, and add our statistical info. Here is our base plot.\nfemale_penguins %\u0026gt;% ggplot(aes(x = species, y = bill_length_mm)) + geom_boxplot(outlier.shape = NA) + geom_jitter(width = 0.3)   First let\u0026rsquo;s make the plot more aesthetically pleasing.\n(bill_length_plot \u0026lt;- female_penguins %\u0026gt;% ggplot(aes(x = species, y = bill_length_mm, color = species)) + geom_boxplot(outlier.shape = NA) + geom_jitter(width = 0.3) + theme_classic() + theme(legend.position = \"none\") + # remove legend bc we don't need it labs(x = \"Species\", y = \"Bill Length, in mm\", title = \"Penguin Culmen Bill Length Among Different Species\", subtitle = \"Data collected from Palmer LTER, Antarctica\"))   We want to add the letters to this plot, so we can tell which groups of species by sex are significantly different. We are going to figure out what the maximum bill_length_mm for each species by sex is, so it will help us determine where to put our letter labels. Then, we cna add our labels to be higher than the largest data point.\nbill_length_max \u0026lt;- female_penguins %\u0026gt;% group_by(species) %\u0026gt;% summarize(max_bill_length_mm = max(bill_length_mm)) bill_length_max #\u0026gt; # A tibble: 3 × 2 #\u0026gt; species max_bill_length_mm #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 Adelie 42.2 #\u0026gt; 2 Chinstrap 58  #\u0026gt; 3 Gentoo 50.5  Let\u0026rsquo;s add our bill_length_max back to the df with our post-hoc groups dunn_for_plotting.\nbill_for_plotting \u0026lt;- full_join(dunn_for_plotting, bill_length_max, by = \"species\") bill_for_plotting #\u0026gt; species groups max_bill_length_mm #\u0026gt; 1 Adelie a 42.2 #\u0026gt; 2 Chinstrap b 58.0 #\u0026gt; 3 Gentoo b 50.5  Let\u0026rsquo;s plot.\nbill_length_plot + geom_text(data = bill_for_plotting, aes(x = species, y = 3 + max_bill_length_mm, color = species, label = groups)) + labs(caption = \"Different letters indicate significant difference as determined by \\na Kruskal-Wallis test with Dunn's post-hoc means separation\")   Also remember Daniel showed us how we can do somthing similar using the package ggpubr.\n Breakout rooms 2 Exercise 4  Conduct a post-hoc analysis to understand which male penguin species have significantly different bill_depth_mm.\n  Hints (click here)  Using the results from your assumption testing in Exercise 3, pick an appropriate post-hoc test to answer your question.    Solutions (click here)  Dunn's test dunn_bill_depth \u0026lt;- male_penguins %\u0026gt;% dunn_test(bill_depth_mm ~ species, p.adjust.method = \"BH\")  View(dunn_bill_depth)     .y. group1 group2 n1 n2 statistic p p.adj p.adj.signif     bill_depth_mm Adelie Chinstrap 73 34 0.9039517 0.366021 0.366021 ns   bill_depth_mm Adelie Gentoo 73 61 -9.5885513 0.000000 0.000000 ****   bill_depth_mm Chinstrap Gentoo 34 61 -8.6487581 0.000000 0.000000 ****     Parametric post-hoc test\nbonferroni_bill_depth \u0026lt;- agricolae::LSD.test(bill_depth_anova, trt = \"species\", p.adj = \"bonferroni\", console = TRUE) #\u0026gt;  #\u0026gt; Study: bill_depth_anova ~ \"species\" #\u0026gt;  #\u0026gt; LSD t Test for bill_depth_mm_log2  #\u0026gt; P value adjustment method: bonferroni  #\u0026gt;  #\u0026gt; Mean Square Error: 0.004880851  #\u0026gt;  #\u0026gt; species, means and individual ( 95 %) CI #\u0026gt;  #\u0026gt; bill_depth_mm_log2 std r LCL UCL Min Max #\u0026gt; Adelie 4.251426 0.07632273 73 4.235282 4.267571 4.087463 4.426265 #\u0026gt; Chinstrap 4.265907 0.05726112 34 4.242250 4.289564 4.129283 4.378512 #\u0026gt; Gentoo 3.972772 0.06803521 61 3.955110 3.990433 3.817623 4.112700 #\u0026gt;  #\u0026gt; Alpha: 0.05 ; DF Error: 165 #\u0026gt; Critical Value of t: 2.418634  #\u0026gt;  #\u0026gt; Groups according to probability of means differences and alpha level( 0.05 ) #\u0026gt;  #\u0026gt; Treatments with the same letter are not significantly different. #\u0026gt;  #\u0026gt; bill_depth_mm_log2 groups #\u0026gt; Chinstrap 4.265907 a #\u0026gt; Adelie 4.251426 a #\u0026gt; Gentoo 3.972772 b      Exercise 5  Bring it all together in a plot.\n  Hints (click here)  Think about what you\u0026rsquo;d like to display and go back to section 5 for more help.    Solutions (click here)  Using Kruskal-Wallis and Dunn's post-hoc test (bill_depth_plot_kruskal \u0026lt;- male_penguins %\u0026gt;% ggplot(aes(x = species, y = bill_depth_mm, color = species)) + geom_boxplot(outlier.shape = NA) + geom_jitter(width = 0.3) + theme_classic() + theme(legend.position = \"none\") + # remove legend bc we don't need it labs(x = \"Species\", y = \"Bill Depth, in mm\", title = \"Penguin Culmen Bill Depth Among Different Species\", subtitle = \"Data collected from Palmer LTER, Antarctica\"))   bill_depth_max \u0026lt;- male_penguins %\u0026gt;% group_by(species) %\u0026gt;% summarize(max_bill_depth_mm = max(bill_depth_mm)) bill_depth_max #\u0026gt; # A tibble: 3 × 2 #\u0026gt; species max_bill_depth_mm #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 Adelie 21.5 #\u0026gt; 2 Chinstrap 20.8 #\u0026gt; 3 Gentoo 17.3  View(dunn_bill_depth)     .y. group1 group2 n1 n2 statistic p p.adj p.adj.signif     bill_depth_mm Adelie Chinstrap 73 34 0.9039517 0.366021 0.366021 ns   bill_depth_mm Adelie Gentoo 73 61 -9.5885513 0.000000 0.000000 ****   bill_depth_mm Chinstrap Gentoo 34 61 -8.6487581 0.000000 0.000000 ****     dunn_depth_for_plotting \u0026lt;- data.frame(species = c(\"Adelie\", \"Chinstrap\", \"Gentoo\"), groups = c(\"a\", \"a\", \"b\")) depth_for_plotting_kruskal \u0026lt;- full_join(dunn_depth_for_plotting, bill_depth_max, by = \"species\") depth_for_plotting_kruskal #\u0026gt; species groups max_bill_depth_mm #\u0026gt; 1 Adelie a 21.5 #\u0026gt; 2 Chinstrap a 20.8 #\u0026gt; 3 Gentoo b 17.3  Let\u0026rsquo;s plot.\nbill_depth_plot_kruskal + geom_text(data = depth_for_plotting_kruskal, aes(x = species, y = 1 + max_bill_depth_mm, color = species, label = groups)) + labs(caption = \"Different letters indicate significant difference as determined by \\nthe Kruskal Wallis with Dunn's test for post-hoc means separation\")   Using ANOVA and Bonferroni post-hoc test\nbonferroni_bill_depth$groups #\u0026gt; bill_depth_mm_log2 groups #\u0026gt; Chinstrap 4.265907 a #\u0026gt; Adelie 4.251426 a #\u0026gt; Gentoo 3.972772 b bonferroni_bill_depth_plotting \u0026lt;- bonferroni_bill_depth$groups %\u0026gt;% rownames_to_column(var = \"species\") bonferroni_bill_depth_plotting #\u0026gt; species bill_depth_mm_log2 groups #\u0026gt; 1 Chinstrap 4.265907 a #\u0026gt; 2 Adelie 4.251426 a #\u0026gt; 3 Gentoo 3.972772 b  bonferroni_bill_depth_plotting \u0026lt;- full_join(bonferroni_bill_depth_plotting, bill_depth_max, by = \"species\") bonferroni_bill_depth_plotting #\u0026gt; species bill_depth_mm_log2 groups max_bill_depth_mm #\u0026gt; 1 Chinstrap 4.265907 a 20.8 #\u0026gt; 2 Adelie 4.251426 a 21.5 #\u0026gt; 3 Gentoo 3.972772 b 17.3  (bill_depth_plot_bonf \u0026lt;- male_penguins %\u0026gt;% ggplot(aes(x = species, y = bill_depth_mm, color = species)) + geom_boxplot(outlier.shape = NA) + geom_jitter(width = 0.3) + geom_text(data = bonferroni_bill_depth_plotting, aes(x = species, y = 1 + max_bill_depth_mm, color = species, label = groups))) + theme_classic() + theme(legend.position = \"none\") + # remove legend bc we don't need it labs(x = \"Species\", y = \"Bill Depth, in mm\", title = \"Penguin Culmen Bill Depth Among Different Species\", subtitle = \"Data collected from Palmer LTER, Antarctica\", caption = \"Different letters indicate significant difference as determined by \\none-way ANOVA with Bonferroni post-hoc means separation\")       ","date":1643846400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656516186,"objectID":"83b3e2d6ff4bcc2161fe8c576a0c6385","permalink":"https://biodash.github.io/codeclub/s03e04_anova2/","publishdate":"2022-02-03T00:00:00Z","relpermalink":"/codeclub/s03e04_anova2/","section":"codeclub","summary":"During this second session of Code Club on ANOVA, we will learn to test whether our data meetings assumptions needed for ANOVA, run non-parametric ANOVA tests and use the output for creating plots with our statistical findings.","tags":null,"title":"S03E04: ANOVA 2","type":"codeclub"},{"authors":["Jessica Cooperstone"],"categories":null,"content":" Prep homework Basic computer setup   If you didn\u0026rsquo;t already do this, please follow the Code Club Computer Setup instructions, which also has pointers for if you\u0026rsquo;re new to R or RStudio.\n  If you\u0026rsquo;re able to do so, please open RStudio a bit before Code Club starts \u0026ndash; and in case you run into issues, please join the Zoom call early and we\u0026rsquo;ll help you troubleshoot.\n  New to ggplot? This isn\u0026rsquo;t a ggplot specific session, though we will be using it a bit. Check out the past Code Club sessions covering ggplot2:\n S01E04: intro to ggplot2 S01E05: intro to ggplot2 round 2 S01E10: faceting and animating S02E06: another intro to ggplot2 S02E07: a second intro to ggplot2 round 2 S02E08: combining plots using faceting S02E09: combining plots using faceting and patchwork S02E10: adding statistics to plots S02E11: making interactive plots with plotly  If you\u0026rsquo;ve never used ggplot2 before (or even if you have), you may find this cheat sheet useful.\nAdding statistics to plots We had a previous session S02E10 developed by Daniel Quiroz that covers the package ggpubr and adding statistics to ggplots.\nWe already did t-tests Mike Sovic covered in the last code club S03E01 how to run t-tests in R. We will be building on what we learned last week.\nGetting Started   Click here to get an Rmd (optional)  RMarkdown for today # directory  dir.create(\"S03E02\") # directory for our RMarkdown # (\"recursive\" to create two levels at once.) dir.create(\"S03E02/Rmd/\") # save the url location for today's script todays_Rmd \u0026lt;- \"https://raw.githubusercontent.com/biodash/biodash.github.io/master/content/codeclub/S03E02_anova/anova.Rmd\" # indicate the name of the new Rmd S03E02_Rmd \u0026lt;- \"S03E02/Rmd/S03E02_anova.Rmd\" # go get that file!  download.file(url = todays_Rmd, destfile = S03E02_Rmd)     Introduction Often people are first introduced to the R programming language when they are wanting to conduct statistical analyses. My experience is that beginners are often able to conduct the analysis they want, and print their results to the console. But, the process of locating and then using the output of their analysis tends to be more complex.\nToday, we are going to go over how to:\n test if our data is suitable for running ANOVA run an ANOVA test run posthoc tests to understand group differences use the ANOVA data output object as a means to understand R data structure.  The purpose of today\u0026rsquo;s session is more to give you practical experience with running and retrieving ANOVA analysis output, than teaching about the assumptions and background of the test itself.\nIf you are looking for a good statistics class, I would recommend Dr. Kristin Mercer\u0026rsquo;s HCS 8887 Experimental Design.\n - Load libraries, get data We are going to start with our favorite dataset palmerpenguins to provide the input data for our analysis.\nIf you don\u0026rsquo;t have any of the packages below, use install.packages() to download them.\nlibrary(tidyverse) library(palmerpenguins) # for data library(rstatix) # for testing assumptions library(agricolae) # for post-hoc comparison of groups   1 - Getting acclimated Some words on syntax: the dataset penguins is an object within the palmerpenguins package. If you call the object penguins (after executing library(palmerpenguins)), you will be able to see what is contained within that dataframe.\npenguins #\u0026gt; # A tibble: 344 × 8 #\u0026gt; species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 Adelie Torgersen 39.1 18.7 181 3750 #\u0026gt; 2 Adelie Torgersen 39.5 17.4 186 3800 #\u0026gt; 3 Adelie Torgersen 40.3 18 195 3250 #\u0026gt; 4 Adelie Torgersen NA NA NA NA #\u0026gt; 5 Adelie Torgersen 36.7 19.3 193 3450 #\u0026gt; 6 Adelie Torgersen 39.3 20.6 190 3650 #\u0026gt; 7 Adelie Torgersen 38.9 17.8 181 3625 #\u0026gt; 8 Adelie Torgersen 39.2 19.6 195 4675 #\u0026gt; 9 Adelie Torgersen 34.1 18.1 193 3475 #\u0026gt; 10 Adelie Torgersen 42 20.2 190 4250 #\u0026gt; # … with 334 more rows, and 2 more variables: sex \u0026lt;fct\u0026gt;, year \u0026lt;int\u0026gt;  However, penguins will not be in your environment tab because it is not in your local environment. You can use it without it being in your local environment, but if you are bothered by this, you can save a copy in your local environment such it shows up in that top right pane.\npenguins \u0026lt;- penguins  What is within this dataset?\nglimpse(penguins) #\u0026gt; Rows: 344 #\u0026gt; Columns: 8 #\u0026gt; $ species \u0026lt;fct\u0026gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel… #\u0026gt; $ island \u0026lt;fct\u0026gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse… #\u0026gt; $ bill_length_mm \u0026lt;dbl\u0026gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, … #\u0026gt; $ bill_depth_mm \u0026lt;dbl\u0026gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, … #\u0026gt; $ flipper_length_mm \u0026lt;int\u0026gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186… #\u0026gt; $ body_mass_g \u0026lt;int\u0026gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, … #\u0026gt; $ sex \u0026lt;fct\u0026gt; male, female, female, NA, female, male, female, male… #\u0026gt; $ year \u0026lt;int\u0026gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…  Illustration by Allison Horst\n 2. ANOVA function We want to see if there are any differences in bill length (bill_length_mm) in penguins by sex, by species, or by their interaction. We do this using ANOVA.\nIllustration by Allison Horst\nFirst let\u0026rsquo;s get some descriptive information about our data.\npenguins %\u0026gt;% drop_na() %\u0026gt;% group_by(species, sex) %\u0026gt;% count() #\u0026gt; # A tibble: 6 × 3 #\u0026gt; # Groups: species, sex [6] #\u0026gt; species sex n #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 Adelie female 73 #\u0026gt; 2 Adelie male 73 #\u0026gt; 3 Chinstrap female 34 #\u0026gt; 4 Chinstrap male 34 #\u0026gt; 5 Gentoo female 58 #\u0026gt; 6 Gentoo male 61  The most commonly used function to run ANOVA in R is called aov() which is a part of the stats package that is pre-loaded with base R. So no new packages need to be installed here.\nIf we want to learn more about the function aov() we can do so using the code below. The help documentation will show up in the bottom right quadrant of your RStudio.\n?aov()  We can run an ANOVA by indicating our model, and here I\u0026rsquo;m also selecting to drop the NAs.\nbill_length_anova \u0026lt;- aov(data = penguins %\u0026gt;% drop_na(), bill_length_mm ~ species + sex + species*sex)  summary(bill_length_anova) #\u0026gt; Df Sum Sq Mean Sq F value Pr(\u0026gt;F)  #\u0026gt; species 2 7015 3508 654.189 \u0026lt;2e-16 *** #\u0026gt; sex 1 1136 1136 211.807 \u0026lt;2e-16 *** #\u0026gt; species:sex 2 24 12 2.284 0.103  #\u0026gt; Residuals 327 1753 5  #\u0026gt; --- #\u0026gt; Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1  Illustration by Allison Horst\nWe can take the output of our ANOVA and use the function tidy() within the broom package to turn our output into a tidy table. Here, the notation broom::tidy() means I want to use the function tidy() that is a part of the broom package. This works even though I haven\u0026rsquo;t called library(broom) at the beginning of my script.\ntidy_anova \u0026lt;- broom::tidy(bill_length_anova) print(tidy_anova) #\u0026gt; # A tibble: 4 × 6 #\u0026gt; term df sumsq meansq statistic p.value #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 species 2 7015. 3508. 654. 5.03e-115 #\u0026gt; 2 sex 1 1136. 1136. 212. 2.42e- 37 #\u0026gt; 3 species:sex 2 24.5 12.2 2.28 1.03e- 1 #\u0026gt; 4 Residuals 327 1753. 5.36 NA NA   We can also look at our data by visually plotting it, as below.\npenguins %\u0026gt;% drop_na() %\u0026gt;% ggplot(aes(x = species, y = bill_length_mm, color = sex)) + geom_boxplot()   3. Posthoc group analysis Now that we\u0026rsquo;ve seen that sex and species are significant effectors of bill_length_mm, our next logical question might be, which groups are different from each other? We can determine this by conducting post-hoc tests. We will do our post-hoc analysis using Tukey\u0026rsquo;s Honestly Significant Difference test and the function HSD.test() which is a part of the useful package agricolae.\ntukey_bill_length \u0026lt;- HSD.test(bill_length_anova, trt = c(\"species\", \"sex\"), console = TRUE) # prints the results to console #\u0026gt;  #\u0026gt; Study: bill_length_anova ~ c(\"species\", \"sex\") #\u0026gt;  #\u0026gt; HSD Test for bill_length_mm  #\u0026gt;  #\u0026gt; Mean Square Error: 5.361892  #\u0026gt;  #\u0026gt; species:sex, means #\u0026gt;  #\u0026gt; bill_length_mm std r Min Max #\u0026gt; Adelie:female 37.25753 2.028883 73 32.1 42.2 #\u0026gt; Adelie:male 40.39041 2.277131 73 34.6 46.0 #\u0026gt; Chinstrap:female 46.57353 3.108669 34 40.9 58.0 #\u0026gt; Chinstrap:male 51.09412 1.564558 34 48.5 55.8 #\u0026gt; Gentoo:female 45.56379 2.051247 58 40.9 50.5 #\u0026gt; Gentoo:male 49.47377 2.720594 61 44.4 59.6 #\u0026gt;  #\u0026gt; Alpha: 0.05 ; DF Error: 327  #\u0026gt; Critical Value of Studentized Range: 4.054126  #\u0026gt;  #\u0026gt; Groups according to probability of means differences and alpha level( 0.05 ) #\u0026gt;  #\u0026gt; Treatments with the same letter are not significantly different. #\u0026gt;  #\u0026gt; bill_length_mm groups #\u0026gt; Chinstrap:male 51.09412 a #\u0026gt; Gentoo:male 49.47377 b #\u0026gt; Chinstrap:female 46.57353 c #\u0026gt; Gentoo:female 45.56379 c #\u0026gt; Adelie:male 40.39041 d #\u0026gt; Adelie:female 37.25753 e  Like we did with t-tests, you can also look at the resulting HSD.test object (here, tukey_bill_length) in your environment pane.\nHere, instead of using the broom package, you can convert the part of the tukey_bill_length object that contains the post-hoc groupings into a dataframe using as.data.frame().\ntidy_tukey \u0026lt;- as.data.frame(tukey_bill_length$groups) tidy_tukey #\u0026gt; bill_length_mm groups #\u0026gt; Chinstrap:male 51.09412 a #\u0026gt; Gentoo:male 49.47377 b #\u0026gt; Chinstrap:female 46.57353 c #\u0026gt; Gentoo:female 45.56379 c #\u0026gt; Adelie:male 40.39041 d #\u0026gt; Adelie:female 37.25753 e   4. Bringing it together in a plot We already looked at a first-pass plot, but let\u0026rsquo;s customize it now, and add our statistical info. Here is our base plot.\npenguins %\u0026gt;% drop_na() %\u0026gt;% ggplot(aes(x = species, y = bill_length_mm, color = sex)) + geom_boxplot()   First let\u0026rsquo;s make the plot more aesthetically pleasing.\n(bill_length_plot \u0026lt;- penguins %\u0026gt;% drop_na() %\u0026gt;% ggplot(aes(x = species, y = bill_length_mm, color = sex)) + geom_boxplot() + theme_classic() + labs(x = \"Species\", y = \"Bill Length, in mm\", color = \"Sex\", title = \"Penguin Culmen Bill Length Among Different Species, and by Sex\", subtitle = \"Data collected from Palmer LTER, Antarctica\"))   We want to add the letters to this plot, so we can tell which groups of species by sex are significantly different. We are going to figure out what the maximum bill_length_mm for each species by sex is, so it will help us determine where to put our letter labels. Then, we can add our labels to be higher than the largest data point.\nbill_length_max \u0026lt;- penguins %\u0026gt;% drop_na() %\u0026gt;% group_by(species, sex) %\u0026gt;% summarize(max_bill_length_mm = max(bill_length_mm)) #\u0026gt; `summarise()` has grouped output by 'species'. You can override using the `.groups` argument. bill_length_max #\u0026gt; # A tibble: 6 × 3 #\u0026gt; # Groups: species [3] #\u0026gt; species sex max_bill_length_mm #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 Adelie female 42.2 #\u0026gt; 2 Adelie male 46  #\u0026gt; 3 Chinstrap female 58  #\u0026gt; 4 Chinstrap male 55.8 #\u0026gt; 5 Gentoo female 50.5 #\u0026gt; 6 Gentoo male 59.6  Let\u0026rsquo;s add our post-hoc group info to bill_length_max, since those two dataframes are not in the same order. We are going to use the function separate() which we used back in a previous code club.\ntidier_tukey \u0026lt;- tidy_tukey %\u0026gt;% rownames_to_column() %\u0026gt;% separate(col = rowname, into = c(\"species\", \"sex\"), sep = \":\") bill_for_plotting \u0026lt;- full_join(tidier_tukey, bill_length_max, by = c(\"species\", \"sex\"))  Let\u0026rsquo;s plot.\nbill_length_plot + geom_text(data = bill_for_plotting, aes(x = species, y = 5 + max_bill_length_mm, color = sex, label = groups))   Almost there. We want the letters to be over the right box plot (coloring here by sex helps us to see what is going on better). Let\u0026rsquo;s fix it.\nbill_length_plot + geom_text(data = bill_for_plotting, aes(x = species, y = 3 + max_bill_length_mm, color = sex, label = groups), position = position_dodge(width = 0.75), show.legend = FALSE) + labs(caption = \"Groups with different letters are statistically different using a\\n two way ANOVA and Tukey's post-hoc test\")   Also remember Daniel showed us how we can do somthing similar using the package ggpubr.\n Breakout rooms We have investigated bill_length_mm - but what about bill_depth_mm? Let\u0026rsquo;s investigate only the male penguins.\nlibrary(palmerpenguins) head(penguins) #\u0026gt; # A tibble: 6 × 8 #\u0026gt; species island bill_length_mm bill_depth_mm flipper_length_… body_mass_g sex  #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; #\u0026gt; 1 Adelie Torge… 39.1 18.7 181 3750 male  #\u0026gt; 2 Adelie Torge… 39.5 17.4 186 3800 fema… #\u0026gt; 3 Adelie Torge… 40.3 18 195 3250 fema… #\u0026gt; 4 Adelie Torge… NA NA NA NA NA  #\u0026gt; 5 Adelie Torge… 36.7 19.3 193 3450 fema… #\u0026gt; 6 Adelie Torge… 39.3 20.6 190 3650 male  #\u0026gt; # … with 1 more variable: year \u0026lt;int\u0026gt;  Exercise 1  Conduct an ANOVA to see if there are significant differences in bill_depth_mm in the Palmer penguins by by species.\n  Hints (click here)  Use the function [`aov()`](https://rdrr.io/r/stats/aov.html). Make sure you provide a model formula.    Solutions (click here)  bill_depth_anova \u0026lt;- aov(data = penguins %\u0026gt;% drop_na() %\u0026gt;% filter(sex == \"male\"), bill_depth_mm ~ species) summary(bill_depth_anova) #\u0026gt; Df Sum Sq Mean Sq F value Pr(\u0026gt;F)  #\u0026gt; species 2 453.0 226.51 294.7 \u0026lt;2e-16 *** #\u0026gt; Residuals 165 126.8 0.77  #\u0026gt; --- #\u0026gt; Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1      Exercise 2  Conduct post-hoc tests to see where significant differences exist between your species. You can use any post-hoc test you like.\n  Hints (click here)  Check out the function [`LSD.test`](https://www.rdocumentation.org/packages/agricolae/versions/1.3-5/topics/LSD.test) and the p-value adjustment procedures.    Solutions (click here)  Using a Bonferroni correction bonferroni_bill_depth \u0026lt;- LSD.test(bill_depth_anova, trt = \"species\", p.adj = \"bonferroni\", console = TRUE) #\u0026gt;  #\u0026gt; Study: bill_depth_anova ~ \"species\" #\u0026gt;  #\u0026gt; LSD t Test for bill_depth_mm  #\u0026gt; P value adjustment method: bonferroni  #\u0026gt;  #\u0026gt; Mean Square Error: 0.7686065  #\u0026gt;  #\u0026gt; species, means and individual ( 95 %) CI #\u0026gt;  #\u0026gt; bill_depth_mm std r LCL UCL Min Max #\u0026gt; Adelie 19.07260 1.0188856 73 18.87000 19.27520 17.0 21.5 #\u0026gt; Chinstrap 19.25294 0.7612730 34 18.95608 19.54981 17.5 20.8 #\u0026gt; Gentoo 15.71803 0.7410596 61 15.49640 15.93966 14.1 17.3 #\u0026gt;  #\u0026gt; Alpha: 0.05 ; DF Error: 165 #\u0026gt; Critical Value of t: 2.418634  #\u0026gt;  #\u0026gt; Groups according to probability of means differences and alpha level( 0.05 ) #\u0026gt;  #\u0026gt; Treatments with the same letter are not significantly different. #\u0026gt;  #\u0026gt; bill_depth_mm groups #\u0026gt; Chinstrap 19.25294 a #\u0026gt; Adelie 19.07260 a #\u0026gt; Gentoo 15.71803 b  Using Tukey\u0026rsquo;s posthoc test\ntukey_bill_length \u0026lt;- HSD.test(bill_depth_anova, trt = \"species\", console = TRUE) # prints the results to console #\u0026gt;  #\u0026gt; Study: bill_depth_anova ~ \"species\" #\u0026gt;  #\u0026gt; HSD Test for bill_depth_mm  #\u0026gt;  #\u0026gt; Mean Square Error: 0.7686065  #\u0026gt;  #\u0026gt; species, means #\u0026gt;  #\u0026gt; bill_depth_mm std r Min Max #\u0026gt; Adelie 19.07260 1.0188856 73 17.0 21.5 #\u0026gt; Chinstrap 19.25294 0.7612730 34 17.5 20.8 #\u0026gt; Gentoo 15.71803 0.7410596 61 14.1 17.3 #\u0026gt;  #\u0026gt; Alpha: 0.05 ; DF Error: 165  #\u0026gt; Critical Value of Studentized Range: 3.344694  #\u0026gt;  #\u0026gt; Groups according to probability of means differences and alpha level( 0.05 ) #\u0026gt;  #\u0026gt; Treatments with the same letter are not significantly different. #\u0026gt;  #\u0026gt; bill_depth_mm groups #\u0026gt; Chinstrap 19.25294 a #\u0026gt; Adelie 19.07260 a #\u0026gt; Gentoo 15.71803 b      Exercise 3  Make a plot to express your findings. I will leave it up to you to decide what this plot will look like. Add your statistical findings.\n  Hints (click here)  Review the information in section 4 of this post. You could also use the package `ggpubr`.    Solutions (click here)  Preparing to plot. bill_depth_max \u0026lt;- penguins %\u0026gt;% drop_na() %\u0026gt;% filter(sex == \"male\") %\u0026gt;% group_by(species) %\u0026gt;% summarize(max_bill_depth_mm = max(bill_depth_mm)) bill_depth_max #\u0026gt; # A tibble: 3 × 2 #\u0026gt; species max_bill_depth_mm #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 Adelie 21.5 #\u0026gt; 2 Chinstrap 20.8 #\u0026gt; 3 Gentoo 17.3  # grab group information from bonferroni test # species is a rowname instead of column so lets change that bonferroni_bill_depth_groups \u0026lt;- as.data.frame(bonferroni_bill_depth$groups %\u0026gt;% rownames_to_column(var = \"species\")) # join dfs bill_depth_for_plotting \u0026lt;- full_join(bill_depth_max, bonferroni_bill_depth_groups, by = \"species\") # check bill_depth_for_plotting #\u0026gt; # A tibble: 3 × 4 #\u0026gt; species max_bill_depth_mm bill_depth_mm groups #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt;  #\u0026gt; 1 Adelie 21.5 19.1 a  #\u0026gt; 2 Chinstrap 20.8 19.3 a  #\u0026gt; 3 Gentoo 17.3 15.7 b  (bill_depth_plot \u0026lt;- penguins %\u0026gt;% drop_na() %\u0026gt;% filter(sex == \"male\") %\u0026gt;% ggplot(aes(x = species, y = bill_depth_mm, color = species)) + geom_boxplot(outlier.shape = NA) + geom_jitter(width = 0.2) + geom_text(data = bill_depth_for_plotting, aes(x = species, y = 1 + max_bill_depth_mm, label = groups), color = \"black\") + theme_classic() + theme(legend.position = \"none\", plot.caption = element_text(hjust = 0)) + labs(x = \"Species\", y = \"Bill Depth, in mm\", title = \"Penguin Culmen Bill Depth Among Different Species\", subtitle = \"Data collected from Palmer LTER, Antarctica\", caption = \"Species significantly affects bill depth as determined by one-way ANOVA \\nwith significantly different species using Bonferroni post-hoc test at P \u0026lt; 0.05 indicated with different letters.\"))       Extra material This session was getting a bit long so I\u0026rsquo;m putting a section about testing assumptions here. ### Testing assumptions\nI know I said we weren\u0026rsquo;t going to talk about this, but I thought I\u0026rsquo;d be remiss if I didn\u0026rsquo;t show you how to test that you aren\u0026rsquo;t violating any of the assumptions needed to conduct an ANOVA. We went over this a little bit back in the session put together by Daniel Quiroz on ggpubr and adding statistical results to ggplots.\nBriefly, in order to use parametric procedures (like ANOVA), we need to be sure our data meets the assumptions for 1) normality and 2) constant variance. This can be done in a few different ways.\nShapiro-Wilk test for normality We are going to use the Shapiro-Wilk test (using the function shapiro_test() which is in the package rstatix to determine normality, but will do it groupwise. This function is a pipe-friendly wrapper for the function shapiro.test(), which just means you can use it with pipes.\npenguins %\u0026gt;% drop_na() %\u0026gt;% rstatix::shapiro_test(bill_length_mm) #\u0026gt; # A tibble: 1 × 3 #\u0026gt; variable statistic p #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 bill_length_mm 0.974 0.0000119 penguins %\u0026gt;% drop_na() %\u0026gt;% group_by(species, sex) %\u0026gt;% rstatix::shapiro_test(bill_length_mm) #\u0026gt; # A tibble: 6 × 5 #\u0026gt; species sex variable statistic p #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 Adelie female bill_length_mm 0.991 0.895  #\u0026gt; 2 Adelie male bill_length_mm 0.986 0.607  #\u0026gt; 3 Chinstrap female bill_length_mm 0.883 0.00170 #\u0026gt; 4 Chinstrap male bill_length_mm 0.955 0.177  #\u0026gt; 5 Gentoo female bill_length_mm 0.989 0.895  #\u0026gt; 6 Gentoo male bill_length_mm 0.940 0.00511  Can we visualize normality in another way?\npenguins %\u0026gt;% drop_na() %\u0026gt;% ggplot(aes(x = bill_length_mm)) + geom_histogram() + facet_grid(cols = vars(species), rows = vars(sex)) #\u0026gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.   Equal variance We can test for equal variance using Levene\u0026rsquo;s test, levene_test() which is part of the rstatix package. Again, this is a pipe-friendly wrapper for the function levene.test().\nrstatix::levene_test(data = penguins %\u0026gt;% drop_na(), bill_length_mm ~ species*sex) #\u0026gt; # A tibble: 1 × 4 #\u0026gt; df1 df2 statistic p #\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 5 327 1.40 0.222  Extra exercise 1  Test the assumptions used by ANOVA to see if it is an appropriate test for you to use in this case. If it is not, find out what the appropriate test to use is, and then use it!\n  Hints (click here)  Test for normality and equal variance using [`shapiro_test()`](https://rpkgs.datanovia.com/rstatix/reference/shapiro_test.html) and [`levene_test()`](https://rpkgs.datanovia.com/rstatix/reference/levene_test.html) respectively.    Solutions (click here)  Testing for normality: penguins %\u0026gt;% drop_na() %\u0026gt;% filter(sex == \"male\") %\u0026gt;% group_by(species) %\u0026gt;% rstatix::shapiro_test(bill_depth_mm) #\u0026gt; # A tibble: 3 × 4 #\u0026gt; species variable statistic p #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 Adelie bill_depth_mm 0.964 0.0335 #\u0026gt; 2 Chinstrap bill_depth_mm 0.983 0.863  #\u0026gt; 3 Gentoo bill_depth_mm 0.980 0.401  Testing for equal variance:\nrstatix::levene_test(data = penguins %\u0026gt;% drop_na() %\u0026gt;% filter(sex == \"male\"), bill_depth_mm ~ species*sex) #\u0026gt; # A tibble: 1 × 4 #\u0026gt; df1 df2 statistic p #\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 2 165 2.30 0.103  We are finding non-normal distribution of the male, Adelie penguins. I will take this opportunity to show you how to run non-parametric tests as well.\n    Extra exercise 2  Conduct an non-parametric ANOVA to see if there are significant differences in bill_depth_mm in the Palmer penguins by by species.\n  Hints (click here)  The non-parametric version of a one-way ANOVA is the Kruskal-Wallis test, and you can use the `rstatix` function [`kruskal_test()`](https://www.rdocumentation.org/packages/rstatix/versions/0.7.0/topics/kruskal_test).    Solutions (click here)  bill_depth_kruskal \u0026lt;- penguins %\u0026gt;% drop_na() %\u0026gt;% filter(sex == \"male\") %\u0026gt;% kruskal_test(bill_depth_mm ~ species) bill_depth_kruskal #\u0026gt; # A tibble: 1 × 6 #\u0026gt; .y. n statistic df p method  #\u0026gt; * \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt;  #\u0026gt; 1 bill_depth_mm 168 116. 2 6e-26 Kruskal-Wallis      ","date":1642636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1642680398,"objectID":"ccabb063df441de0c9d13a76508eafd4","permalink":"https://biodash.github.io/codeclub/s03e02_anova/","publishdate":"2022-01-20T00:00:00Z","relpermalink":"/codeclub/s03e02_anova/","section":"codeclub","summary":"During this session of Code Club, we will learn to run parametric and non-parametric ANOVA tests and use the output for creating plots with our statistical findings.","tags":null,"title":"S03E02: ANOVA","type":"codeclub"},{"authors":["Mike Sovic"],"categories":null,"content":"\nLearning objectives   Identify the types of data structures underlying the inputs and outputs of a basic t-test in R, with focus on vectors, lists, and data frames. Use the \u0026ldquo;broom\u0026rdquo; package to clean (tidy) up t-test results. Create some t-test-related visualizations. Practice with some basic tidyverse/dplyr functions (select(), filter())   Notes For Beginners We\u0026rsquo;re starting this new session of Code Club a bit differently than we\u0026rsquo;ve started previous major sessions in that we\u0026rsquo;re not easing in with basic R material, but instead assuming a bit of prior knowledge. But if you\u0026rsquo;re new to Code Club and/or R though, don\u0026rsquo;t worry - this section, and the rest of us in Code Club are here to help. Below I\u0026rsquo;ll give a very brief overview of R, with an emphasis on things that will be relevant for today\u0026rsquo;s session. Remember too that all previous Code Club material is available online, so feel free to check that out. And please don\u0026rsquo;t hesitate to ask questions during the session, whether in the main room or in Breakout rooms. Some highlights for today\u0026rsquo;s session\u0026hellip;\nIn R there are objects and functions.\nObjects Objects are things you assign a name to, and we do that with something called the assignment operator ( \u0026lt;- )\u0026hellip;\n#assign the value 5 to an object named \"my_object\" my_object \u0026lt;- 5 #ask R to return that object my_object #\u0026gt; [1] 5   You create lots of objects as you work in R. They persist until you remove them or until you close your R session.\nEach object is categorized into a type of data structure. Some common ones in R (and ones relevant for this session) include vectors, lists, and data frames.\n Vectors: Have one dimension, meaning they can be characterized by their length. \u0026ldquo;my_object\u0026rdquo; that I created above is a vector of length 1. If vectors contain more than one entry, all the entries must be of the same type, or class, at least from R\u0026rsquo;s perspective. Numeric, character, and logical (TRUE/FALSE) are a few examples, and \u0026ldquo;my_object\u0026rdquo; is a numeric vector. Lists: A list is an object that can store multiple objects. Unlike vectors, the different objects stored in a list can be of different types. For example, a list could have three entries - 1.) a numeric vector of length 5 that stores the values 10:14, 2.) another list that itself stores several objects, and 3.) a character vector of length 3 that stores \u0026ldquo;red\u0026rdquo;, \u0026ldquo;blue\u0026rdquo;, \u0026ldquo;green\u0026rdquo;. Data Frames: Data frames store \u0026ldquo;rectangular data\u0026rdquo;, meaning datasets that are organized into colums and rows, and specifically those that have the same number of rows for each column. They are somewhat analgous to an Excel spreadsheet, assuming the data in that worksheet are perfectly rectangular.  Functions Functions in R usually have a descriptive name that gives you an idea of what they do, and they look like\u0026hellip; mean() sum() c()\nThe c() one might not be all that descriptive, but it stands for combine, and is a function that\u0026rsquo;s used a lot to combine items into vectors of length \u0026gt;1.\nInside the parentheses you often specify one or more objects for the function to operate on, and also might give some function-specific directives that tweak its behavior.\nSyntax There are essentially two dialects in R - often referred to as \u0026ldquo;base R\u0026rdquo; and the \u0026ldquo;tidyverse\u0026rdquo;. You should be able to do most anything you want in R with either approach, and they can be mixed and matched. In general, base R tends to use more parentheses, square brackets, and $\u0026rsquo;s, while with the \u0026ldquo;tidyverse\u0026rdquo; you\u0026rsquo;ll see functions like select() and filter() a lot, along with the pipe symbol ( %\u0026gt;% ) which takes results from one function and \u0026ldquo;pipes\u0026rdquo; them directly into a next function. Admittedly, essentially having two languages can lead to confusion, especially if you\u0026rsquo;re just getting started. We mostly stick with \u0026ldquo;tidy\u0026rdquo; code during Code Club, as it tends to be easier to follow/interpret.\nData Visualization R is really useful for creating data visualizations. Again, there are \u0026ldquo;base R\u0026rdquo;\u0026quot; ways to plot data and \u0026ldquo;tidyverse\u0026rdquo; ways, which utilize functions that are part of a package named \u0026ldquo;ggplot2\u0026rdquo;. While ggplot is a bit harder to learn than the base R plotting functions, it\u0026rsquo;s also much more powerful, and in sticking with the \u0026ldquo;tidy\u0026rdquo; theme, we generally use it. Plots in ggplot are built by adding one or more layers to a plot, and data are specified with aesthetics, which link aspects of the plot (like the x and y axes, colors for points or lines on the plot, etc) to columns of a data frame that store the data.\n 1 \u0026ndash; Intro Starting today, the upcoming block of Code Club sessions will center around performing some common statistical tests in R. We don\u0026rsquo;t plan to dive in to a lot of statistical details, but will instead focus on practical aspects of running the tests/analyses. We\u0026rsquo;ll pay particular attention to things like the types/classes of data that need to go in to each test, the structure of the results that come out, and since R is so good with visualization, we\u0026rsquo;ll probably do a fair amount of plotting along the way too. Let\u0026rsquo;s go ahead and load the tidyverse before we get started\u0026hellip;\nlibrary(tidyverse)   In this session we\u0026rsquo;ll do t-tests with the t.test() function. First I\u0026rsquo;m going to use the rnorm() function to get some example data to work with by generating random samples (N=20) for each of two populations from a normal distribution with a mean of 10 and standard deviation of 3.\n#generate a sample for population 1 pop1 \u0026lt;- rnorm(n = 20, mean = 10, sd = 3) #view the population 1 data pop1 #\u0026gt; [1] 10.154661 7.498932 3.426916 7.597499 11.533421 16.520010 9.607716 #\u0026gt; [8] 13.356270 5.174309 13.373562 9.085778 6.630155 6.513986 11.570739 #\u0026gt; [15] 15.113513 9.793049 11.512667 11.191199 14.772047 5.842280 #generate a sample for population 2 pop2 \u0026lt;- rnorm(n = 20, mean = 10, sd = 3) #view the population 2 data pop2 #\u0026gt; [1] 9.356629 13.664574 12.295782 16.459519 14.261572 9.605285 8.537716 #\u0026gt; [8] 9.808906 7.465455 13.998645 11.770977 14.035046 4.744950 14.298506 #\u0026gt; [15] 9.867982 8.149314 5.317449 12.199504 9.139201 9.280096   A t-test is a good choice here if we want to use our samples to draw inference around whether the true means of these two populations are different. First we can check the means of the samples\u0026hellip;\nmean(pop1) #\u0026gt; [1] 10.01344 mean(pop2) #\u0026gt; [1] 10.71286   The sample means are a bit different, but given they\u0026rsquo;re random samples, that\u0026rsquo;s not surprising even if they were drawn from the same population. The question is whether the observed difference in the means is large enough that, at some given level of confidence, we can infer that the true population means are different. Let\u0026rsquo;s take a look at the documentation for the t.test() function\u0026hellip;\n?t.test   Like documentation for all R functions, this gives us some information on how to use t.test(). In its most basic form, the only thing that has to be provided is the \u0026lsquo;x\u0026rsquo; argument, though since we have a two-sample test, we\u0026rsquo;ll need to provide both \u0026lsquo;x\u0026rsquo; and \u0026lsquo;y\u0026rsquo;. According to the documentation, these need to be numeric vectors. Let\u0026rsquo;s check that they are\u0026hellip;\nis.numeric(pop1) #\u0026gt; [1] TRUE is.vector(pop1) #\u0026gt; [1] TRUE is.numeric(pop2) #\u0026gt; [1] TRUE is.vector(pop2) #\u0026gt; [1] TRUE   Looks like our two sets of data are in the right format, so we can run the t-test\u0026hellip;\n#run the t-test t.test(x = pop1, y = pop2) #\u0026gt;  #\u0026gt; Welch Two Sample t-test #\u0026gt;  #\u0026gt; data: pop1 and pop2 #\u0026gt; t = -0.65482, df = 37.453, p-value = 0.5166 #\u0026gt; alternative hypothesis: true difference in means is not equal to 0 #\u0026gt; 95 percent confidence interval: #\u0026gt; -2.862735 1.463895 #\u0026gt; sample estimates: #\u0026gt; mean of x mean of y  #\u0026gt; 10.01344 10.71286   This result gives us several pieces of information. If it\u0026rsquo;s not immediately apparent, we can again get some information about these results from the help, where the \u0026ldquo;Value\u0026rdquo; section tells us about what\u0026rsquo;s returned by the function\u0026hellip;\nIt says the result is a list, which is a flexible data structure in R that allows you to store multiple items of different types. This particular list has 10 entries. We can see details with the str() command. First we\u0026rsquo;ll rerun t.test() and this time save the results as an object named tresult\u0026hellip;\n#run t.test and save output to 'tresult' tresult \u0026lt;- t.test(x = pop1, y = pop2) #get the structure of 'tresult' str(tresult) #\u0026gt; List of 10 #\u0026gt; $ statistic : Named num -0.655 #\u0026gt; ..- attr(*, \"names\")= chr \"t\" #\u0026gt; $ parameter : Named num 37.5 #\u0026gt; ..- attr(*, \"names\")= chr \"df\" #\u0026gt; $ p.value : num 0.517 #\u0026gt; $ conf.int : num [1:2] -2.86 1.46 #\u0026gt; ..- attr(*, \"conf.level\")= num 0.95 #\u0026gt; $ estimate : Named num [1:2] 10 10.7 #\u0026gt; ..- attr(*, \"names\")= chr [1:2] \"mean of x\" \"mean of y\" #\u0026gt; $ null.value : Named num 0 #\u0026gt; ..- attr(*, \"names\")= chr \"difference in means\" #\u0026gt; $ stderr : num 1.07 #\u0026gt; $ alternative: chr \"two.sided\" #\u0026gt; $ method : chr \"Welch Two Sample t-test\" #\u0026gt; $ data.name : chr \"pop1 and pop2\" #\u0026gt; - attr(*, \"class\")= chr \"htest\"   Another (arguably better, or at least cleaner) way to view this list is by simply clicking on tresult in the Environment window (usually top right on screen). Doing so allows us to view the object itself\u0026hellip;\nFrom this we see that the list of 10 items consists of a mixture of numerics and characters. Each of the list entries has a name, and the \u0026lsquo;$\u0026rsquo; can be used with the name of the entry to extract an item from the list (this applies to lists in general in R - not just this one). Alternatively, the square bracket notation can be used to index (pull out) items from the list. Let\u0026rsquo;s try several options for pulling out the pvalue, which is the 3rd entry\u0026hellip;\ntresult$p.value #\u0026gt; [1] 0.5165855 tresult[3] #\u0026gt; $p.value #\u0026gt; [1] 0.5165855 tresult[[3]] #\u0026gt; [1] 0.5165855   We\u0026rsquo;ve talked in a number of previous Code Club sessions about working with tidy data - this list doesn\u0026rsquo;t fall in to that category, but can be converted to a tidy object with the tidy() function from the broom package, which is installed as part of the tidyverse (though not loaded).\ntidytresult \u0026lt;- broom::tidy(tresult) tidytresult #\u0026gt; # A tibble: 1 x 10 #\u0026gt; estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high #\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 -0.699 10.0 10.7 -0.655 0.517 37.5 -2.86 1.46 #\u0026gt; # … with 2 more variables: method \u0026lt;chr\u0026gt;, alternative \u0026lt;chr\u0026gt;   The tidy() function from broom takes results from a number of base R functions and makes them tidy. Now instead of a list we have a data frame (which actually is just a special kind of list) with 10 columns (though note they don\u0026rsquo;t correspond directly to the 10 entries of the original list).\nHaving results in this tidy format allows you to use the tidy approaches we\u0026rsquo;ve worked with in the past. Let\u0026rsquo;s try plotting the means of the two samples as a simple bar (column) plot. The two sample means are in the estimate1 and estimate2 columns. First we\u0026rsquo;ll create a separate data frame that stores those means in long format (the values in one column and associated labels in a separate column).\n#get popualation 1 mean pop1_mean \u0026lt;- tidytresult %\u0026gt;% select(estimate1) %\u0026gt;% unlist() #view population 1 mean pop1_mean #\u0026gt; estimate1  #\u0026gt; 10.01344 #get popualation 1 mean pop2_mean \u0026lt;- tidytresult %\u0026gt;% select(estimate2) %\u0026gt;% unlist() #view population 2 mean pop2_mean #\u0026gt; estimate2  #\u0026gt; 10.71286 means_to_plot \u0026lt;- data.frame(\"mean\" = c(pop1_mean, pop2_mean), \"sample\" = c(\"Population 1\", \"Population 2\")) means_to_plot #\u0026gt; mean sample #\u0026gt; estimate1 10.01344 Population 1 #\u0026gt; estimate2 10.71286 Population 2   Now we can use that data frame to create the plot with ggplot2.\n#get a basic plot means_to_plot %\u0026gt;% ggplot(aes(x = sample, y = mean)) + geom_col()   And we can do a little customization\u0026hellip;\n#customize the plot a bit means_to_plot %\u0026gt;% ggplot(aes(x = sample, y = mean)) + geom_col() + theme_classic() + labs(title = \"Mean Values From Random Samples\", x = NULL, y = \"Mean Value\")   Now let\u0026rsquo;s work with some real data. R has a small example dataset named mtcars already built in. It\u0026rsquo;s a data frame with data on different models of car. It can be called with mtcars.\nmtcars #\u0026gt; mpg cyl disp hp drat wt qsec vs am gear carb #\u0026gt; Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 #\u0026gt; Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 #\u0026gt; Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 #\u0026gt; Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 #\u0026gt; Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 #\u0026gt; Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 #\u0026gt; Duster 360 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 #\u0026gt; Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 #\u0026gt; Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 #\u0026gt; Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 #\u0026gt; Merc 280C 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 #\u0026gt; Merc 450SE 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 3 #\u0026gt; Merc 450SL 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3 #\u0026gt; Merc 450SLC 15.2 8 275.8 180 3.07 3.780 18.00 0 0 3 3 #\u0026gt; Cadillac Fleetwood 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 4 #\u0026gt; Lincoln Continental 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 4 #\u0026gt; Chrysler Imperial 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4 #\u0026gt; Fiat 128 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 #\u0026gt; Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 #\u0026gt; Toyota Corolla 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 #\u0026gt; Toyota Corona 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1 #\u0026gt; Dodge Challenger 15.5 8 318.0 150 2.76 3.520 16.87 0 0 3 2 #\u0026gt; AMC Javelin 15.2 8 304.0 150 3.15 3.435 17.30 0 0 3 2 #\u0026gt; Camaro Z28 13.3 8 350.0 245 3.73 3.840 15.41 0 0 3 4 #\u0026gt; Pontiac Firebird 19.2 8 400.0 175 3.08 3.845 17.05 0 0 3 2 #\u0026gt; Fiat X1-9 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1 #\u0026gt; Porsche 914-2 26.0 4 120.3 91 4.43 2.140 16.70 0 1 5 2 #\u0026gt; Lotus Europa 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 2 #\u0026gt; Ford Pantera L 15.8 8 351.0 264 4.22 3.170 14.50 0 1 5 4 #\u0026gt; Ferrari Dino 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6 #\u0026gt; Maserati Bora 15.0 8 301.0 335 3.54 3.570 14.60 0 1 5 8 #\u0026gt; Volvo 142E 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2   As a quick reminder, select() allows you to choose columns of a data frame by name, while filter() allows you to select rows based on one or more logical (True/False) expressions.\n#example of \"select()\" mtcars %\u0026gt;% select(mpg, disp, carb) #\u0026gt; mpg disp carb #\u0026gt; Mazda RX4 21.0 160.0 4 #\u0026gt; Mazda RX4 Wag 21.0 160.0 4 #\u0026gt; Datsun 710 22.8 108.0 1 #\u0026gt; Hornet 4 Drive 21.4 258.0 1 #\u0026gt; Hornet Sportabout 18.7 360.0 2 #\u0026gt; Valiant 18.1 225.0 1 #\u0026gt; Duster 360 14.3 360.0 4 #\u0026gt; Merc 240D 24.4 146.7 2 #\u0026gt; Merc 230 22.8 140.8 2 #\u0026gt; Merc 280 19.2 167.6 4 #\u0026gt; Merc 280C 17.8 167.6 4 #\u0026gt; Merc 450SE 16.4 275.8 3 #\u0026gt; Merc 450SL 17.3 275.8 3 #\u0026gt; Merc 450SLC 15.2 275.8 3 #\u0026gt; Cadillac Fleetwood 10.4 472.0 4 #\u0026gt; Lincoln Continental 10.4 460.0 4 #\u0026gt; Chrysler Imperial 14.7 440.0 4 #\u0026gt; Fiat 128 32.4 78.7 1 #\u0026gt; Honda Civic 30.4 75.7 2 #\u0026gt; Toyota Corolla 33.9 71.1 1 #\u0026gt; Toyota Corona 21.5 120.1 1 #\u0026gt; Dodge Challenger 15.5 318.0 2 #\u0026gt; AMC Javelin 15.2 304.0 2 #\u0026gt; Camaro Z28 13.3 350.0 4 #\u0026gt; Pontiac Firebird 19.2 400.0 2 #\u0026gt; Fiat X1-9 27.3 79.0 1 #\u0026gt; Porsche 914-2 26.0 120.3 2 #\u0026gt; Lotus Europa 30.4 95.1 2 #\u0026gt; Ford Pantera L 15.8 351.0 4 #\u0026gt; Ferrari Dino 19.7 145.0 6 #\u0026gt; Maserati Bora 15.0 301.0 8 #\u0026gt; Volvo 142E 21.4 121.0 2 #example of \"filter()\" mtcars %\u0026gt;% filter(wt \u0026gt; 4) #\u0026gt; mpg cyl disp hp drat wt qsec vs am gear carb #\u0026gt; Merc 450SE 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 3 #\u0026gt; Cadillac Fleetwood 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 4 #\u0026gt; Lincoln Continental 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 4 #\u0026gt; Chrysler Imperial 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4   We\u0026rsquo;ll focus on just a couple of the variables in this dataset: Horsepower (hp) and 1/4 mile time in seconds (qsec). Note that in some cases I\u0026rsquo;ve provided a couple equivalent solutions that represent a tidy way of doing things (generally preferred) and a non-tidy (aka base R) way of doing things, just so you can see the difference if you are interested. If this is likely to confuse you, feel free to ignore the alternative (non-tidy) parts.\n Breakout Room: Does Average 1/4-mile Speed Differ For High And Low Horsepower Cars?  We\u0026rsquo;ll define \u0026ldquo;low\u0026rdquo; horsepower cars as those with \u0026lt;120 hp, and \u0026ldquo;high\u0026rdquo; horsepower cars as \u0026gt;=120 hp. First let\u0026rsquo;s get the average 1/4-mile times (qsec) for each of these groups. Save these mean times as the objects \u0026lsquo;low_hp_time\u0026rsquo; and \u0026lsquo;high_hp_time\u0026rsquo;.\n  Hint (click here)  \nUse filter(), select(), and unlist() to pull out the appropriate sets of numeric values for each group, and then mean() to get the averages. Note that unlist() takes a list or data frame object and \u0026ldquo;flattens\u0026rdquo; it, or in other words, simplifies it down to, in this case, a numeric vector.    Solution 1 (tidy) (click here)  low_hp_time \u0026lt;- mtcars %\u0026gt;% filter(hp \u0026lt; 120) %\u0026gt;% select(qsec) %\u0026gt;% unlist() %\u0026gt;% mean() low_hp_time #\u0026gt; [1] 18.91 high_hp_time \u0026lt;- mtcars %\u0026gt;% filter(hp \u0026gt;= 120) %\u0026gt;% select(qsec) %\u0026gt;% unlist() %\u0026gt;% mean() high_hp_time #\u0026gt; [1] 16.91235      Solution 2 (not tidy)  (click here)  low_hp_time \u0026lt;- mean(mtcars$qsec[mtcars$hp \u0026lt; 120]) low_hp_time #\u0026gt; [1] 18.91 high_hp_time \u0026lt;- mean(mtcars$qsec[mtcars$hp \u0026gt;= 120]) high_hp_time #\u0026gt; [1] 16.91235       Now let\u0026rsquo;s plot those mean values with a bar plot (geom_col()).\n  Hint (click here)  \nAll ggplots (geom_col() is a ggplot function) need input data from a data frame (or tibble, which is just a special data frame). First create a data frame named \u0026ldquo;time_df\u0026rdquo; that has a column named \u0026ldquo;time\u0026rdquo; that stores the average qsec values and a column named \u0026ldquo;hp_class\u0026rdquo; that indicates whether the corresponding time is for \u0026ldquo;High HP\u0026rdquo; or \u0026ldquo;Low HP\u0026rdquo; cars. Then map the ggplot \u0026ldquo;x\u0026rdquo; aesthetic to the \u0026ldquo;hp_class\u0026rdquo; column and the \u0026ldquo;y\u0026rdquo; aesthetic to the \u0026ldquo;time\u0026rdquo; column and add a geom_col() layer.\n   Solution (click here)  time_df \u0026lt;- data.frame(\"time\" = c(low_hp_time, high_hp_time), \"hp_class\" = c(\"Low HP\", \"High HP\")) time_df #\u0026gt; time hp_class #\u0026gt; 1 18.91000 Low HP #\u0026gt; 2 16.91235 High HP time_df %\u0026gt;% ggplot(aes(x = hp_class, y = time)) + geom_col()       So, do these data provide evidence for true differences in 1/4 mile speed between high and low horsepower cars? Try running a t-test to test for differences in the two groups of 1/4 mile times, and save the pvalue as an object named speed_p.\n  Hint (click here)  \nUse the t.test() function. Remember that the \u0026ldquo;x\u0026rdquo; and \u0026ldquo;y\u0026rdquo; arguments each need to be numeric vectors.    Solution 1 (tidy) (click here)  #get the values for low-horsepower cars low_hp_vals \u0026lt;- mtcars %\u0026gt;% filter(hp \u0026lt; 120) %\u0026gt;% select(qsec) %\u0026gt;% unlist() #get the values for high-horsepower cars high_hp_vals \u0026lt;- mtcars %\u0026gt;% filter(hp \u0026gt;= 120) %\u0026gt;% select(qsec) %\u0026gt;% unlist() #run the t-test and save the results speed_res \u0026lt;- t.test(x = low_hp_vals, y = high_hp_vals) speed_p \u0026lt;- speed_res$p.value speed_p #\u0026gt; [1] 0.001007059      Solution 2 (not tidy) (click here)  #get the values for low-horsepower cars low_hp_vals \u0026lt;- mtcars$qsec[mtcars$hp \u0026lt; 120] #get the values for high-horsepower cars high_hp_vals \u0026lt;- mtcars$qsec[mtcars$hp \u0026gt;= 120] #run the t-test and save the results speed_res \u0026lt;- t.test(x = low_hp_vals, y = high_hp_vals) speed_p \u0026lt;- speed_res$p.value speed_p #\u0026gt; [1] 0.001007059       Try running the same test as above, but this time, do it as a one-sided test, with the alternative hypothesis that the 1/4-mile time for the high-horsepower group will be lower than that for the low-horsepower group. How does the p-value from this test compare to the previous (two-sided) test?\n  Hint (click here)  \nCheck out the alternative argument on the t.test help page, along with information in the Details section of that same page.    Solution (click here)  #run the t-test and save the results speed_res \u0026lt;- t.test(x = low_hp_vals, y = high_hp_vals, alternative = \"greater\") speed_p \u0026lt;- speed_res$p.value speed_p #\u0026gt; [1] 0.0005035295       Bonus  Let\u0026rsquo;s visualize the data in a different way. This time, try plotting the distribution of values in each of the samples together on a single plot. See if you can recreate the plot below\u0026hellip;\n   Hint (click here)  \nFirst obtain a long data frame with the raw values for both groups in one column, and an identifier (high-hp, low-hp) in another column. You can do this by hand, using similar methods as we used in exercises above, or you can go back to the original mtcars data frame and use mutate() in combination with either ifelse() or case_when() to add the identifier column. Then use geom_histogram(), adjusting the alpha and position arguments, and customize the theme and the labels.    Solution (click here)  mtcars %\u0026gt;% mutate(\"hp_class\" = ifelse(hp \u0026lt; 120, \"Low_HP\", \"High_HP\")) %\u0026gt;% ggplot(aes(x = qsec, fill = hp_class)) + geom_histogram(alpha = 0.7, position = \"identity\") + theme_classic() + labs(x = \"Time (seconds)\", y = \"Count\", title = \"Distribution of 1/4 Mile Times By Horsepower Class\") #\u0026gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.       ","date":1641945600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1642014381,"objectID":"eb0c91c55bbdbb6b7fa83685bb498369","permalink":"https://biodash.github.io/codeclub/s03e01_ttests/","publishdate":"2022-01-12T00:00:00Z","relpermalink":"/codeclub/s03e01_ttests/","section":"codeclub","summary":"We'll start off our series on statistical tests by running some basic t-tests, extracting the results, and creating some visualizations.","tags":null,"title":"Code Club S03E01: t-tests In R","type":"codeclub"},{"authors":["Jessica Cooperstone"],"categories":null,"content":" Prep homework Basic computer setup   If you didn\u0026rsquo;t already do this, please follow the Code Club Computer Setup instructions, which also has pointers for if you\u0026rsquo;re new to R or RStudio.\n  If you\u0026rsquo;re able to do so, please open RStudio a bit before Code Club starts \u0026ndash; and in case you run into issues, please join the Zoom call early and we\u0026rsquo;ll help you troubleshoot.\n  New to ggplot? Check out the PAST Code Club sessions covering ggplot2:\n S01E04: intro to ggplot2 S01E05: intro to ggplot2 round 2 S01E10: faceting and animating S02E06: another intro to ggplot2 S02E07: a second intro to ggplot2 round 2 S02E08: combining plots using faceting S02E09: combining plots using faceting and patchwork S02E10: adding statistics to plots  If you\u0026rsquo;ve never used ggplot2 before (or even if you have), you may find this cheat sheet useful.\nGetting Started RMarkdown for today\u0026rsquo;s session # directory for Code Club Session 15: dir.create(\"S02E12\") # directory for our RMarkdown # (\"recursive\" to create two levels at once.) dir.create(\"S02E12/Rmd/\") # save the url location for today's script todays_Rmd \u0026lt;- \"https://raw.githubusercontent.com/biodash/biodash.github.io/master/content/codeclub/S02E12_plotly/plotly.Rmd\" # indicate the name of the new Rmd S02E12_Rmd \u0026lt;- \"S02E12/Rmd/S02E12_plotly.Rmd\" # go get that file!  download.file(url = todays_Rmd, destfile = S02E12_Rmd)   1 - What is plotly? Today we are going to talk about making interactive plots using Plotly. Plotly exists in a variety of programming languages, but today we will be just talking about using it in R. All of the plotly documentation can be found here.\nIf you have never used plotly before, install it with the code below.\ninstall.packages(\"plotly\")  Here are some useful links to find info about using ggplotly.\n Basic ggplot2 charts Plotly R library fundamentals Intro to ggplotly() Using layout() ggplotly() tooltips  Before we start, there are two basic ways to use plot in R using plotly:\n Using ggplotly() - this is what we will go over today because it has the same syntax as ggplot() which we have already learned Using plot_ly() - there is slightly more functionality in this function, but the syntax is all new, so I\u0026rsquo;d suggest if you can do what you want with ggplotly(), do that. The syntax is not particularly hard so don\u0026rsquo;t be scared to use it if interactive plots are something you\u0026rsquo;re very interested in.  When you are googling about using plotly, you will find a combination of ggplotly() and plot_ly() approaches, and some parts of the code are interchangable. The easiesy way to see which parts are, is to try.\nAlso note, Google gets a bit confused when googling \u0026ldquo;ggplotly\u0026rdquo; and often returns information about just ggplot, so read extra carefully when problem solving.\nThis is an example of work from my group where we have found plotly to be particularly useful.\n   (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./apples.json\", function(chart) { Plotly.plot('chart-497583261', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })();  Data from Bilbrey et al., New Phytologist 2021\n 2 - Load libraries, get data Lets load the libraries we are using for today.\nlibrary(tidyverse) library(plotly) # for making interactive plots library(glue) # for easy pasting library(htmlwidgets) # for saving html files  We are going to continue to use the pumpkins data we downloaded last week when we were learning about Shiny.\npumpkins \u0026lt;- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-10-19/pumpkins.csv') #\u0026gt; Rows: 28065 Columns: 14 #\u0026gt; ── Column specification ──────────────────────────────────────────────────────── #\u0026gt; Delimiter: \",\" #\u0026gt; chr (14): id, place, weight_lbs, grower_name, city, state_prov, country, gpc... #\u0026gt;  #\u0026gt; ℹ Use `spec()` to retrieve the full column specification for this data. #\u0026gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.  We will start with the wrangling that Matt shared with us last week, and then go from there.\npumpkins \u0026lt;- pumpkins %\u0026gt;% # separate the year column separate(col = id, into = c(\"year\", \"vegetable\"), sep = \"-\") %\u0026gt;% # find and tag the rows that do not have data mutate(delete = str_detect(place, \"\\\\d*\\\\s*Entries\")) %\u0026gt;% # filter out the rows that do not have data filter(delete==FALSE) %\u0026gt;% # remove the tagging column select(-delete) # rename the vegetables to their actual names pumpkins$vegetable \u0026lt;- pumpkins$vegetable %\u0026gt;% str_replace(\"^F$\", \"Field Pumpkin\") %\u0026gt;% str_replace(\"^P$\", \"Giant Pumpkin\") %\u0026gt;% str_replace(\"^S$\", \"Giant Squash\") %\u0026gt;% str_replace(\"^W$\", \"Giant Watermelon\") %\u0026gt;% str_replace(\"^L$\", \"Long Gourd\") %\u0026gt;% str_replace(\"^T$\", \"Tomato\") # get rid of commas in the weight_lbs column pumpkins$weight_lbs \u0026lt;- as.numeric(gsub(\",\",\"\",pumpkins$weight_lbs))  Lets look at our data structure.\nglimpse(pumpkins) #\u0026gt; Rows: 28,011 #\u0026gt; Columns: 15 #\u0026gt; $ year \u0026lt;chr\u0026gt; \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"2013\", \"201… #\u0026gt; $ vegetable \u0026lt;chr\u0026gt; \"Field Pumpkin\", \"Field Pumpkin\", \"Field Pumpkin\", \"… #\u0026gt; $ place \u0026lt;chr\u0026gt; \"1\", \"2\", \"3\", \"4\", \"5\", \"5\", \"7\", \"8\", \"9\", \"10\", \"… #\u0026gt; $ weight_lbs \u0026lt;dbl\u0026gt; 154.5, 146.5, 145.0, 140.8, 139.0, 139.0, 136.5, 136… #\u0026gt; $ grower_name \u0026lt;chr\u0026gt; \"Ellenbecker, Todd \u0026amp; Sequoia\", \"Razo, Steve\", \"Ellen… #\u0026gt; $ city \u0026lt;chr\u0026gt; \"Gleason\", \"New Middletown\", \"Glenson\", \"Combined Lo… #\u0026gt; $ state_prov \u0026lt;chr\u0026gt; \"Wisconsin\", \"Ohio\", \"Wisconsin\", \"Wisconsin\", \"Wisc… #\u0026gt; $ country \u0026lt;chr\u0026gt; \"United States\", \"United States\", \"United States\", \"… #\u0026gt; $ gpc_site \u0026lt;chr\u0026gt; \"Nekoosa Giant Pumpkin Fest\", \"Ohio Valley Giant Pum… #\u0026gt; $ seed_mother \u0026lt;chr\u0026gt; \"209 Werner\", \"150.5 Snyder\", \"209 Werner\", \"109 Mar… #\u0026gt; $ pollinator_father \u0026lt;chr\u0026gt; \"Self\", NA, \"103 Mackinnon\", \"209 Werner '12\", \"open… #\u0026gt; $ ott \u0026lt;chr\u0026gt; \"184.0\", \"194.0\", \"177.0\", \"194.0\", \"0.0\", \"190.0\", … #\u0026gt; $ est_weight \u0026lt;chr\u0026gt; \"129.00\", \"151.00\", \"115.00\", \"151.00\", \"0.00\", \"141… #\u0026gt; $ pct_chart \u0026lt;chr\u0026gt; \"20.0\", \"-3.0\", \"26.0\", \"-7.0\", \"0.0\", \"-1.0\", \"-4.0… #\u0026gt; $ variety \u0026lt;chr\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …  Note that all of the columns have the class \u0026ldquo;character\u0026rdquo; except weight_lbs which is numeric. We could just fix this now, but I\u0026rsquo;m going to show you an alternative way to handle this in a minute.\n 3 - Create base ggplot object Using the pumpkins dataset lets work towards creating a plot that shows the distribution of weights of tomatoes by country. I will show you here how you can use dplyr functions within your ggplot2 call.\npumpkins %\u0026gt;% filter(vegetable == \"Tomato\") %\u0026gt;% ggplot(aes(x = country, y = weight_lbs, color = country)) + geom_jitter()   We have a plot, its not horrible but it has a number of issues.\n The country names are getting cut off because some are too long, and there are enough of them that we are having overlapping. We have an overplotting problem The x-axis countries are ordered alphabetically. We will order our axis based on something more meaningful, like a characteristic of our data (more about this later). The aesthetics need some adjustment for a more beautiful plot  We will work on making our plot a bit better, and then we will make it interactive, such that you can hover your mouse over each datapoint, and learn more about that datapoint than what is directly visualized in the plot.\n 4 - Optimize our base plot 1. Prevent country name overlap We can do this using by using guide_axis() within a scale function, here, scale_x_discrete(). To learn more about ggplot scales, click here.\npumpkins %\u0026gt;% filter(vegetable == \"Tomato\") %\u0026gt;% ggplot(aes(x = country, y = weight_lbs, color = country)) + geom_jitter() + scale_x_discrete(guide = guide_axis(n.dodge = 2)) # dodge every other name   Wow that was easy. We still have some overlapping, though we have a big figure legend that is in this case, not necessary. Lets remove it.\npumpkins %\u0026gt;% filter(vegetable == \"Tomato\") %\u0026gt;% ggplot(aes(x = country, y = weight_lbs, color = country)) + geom_jitter() + scale_x_discrete(guide = guide_axis(n.dodge = 2)) + theme(legend.position = \"none\")   This is not the only way to fix the plot to avoid label overlap. Instead, you could put the x-axis country labels on an angle by using theme(axis.text.x = element_text(angle = 45)).\n2. Reduce overplotting For the countries that have a lot of tomato entries, its hard to see some individual data points because there are just so many of them. We can add some transparency to the datapoints such that its easier to see them. I am also playing around with color, fill, and point shape so you can see what changing those values does to the plot.\npumpkins %\u0026gt;% filter(vegetable == \"Tomato\") %\u0026gt;% ggplot(aes(x = country, y = weight_lbs, fill = country)) + geom_jitter(alpha = 0.5, color = \"black\", shape = 21) + theme(legend.position = \"none\", axis.text.x = element_text(angle = 45))   We still have overplotting but I think this is an improvement.\n3. Reorder x-axis to something meaningful Our x-axis is currently ordered alphabetically. This is really a meaningless ordering - instead lets order our data by some characteristic of the data that we want to communicate to our viewer. For example, we could order by increasing mean tomato weight. This would tell us, just by looking at the order of the x-axis, which country has on average, the biggest tomatoes. This is something that is hard to see with the data in its current form.\nRemember before we saw that each of the columns except for weight_lbs was of the class \u0026ldquo;character.\u0026rdquo; To allow reordering, we need to change country to be a factor. We can do this directly in the pumpkins dataframe, or we can do it within the ggplot call using the pipe %\u0026gt;%.\nWe will use fct_reorder() to do this, where we provide the the column we want to reorder (here, country), and what we want to reorder based on (here, weight_lbs), and what function to use for the reordering (here, .fun = mean).\npumpkins %\u0026gt;% filter(vegetable == \"Tomato\") %\u0026gt;% mutate(country = as.factor(country)) %\u0026gt;% ggplot(aes(x = fct_reorder(country, weight_lbs, .fun = mean), y = weight_lbs, fill = country)) + geom_jitter(alpha = 0.5, color = \"black\", shape = 21) + theme(legend.position = \"none\", axis.text.x = element_text(angle = 45, hjust = 1))   Now we can see easily that Switzerland has the heaviest tomatoes on average entered into this competition.\n4. Pretty it up Let\u0026rsquo;s fix up the aesthetics of the plot, and adjust the axis labels, and add a title. Note, in the title, adding \\n into your title inserts a line break.\ntomato_plot \u0026lt;- pumpkins %\u0026gt;% filter(vegetable == \"Tomato\") %\u0026gt;% mutate(country = as.factor(country)) %\u0026gt;% ggplot(aes(x = fct_reorder(country, weight_lbs, .fun = mean), y = weight_lbs, fill = country)) + geom_jitter(alpha = 0.5, color = \"black\", shape = 21) + theme_classic() + theme(legend.position = \"none\", axis.text.x = element_text(angle = 45, hjust = 1)) + labs (x = \"Country\", y = \"Weight (in lbs)\", title = \"Weights of Tomatoes by Country Entered \\nin the Great Pumpkin Commonwealth Competition\") tomato_plot   5 - Make it interactive with ggplotly() You can learn more about the ggplotly() function, including its arguments here.\nggplotly(tomato_plot)      (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./tomato1.json\", function(chart) { Plotly.plot('chart-186295374', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })(); \nWow that was easy! Note that when you hover over a data point you see the information mapped in your aes() statement \u0026ndash; this is the default. We will go over ways to change this.\n 6 - Using tooltip Using tooltip helps you to indicate what appears when you hover over different parts of your plot. You can learn more about controlling tooltip here.\nWhat if we want to hover over each point and be able to tell who grew that tomato?\nTo do this, we indicate what we want to hover with using text = in our aesthetic mappings. Then, we indicate tooltip = \u0026quot;text\u0026quot; to tell ggplotly() what we want to hover.\ntomato_plot \u0026lt;- pumpkins %\u0026gt;% filter(vegetable == \"Tomato\") %\u0026gt;% mutate(country = as.factor(country)) %\u0026gt;% ggplot(aes(x = fct_reorder(country, weight_lbs, .fun = mean), y = weight_lbs, fill = country, text = grower_name)) + geom_jitter(alpha = 0.5, color = \"black\", shape = 21) + theme_classic() + theme(legend.position = \"none\", axis.text.x = element_text(angle = 45)) + labs(x = \"Country\", y = \"Weight (in lbs)\", title = \"Weights of Tomatoes by Country Entered \\nin the Great Pumpkin Commonwealth Competition\")  ggplotly(tomato_plot, tooltip = \"text\")      (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./tomato2.json\", function(chart) { Plotly.plot('chart-637154892', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })(); \nYou can play around a lot with tooltip to get it to be exactly how you want, and you can include multiple things in your hover text.\nYou can add multiple items to text, and also use the function glue() which allows more intuitive pasting to get your hover text to in your preferred format.\ntomato_plot \u0026lt;- pumpkins %\u0026gt;% filter(vegetable == \"Tomato\") %\u0026gt;% mutate(country = as.factor(country)) %\u0026gt;% ggplot(aes(x = fct_reorder(country, weight_lbs, .fun = mean), y = weight_lbs, fill = country, text = glue(\"Grown by \u0026#123;grower_name\u0026#125; From \u0026#123;city\u0026#125;, \u0026#123;state_prov\u0026#125;\"))) + geom_jitter(alpha = 0.5, color = \"black\", shape = 21) + theme_classic() + theme(legend.position = \"none\", axis.text.x = element_text(angle = 45)) + labs(x = \"Country\", y = \"Weight (in lbs)\", title = \"Weights of Tomatoes by Country Entered \\nin the Great Pumpkin Commonwealth Competition\")  ggplotly(tomato_plot, tooltip = \"text\")      (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./tomato3.json\", function(chart) { Plotly.plot('chart-428713965', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })(); \n 7 - Hover label aesthetics You might not like the default hover text aesthetics, and can change them! You can do this using style and layout and adding these functions using the pipe %\u0026gt;%.\n# setting fonts for the plot font \u0026lt;- list( family = \"Calibri\", size = 15, color = \"white\") # setting hover label specs label \u0026lt;- list( bgcolor = \"#FF0000\", bordercolor = \"transparent\", font = font) # we can do this bc we already set font # amending our ggplotly call to include new fonts and hover label specs ggplotly(tomato_plot, tooltip = \"text\") %\u0026gt;% style(hoverlabel = label) %\u0026gt;% layout(font = font)      (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./tomato4.json\", function(chart) { Plotly.plot('chart-879154632', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })(); \n 8 - Saving your plots Now that you\u0026rsquo;ve made a beautiful interactive plot, you probably want to save it.\nAssign the plot you want to save to an object, and use the function saveWidget() to save it. You can find the documentation here.\n# assign ggplotly plot to an object ggplotly_to_save \u0026lt;- ggplotly(tomato_plot, tooltip = \"text\") %\u0026gt;% style(hoverlabel = label) %\u0026gt;% layout(font = font) # save saveWidget(widget = ggplotly_to_save, file = \"ggplotlying.html\")   Breakout rooms We are going to use the palmerpenguins dataset called penguins.\nlibrary(palmerpenguins) head(penguins) #\u0026gt; # A tibble: 6 × 8 #\u0026gt; species island bill_length_mm bill_depth_mm flipper_length_… body_mass_g sex  #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; #\u0026gt; 1 Adelie Torge… 39.1 18.7 181 3750 male  #\u0026gt; 2 Adelie Torge… 39.5 17.4 186 3800 fema… #\u0026gt; 3 Adelie Torge… 40.3 18 195 3250 fema… #\u0026gt; 4 Adelie Torge… NA NA NA NA NA  #\u0026gt; 5 Adelie Torge… 36.7 19.3 193 3450 fema… #\u0026gt; 6 Adelie Torge… 39.3 20.6 190 3650 male  #\u0026gt; # … with 1 more variable: year \u0026lt;int\u0026gt;  Exercise 1  Using the penguins dataset and make a base scatter plot with bill length on the y, and bill depth on the x. Remove any observations with missing data.\n  Hints (click here)  You can use `drop_na()` to remove NAs. The helper `any_of()` is useful for removing NAs only from certain variables. You can also just remove any NAs, it doesn't really matter here.    Solutions (click here)  bill_depth_length \u0026lt;- penguins %\u0026gt;% drop_na(any_of(c(\"bill_depth_mm\", \"bill_length_mm\"))) %\u0026gt;% ggplot(aes(x = bill_depth_mm, y = bill_length_mm)) + geom_point() bill_depth_length       Exercise 2  Add appropriate x and y-axis labels, and a title to your plot.\n  Hints (click here)  You can add labels for x, y, and a title using `labs().`    Solutions (click here)  bill_depth_length \u0026lt;- penguins %\u0026gt;% drop_na(any_of(c(\"bill_depth_mm\", \"bill_length_mm\"))) %\u0026gt;% ggplot(aes(x = bill_depth_mm, y = bill_length_mm)) + geom_point() + labs(x = \"Culmen Depth (mm)\", y = \"Culmen Length (mm)\", title = \"Exploration of penguin bill length and depth relationships\") bill_depth_length       Exercise 3  Make your plot interactive such that when you hover over a point, it tell you what island the penguin is from.\n  Hints (click here)  Specify what you want your \"tooltip\" to be by using `text` within your `aes()` statement.    Solutions (click here)  bill_depth_length \u0026lt;- penguins %\u0026gt;% drop_na(any_of(c(\"bill_depth_mm\", \"bill_length_mm\"))) %\u0026gt;% ggplot(aes(x = bill_depth_mm, y = bill_length_mm, text = island)) + geom_point() + labs(x = \"Culmen Depth (mm)\", y = \"Culmen Length (mm)\", title = \"Exploration of penguin bill length and depth relationships\") ggplotly(bill_depth_length, tooltip = \"text\")      (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./penguins1.json\", function(chart) { Plotly.plot('chart-687351492', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })(); \n    Exercise 4  Add the sex of the penguin to the hover text, change the hover text so that the background color is red, and make all the fonts for the plot something other than the default.\n  Hints (click here)  You can set fonts either within your `ggplot()` call, or setting `font` within [`layout()`](https://rdrr.io/pkg/plotly/man/layout.html). You can customize the hover label with [`style()`](https://rdrr.io/pkg/plotly/man/style.html). Use [`glue()`](https://glue.tidyverse.org/reference/glue.html) to paste in some information that helps your reader know what your hover text is referring to.    Solutions (click here)  # setting fonts for the plot penguins_font \u0026lt;- list( family = \"Proxima Nova\", # this is the official OSU font size = 15, color = \"white\") # setting hover label specs penguins_label \u0026lt;- list( bgcolor = \"blue\", bordercolor = \"transparent\", font = penguins_font) # we can do this bc we already set font bill_depth_length \u0026lt;- penguins %\u0026gt;% drop_na(any_of(c(\"bill_depth_mm\", \"bill_length_mm\"))) %\u0026gt;% ggplot(aes(x = bill_depth_mm, y = bill_length_mm, text = glue(\"Island: \u0026#123;island\u0026#125; Sex: \u0026#123;sex\u0026#125;\"))) + geom_point() + labs(x = \"Culmen Depth (mm)\", y = \"Culmen Length (mm)\", title = \"Exploration of penguin bill length and depth relationships\") # amending our ggplotly call to include new fonts and hover label specs ggplotly(bill_depth_length, tooltip = \"text\") %\u0026gt;% style(hoverlabel = penguins_label) %\u0026gt;% layout(font = penguins_font)      (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./penguins2.json\", function(chart) { Plotly.plot('chart-187923456', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })(); \n    ","date":1639008000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639065264,"objectID":"1269559e3da64675bda6e063f9b2db5c","permalink":"https://biodash.github.io/codeclub/s02e12_plotly/","publishdate":"2021-12-09T00:00:00Z","relpermalink":"/codeclub/s02e12_plotly/","section":"codeclub","summary":"During this session of Code Club, we will learn to make interactive plots using ggplotly.","tags":null,"title":"S02E12: Plotly","type":"codeclub"},{"authors":null,"categories":null,"content":"\n What will we go over today\n What is Shiny, and why would I want to use it? Understanding the principles of user interfaces and servers The structure of a basic Shiny app     Interactive Applications with R Shiny is a package that builds interactive html-based applications using the R coding language and capabilities.\nThe goal of this session is to provide a basic introduction to the structure of simple Shiny apps. For more detailed information as you work to build more complicated apps, I recommend you reference the Shiny tutorial page.\nThe code for a Shiny app contains three main parts.\n A section defining the user interface (UI) A section defining the server A piece of code that combines the UI and server sections  Let\u0026rsquo;s talk through some examples to illustrate what a UI and server are.\nA Conceptual Example Imagine you are in need for directions from Columbus, OH to Wooster, OH.\nYou decide to use a website such as Google maps to give you precise directions for the quickest route.\nYou, the user, type in your location (Columbus) and your destination (Wooster), and as if by magic, turn-by-turn directions pop up on your screen.\nEverything you have just interacted with and experienced is the User Interface of the maps application. Think of this as the front-end of the application.\nThe Server is everything going on behind the scenes- the lines of code and calculations- invisible to the user- that take the user input and produce the results that are then presented back to the user.\nA more grounded example The OSU Infectious Disease Institute maintains a webapp that tracked COVID-19 statistics in Ohio from March 2020 to March 2021. Click or navigate to: https://covidmap.osu.edu/\nThis is a nice demonstration of a \u0026ldquo;data dashboard\u0026rdquo;, which is a very common use of the Shiny package.\nIt provides a way for a user of virtually any skill level to interact with their COVID-19 data set. It may look very fancy, but it was all put together using Shiny and many R commands you already know.\nWhat function would the app use to pair down the data to the selected date range? What R package are they using the plot the daily counts of new cases?\nYou can apply what you have already learned in code club to make a fancy, interactive web app for your own data sets!\nWe just need to learn how to code R within the Shiny environment.\nLet\u0026rsquo;s start with a simple Shiny app First, take a second to install Shiny, and for the purpose of our demonstration today, I recommend you also pre-load the tidyverse (I am assuming you already have that installed)\ninstall.packages(\"shiny\") library(shiny) library(tidyverse)  You can use a common structure to set up the basic parts of almost any shiny app. Below you will see that we are creating an object for the UI, another for the server, and then we combine them with the shinyApp call. Before we start building the app, we need to tell it where to live on our computer.\nui \u0026lt;- ... server \u0026lt;- ... shinyApp(ui = ui, server = server)  Inside your directory for code club, create a folder for this week. In this folder, save a new R script with the name app.R\nShiny uses the directory as a sort of bundle to run your app from. The name of your directory is the name of the app, the app.R file is the code, and there are other file types (with specific names) that you can add to the directory later as your app gets more complicated.\ndir.create(\"S02E12\")  Let\u0026rsquo;s make an app! We are going to start simple, using a data set about large pumpkins, squashes, and tomatoes.\nFirst, let\u0026rsquo;s download the data and do a little cleanup.\n#download the dataframe from github. This data can also be found in the `tidytuesdayR` package.  pumpkins \u0026lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-10-19/pumpkins.csv') #the year and type of crop are combined into a single column, separated by a \"-\". Need to split. #also, there are some rows in the data frame that contain information about the number of entries #need to remove these interceding rows pumpkins \u0026lt;- pumpkins %\u0026gt;% #separate the year column separate(col = id, into = c(\"year\", \"vegetable\"), sep = \"-\") %\u0026gt;% #find and tag the rows that do not have data mutate(delete = str_detect(place, \"\\\\d*\\\\s*Entries\")) %\u0026gt;% #filter out the rows that do not have data filter(delete==FALSE) %\u0026gt;% #remove the tagging column select(-delete) #Rename the vegetables to their actual names pumpkins$vegetable \u0026lt;- pumpkins$vegetable %\u0026gt;% str_replace(\"^F$\", \"Field Pumpkin\") %\u0026gt;% str_replace(\"^P$\", \"Giant Pumpkin\") %\u0026gt;% str_replace(\"^S$\", \"Giant Squash\") %\u0026gt;% str_replace(\"^W$\", \"Giant Watermelon\") %\u0026gt;% str_replace(\"^L$\", \"Long Gourd\") %\u0026gt;% str_replace(\"^T$\", \"Tomato\") #get rid of commas in the weight_lbs column pumpkins$weight_lbs \u0026lt;- as.numeric(gsub(\",\",\"\",pumpkins$weight_lbs)) #look at the data structure head(pumpkins)   User Interface Now that we have our data sorted out, let\u0026rsquo;s start building out our user interface\nThe UI portion of your code is where you will define the appearance and physical layout of your app, and in Shiny, layouts are commonly defined using fluidPage(). Fluid pages scale to fill the available window size of your browser, and are formatted to a grid with 12 equally spaced columns and as many rows of equal height that you need for your app. Within the `fluidPage()' command, you define how different components of your app physically fit within the browser window.\nThere are some handy pre-sets that we can use to build an app more quickly, but just know that the interface you build is very customizable with enough effort. For the purpose of this demonstration, we will use the preset commands that define a sidebarPanel() for user inputs and a `mainPanel()' for data output.\nui \u0026lt;- fluidPage( sidebarPanel( ), mainPanel( ) ) server \u0026lt;- function(input,output)\u0026#123; \u0026#125; shinyApp(ui=ui,server=server)  This is the basic layout of our app. It may read a little funny because I am leaving space to put in different components.\nNow, back to our example dataset. The World Pumpkin Committee is very interested in being able to view the weight distribution of each vegetable by year. Let\u0026rsquo;s make them an app for that!\nWe will add a place for them to select the year in sidebarPanel() Shiny has a number of data input options. The one you pick, will depend on what information you want to get from the user. No matter the input, you will need to specify the InputID. This piece of information is very important, as it is how the server references the inputs in the UI.\nFor this example, we have a defined set of inputs (this competition has only been going on for a set number of years), so we will use selectInput(). We will also define the choices as the years avilable in our dataset.\nui \u0026lt;- fluidPage( sidebarPanel( #here we are naming the input as 'year', the box will display 'Select Year', and  #the user will choose from the competition years as referenced from the  #pumpkins dataframe selectInput(inputId = \"year\", label = \"Select Year\", choices = unique(pumpkins$year)) ), mainPanel( ) ) server \u0026lt;- function(input,output)\u0026#123; \u0026#125; shinyApp(ui=ui,server=server)  We now have the input side of our UI, but we also need to define what and where our output will be by essentially adding a placeholder for our output. There are many options in Shiny, but we will be using a plotOutput() for this example. The main thing that needs to be set is the OutputID. Similar to the InputID, this tells the server where to place our results.\nui \u0026lt;- fluidPage( sidebarPanel( #here we are naming the input as 'year', the box will display 'Select Year', and  #the user will choose from the competition years as referenced from the  #pumpkins dataframe selectInput(inputId = \"year\", label = \"Select Year\", choices = unique(pumpkins$year)) ), mainPanel( #placeholder for a plot to generate plotOutput(outputId = \"weight_distribution\") ) ) server \u0026lt;- function(input,output)\u0026#123; \u0026#125; shinyApp(ui=ui,server=server)  Run this whole chunk of code. Notice you will create a ui object and a server function that are combined with shinyApp. Once you run that last piece, a built in web browser will appear and run your app.\nWhen you are not using the app, be sure to fully exit the browser window, otherwise it will keep running in the background.\nServer Now that we have our UI built, let\u0026rsquo;s move on to the server. This is where you will code what R should do with inputs from the UI side and how results should be presented to the user.\nNotice the server is built as a function() with input and output objects.\nFor our example. we basically want to use the inputID \u0026ldquo;year\u0026rdquo; to filter our list, then we want to plot the weight distribution of each vegetable.This is done using code you are likely already familiar with.\nui \u0026lt;- fluidPage( sidebarPanel( #here we are naming the input as 'year', the box will display 'Select year', and  #the user will choose from the competition years as referenced from the  #pumpkins dataframe selectInput(inputId = \"year\", label = \"Select Year\", choices = unique(pumpkins$year)) ), mainPanel( #placeholder for a plot to generate plotOutput(outputId = \"weight_distribution\") ) ) server \u0026lt;- function(input,output)\u0026#123; #notice the callback to our outputID output$weight_distribution \u0026lt;- renderPlot( pumpkins %\u0026gt;% filter(year==input$year) %\u0026gt;% ggplot(aes(vegetable, weight_lbs)) + geom_boxplot() ) \u0026#125; shinyApp(ui=ui,server=server)  Notice how the outputID from the ui section is referenced as output$weight_distribution on the server side. Do you see where our inputID is referenced on the server side?\nAlso notice how the code is wrapped within a renderPlot command. This is a special Shiny command specific to plot outputs. There are other commands for other types of outputs, such as renderTable for table outputs.\nThese commands are also special because they are \u0026ldquo;reactive\u0026rdquo;, meaning they react to changes in user inputs and re-execute the code when those changes occur. This is a simple explanation of a somewhat complex topic. For a more in-depth explanation, see this explanation.\nRe-run the code, and you should now see a plot that will respond to changes in the year input box.\nExercise 1  Now it is your turn to add on to the app. The World Pumpkin Committee would like to also have the app generate a table that contains the weight, grower name, city, and state/province of each first place vegetable for the year selected in the input.\nBonus Explore renderTable to see how you can change the number of significant figures displayed.\n  Hints (click here)  \nFocus on the server side of the app, you already have the input from the UI that you need. Use renderTable().\n   Solution (click here)  ui \u0026lt;- fluidPage( sidebarPanel( #here we are naming the input as 'year', the box will display 'Select year', and  #the user will choose from the competition years as referenced from the  #pumpkins dataframe selectInput(inputId = \"year\", label = \"Select Year\", choices = unique(pumpkins$year)) ), mainPanel( #placeholder for a plot to generate plotOutput(outputId = \"weight_distribution\"), tableOutput(outputId = \"winner_table\") ) ) server \u0026lt;- function(input,output)\u0026#123; #notice the callback to our outputID output$weight_distribution \u0026lt;- renderPlot( pumpkins %\u0026gt;% filter(year==input$year) %\u0026gt;% ggplot(aes(vegetable, weight_lbs)) + geom_boxplot() ) #create the output for the table using renderTable output$winner_table \u0026lt;- renderTable( pumpkins %\u0026gt;% filter(year==input$year) %\u0026gt;% filter(place == \"1\") %\u0026gt;% select(vegetable, weight_lbs, grower_name, city, state_prov) %\u0026gt;% #I just wanted to rename the columns to look nicer rename(\"weight (lbs)\"=weight_lbs, \"grower name\"=grower_name, \"state/provice\"=state_prov), #setting significant figures as an option under renderTable digits = 1, #adding a caption to be fancy caption = \"Table of Winners\", caption.placement = \"top\" ) \u0026#125; shinyApp(ui=ui,server=server)     Exercise 2  Create a new app that allows a user to visualize how changing the value of \u0026ldquo;m\u0026rdquo; in y=mx+b affects the slope of a line.\n  Hints (click here)  \nYou will need to define the values of x. You can do this within the server function (i.e. a \u0026lt;- tibble(a=-100:100)).\nAlso note that changing the slope of a line may not be so noticable because plots will automatically adjust to the scale of the data. Consider locking your coordinates so you notice changing slopes more easily.\n   Solution (click here)  ui \u0026lt;- fluidPage( sidebarPanel( #I chose sliderInput here, you can choose another input type sliderInput(inputId = \"slope\", min = -10, max = 10, value = 2, label = \"Slope\") ), mainPanel( plotOutput(outputId = \"graph\") ) ) server \u0026lt;- function(input,output)\u0026#123; a \u0026lt;- tibble(a=-100:100) output$graph \u0026lt;- renderPlot( ggplot(a, aes(a, input$slope*a))+ geom_line()+ coord_cartesian(xlim = c(-25,25), ylim = c(-100,100)) ) \u0026#125; shinyApp(ui=ui,server=server)     Exercise 3  Add on to your app from Exercise 2 by providing another input for user to adjust the y-intercept.\n  Solution (click here)  ui \u0026lt;- fluidPage( sidebarPanel( sliderInput(inputId = \"slope\", min = -10, max = 10, value = 2, label = \"Slope\"), sliderInput(inputId = \"intercept\", min = -10, max = 10, value = 2, label = \"Y-Intercept\") ), mainPanel( plotOutput(outputId = \"graph\") ) ) server \u0026lt;- function(input,output)\u0026#123; a \u0026lt;- tibble(a=-100:100) output$graph \u0026lt;- renderPlot( ggplot(a, aes(a, input$slope*a+input$intercept))+ geom_line()+ coord_cartesian(xlim = c(-25,25), ylim = c(-100,100)) ) \u0026#125; shinyApp(ui=ui,server=server)     ","date":1638230400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638325869,"objectID":"6669339dbec9b19c4162031329be13dc","permalink":"https://biodash.github.io/codeclub/s02e11_shiny_intro/","publishdate":"2021-11-30T00:00:00Z","relpermalink":"/codeclub/s02e11_shiny_intro/","section":"codeclub","summary":"What will we go over today\n What is Shiny, and why would I want to use it? Understanding the principles of user interfaces and servers The structure of a basic Shiny app     Interactive Applications with R Shiny is a package that builds interactive html-based applications using the R coding language and capabilities.","tags":null,"title":"Code Club S02E11: Shiny Bright Like a Diamond","type":"codeclub"},{"authors":["Daniel Quiroz"],"categories":null,"content":"\nLearning objectives   Understand what is a ggplot extension Define what is ggpubr Recognize the advantages and drawback of ggpubr Create a density plot with ggpubr Add statistical results to density plots    Understand what is a ggplot extension A ggplot extension is a set of functions that helps in the automation of a given task. In the case of ggplot extensions, there are 102 registered extensions up today.\nggplot\u0026rsquo;s extensions are developed based on the core of ggplot and help to create a customized plot with certain features such as animation, specific color scales, or to produce a ready to publish figure.\nThe ggplot extensions website has an overview of the available extensions:\n   Define what is ggpubr ggpubr is an R package that produces ggplot-based plots with a more elegant aesthetic. Although ggpubr has default figures themes, plots usually require some formatting before sending them for publication.\n Getting everything ready First, we can take a look at the data that we are going to use. In this case, since we are already familiar with palmerpenguins dataset, we are going to continue using this data.\nTo remember the data structure, we can use the glimpse function.\n If you do not have installed the palmergenguins library, you can do it with this line. If you already have it, skip this line.\ninstall.packages(\"palmerpenguins\")  To install the ggpubr package for the first time, you can use this command.\ninstall.packages(\"ggpubr\")     Loading all required packages  A good programming style is to load all necessary packages at the very beginning of the script.\n  library(tidyverse) # To load all packages including in the tidyverse library(palmerpenguins) # Load the example data library(ggpubr) # Create ready to publish figures  Remembering the palmer penguins data glimpse(penguins) #\u0026gt; Rows: 344 #\u0026gt; Columns: 8 #\u0026gt; $ species \u0026lt;fct\u0026gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel… #\u0026gt; $ island \u0026lt;fct\u0026gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse… #\u0026gt; $ bill_length_mm \u0026lt;dbl\u0026gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, … #\u0026gt; $ bill_depth_mm \u0026lt;dbl\u0026gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, … #\u0026gt; $ flipper_length_mm \u0026lt;int\u0026gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186… #\u0026gt; $ body_mass_g \u0026lt;int\u0026gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, … #\u0026gt; $ sex \u0026lt;fct\u0026gt; male, female, female, NA, female, male, female, male… #\u0026gt; $ year \u0026lt;int\u0026gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…  For the downstream example, we are going to contrast the bill depth of the female vs male penguins of the Adelie species found on the island of Biscoe.\nTherefore, we need to filter based on species == Adelie and island == Biscoe.\n# Filter by species and island penguins_filtered \u0026lt;- penguins %\u0026gt;% filter(species == \"Adelie\", island == \"Biscoe\") # Count the occurence of the factors levels of sex, species and island penguins_filtered %\u0026gt;% count(sex, species, island) #\u0026gt; # A tibble: 2 × 4 #\u0026gt; sex species island n #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 female Adelie Biscoe 22 #\u0026gt; 2 male Adelie Biscoe 22  Creating the plot with ggplot One of the main differences between ggplot and ggpubr is the syntax to create the base plot. In the case of ggplot, it is a layer based syntax, while in ggpubr, the syntax is embedded in a single function.\nggplot(penguins_filtered, aes(x = sex, y = bill_depth_mm, fill = sex)) + geom_boxplot()   Now, since ggpubr has its own built-in geoms, we can use the ggboxplot() function:\nbase_plot \u0026lt;- ggboxplot(data = penguins_filtered, x = \"sex\", y = \"bill_depth_mm\", fill = \"sex\") base_plot    Since ggpubr creates plots based on ggplot syntax, you can treat these plots as another ggplot figure and use the same functions that you would use to format base ggplot plots.\n  base_plot \u0026lt;- ggboxplot(data = penguins_filtered, x = \"sex\", y = \"bill_depth_mm\", fill = \"sex\") + labs(title = \"Comparison between Adelie penguins by sex\", fill = \"Sex:\", x = \"Sex\", y = \"Bill depth (mm)\") base_plot    Add statistical results to the density plot When hypotheses testing is brought to the table, we need to consider which type of stats we can and cannot apply to our data. Briefly, if our data fits the assumptions of being normally distributed and having variance homogeneity, we can apply parametric tests. On the other hand, if our data does not fit the assumptions, we need to apply a nonparametric test to the data.\n   Comparison Parametric Nonparametric     2 groups t-test Wilcoxon test   \u0026gt;2 groups one-way anova Krustal-Wallis test    Checking assumptions If we would like to conduct a hypotheses test, we need first to check assumptions (homogeneity of variance and and normality):\nbartlett.test(bill_depth_mm ~ sex, data = penguins_filtered) #\u0026gt;  #\u0026gt; Bartlett test of homogeneity of variances #\u0026gt;  #\u0026gt; data: bill_depth_mm by sex #\u0026gt; Bartlett's K-squared = 0.94576, df = 1, p-value = 0.3308  Exploring the stat_compare_means() function We can use the function stat_compare_means() to annotate the plot with the results of a statistical test comparing group means:\nbase_plot + stat_compare_means()   In order to be 100% sure about the computed p-value, we can compute the Wilcoxon test in the console:\nwilcox.test(bill_depth_mm ~ sex, data = penguins_filtered) #\u0026gt;  #\u0026gt; Wilcoxon rank sum test with continuity correction #\u0026gt;  #\u0026gt; data: bill_depth_mm by sex #\u0026gt; W = 72, p-value = 6.79e-05 #\u0026gt; alternative hypothesis: true location shift is not equal to 0  Using ggplot to recreate the plot Above, we created the base plot using ggpubr syntax, but we don\u0026rsquo;t need to do this. The following code would create a very similar plot using ggplot syntax and then only adding the stat_compare_means() at the end:\nggplot(penguins_filtered, aes(sex, bill_depth_mm, fill = sex)) + geom_boxplot() + theme_bw() + stat_compare_means()   Breakout Rooms I (10 min)  Exercise 1   Filter the penguins data in order to have only the observations (rows) from the Chinstrap species from Dream island.\n  Create a boxplot using the ggpubr package or using base ggplot.\n  Use stat_compare_means() function to add a p-value.\n    Hints (click here)   Use the filter() function to filter rows Filter using species == Chinstrap and island == Dream If you want to use the ggpubr package to create the boxplot, use the ggboxplot() function.     Solution (click here)  # Using ggpubr # Filter by species and island penguins_exc1 \u0026lt;- penguins %\u0026gt;% filter(species == \"Chinstrap\", island == \"Dream\") # Create the plot exc1_plot \u0026lt;- ggboxplot(data = penguins_exc1, x = \"sex\", y = \"bill_depth_mm\", fill = \"sex\") + stat_compare_means() # Show the plot exc1_plot   # Using ggplot only # Filter by species and island penguins_exc1 \u0026lt;- penguins %\u0026gt;% filter(species == \"Chinstrap\", island == \"Dream\") # Create the plot exc1_plot_ggplot \u0026lt;- ggplot(penguins_exc1, aes(sex, bill_depth_mm, fill = sex)) + geom_boxplot() + stat_compare_means() + theme_bw() # Show the plot exc1_plot_ggplot     Multiple group comparison within a variable In many experiments we can have multiple groups in a single variable. For example, within a variable nutrient concentration we can have multiple nutrient concentration levels such as 10%, 20%, 30% and so on.\nIn the case of the penguins data we can find this layout if we need to compare the males bill depth between species. Only for teaching purposes, we are not going to consider any difference by island.\npenguins_male \u0026lt;- penguins %\u0026gt;% filter(sex == \"male\") penguins_male %\u0026gt;% count(species, sex) #\u0026gt; # A tibble: 3 × 3 #\u0026gt; species sex n #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 Adelie male 73 #\u0026gt; 2 Chinstrap male 34 #\u0026gt; 3 Gentoo male 61  penguins_male_plot \u0026lt;- ggboxplot(penguins_male, x = \"species\", y = \"bill_depth_mm\", color = \"species\") penguins_male_plot   Like before, we can use the function stat_compare_means to add a p-value for an overall comparison between groups:\npenguins_male_plot + stat_compare_means()   However, we may also be interested in getting a p-value for the different pairwise comparisons between groups. For instance, does the bill depth of Chinstrap penguins differ significantly from that in Gentoo penguins?\nTo do this, we can pass a list to that the stat_compare_means function. A list is a flexible, hierarchical R data structure. Here, we will use a list to store multiple vectors, each of which contains a pairwise comparison of interest:\ncomparison_list \u0026lt;- list(c(\"Adelie\", \"Chinstrap\"), c(\"Chinstrap\", \"Gentoo\"), c(\"Adelie\", \"Gentoo\")) penguins_male_plot + stat_compare_means(comparisons = comparison_list) + stat_compare_means(label.y = 25)   Breakout Rooms II (10 min)  Exercise 2   Filter the penguins data in order to have only the observations (rows) from female penguins.\n  Create a boxplot using the ggpubr package.\n  Add the multiple group comparison\n  Add pairwise comparisons between all groups combinations\n    Hints (click here)    Use the filter() function to select desired rows\n  Filter by sex == \u0026quot;female\u0026quot;\n  Use ggboxplot() function for the base plot\n  Use the stat_compare_means() function for multiple group comparison\n  Use the comparison argument to add pairwise comparison\n     Solution (click here)  # Filtering by sex penguins_exc2 \u0026lt;- penguins %\u0026gt;% # Filter by species and island filter(sex == \"female\") # Creating the base plot exc2_plot \u0026lt;- ggboxplot(data = penguins_exc2, x = \"species\", y = \"bill_depth_mm\", fill = \"species\") # Adding the multiple group comparison exc2_plot \u0026lt;- exc2_plot # Creating the pairwise comparison exc2_comparison \u0026lt;- list(c(\"Adelie\", \"Chinstrap\"), c(\"Chinstrap\", \"Gentoo\"), c(\"Adelie\", \"Gentoo\")) exc2_plot \u0026lt;- exc2_plot + stat_compare_means(comparisons = comparison_list) + stat_compare_means(label.y = 25) exc2_plot     ","date":1635897600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635951788,"objectID":"20d4cc04e71352036bd538c40794fe7b","permalink":"https://biodash.github.io/codeclub/s02e10_ggpubr/","publishdate":"2021-11-03T00:00:00Z","relpermalink":"/codeclub/s02e10_ggpubr/","section":"codeclub","summary":"In this session, we will be exploring one of the most powerful ggplot extensions, ggpubr. We will take a look at how to add statistical results to a comparison plot.","tags":null,"title":"Code Club S02E10: An introduction to ggpubr","type":"codeclub"},{"authors":["Mike Sovic"],"categories":null,"content":"\nLearning objectives   Continue to practice creating plots with ggplot Compare the ggplot functions facet_grid() and facet_wrap() Arrange multiple plots of different types on a single figure    1 \u0026ndash; Intro In the previous session we worked with the facet_wrap() function from ggplot, which allowed us to use some variable (column) in the dataset to partition data into multiple panels of a single plot. In this session, we\u0026rsquo;ll see how the facet_wrap() approach compares to a similar function, facet_grid(), and also explore the patchwork package, which offers more control and flexibility in arranging multiple plots in a single figure.\nWe\u0026rsquo;ll continue to use tidyverse functions and data from palmerpenguins, so install those if you need to. If you already have them installed, just load them into your current R session with the library() functions below\u0026hellip;\ninstall.packages(\"tidyverse\") install.packages(\"palmerpenguins\")   library(palmerpenguins) library(tidyverse)   And now let\u0026rsquo;s preview/explore the penguins dataset just to remind ourselves of what\u0026rsquo;s in there\u0026hellip;\nhead(penguins) #\u0026gt; # A tibble: 6 x 8 #\u0026gt; species island bill_length_mm bill_depth_mm flipper_length_… body_mass_g sex  #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; #\u0026gt; 1 Adelie Torge… 39.1 18.7 181 3750 male  #\u0026gt; 2 Adelie Torge… 39.5 17.4 186 3800 fema… #\u0026gt; 3 Adelie Torge… 40.3 18 195 3250 fema… #\u0026gt; 4 Adelie Torge… NA NA NA NA NA  #\u0026gt; 5 Adelie Torge… 36.7 19.3 193 3450 fema… #\u0026gt; 6 Adelie Torge… 39.3 20.6 190 3650 male  #\u0026gt; # … with 1 more variable: year \u0026lt;int\u0026gt; summary(penguins) #\u0026gt; species island bill_length_mm bill_depth_mm  #\u0026gt; Adelie :152 Biscoe :168 Min. :32.10 Min. :13.10  #\u0026gt; Chinstrap: 68 Dream :124 1st Qu.:39.23 1st Qu.:15.60  #\u0026gt; Gentoo :124 Torgersen: 52 Median :44.45 Median :17.30  #\u0026gt; Mean :43.92 Mean :17.15  #\u0026gt; 3rd Qu.:48.50 3rd Qu.:18.70  #\u0026gt; Max. :59.60 Max. :21.50  #\u0026gt; NA's :2 NA's :2  #\u0026gt; flipper_length_mm body_mass_g sex year  #\u0026gt; Min. :172.0 Min. :2700 female:165 Min. :2007  #\u0026gt; 1st Qu.:190.0 1st Qu.:3550 male :168 1st Qu.:2007  #\u0026gt; Median :197.0 Median :4050 NA's : 11 Median :2008  #\u0026gt; Mean :200.9 Mean :4202 Mean :2008  #\u0026gt; 3rd Qu.:213.0 3rd Qu.:4750 3rd Qu.:2009  #\u0026gt; Max. :231.0 Max. :6300 Max. :2009  #\u0026gt; NA's :2 NA's :2   2 \u0026ndash; Review Of facet_wrap() Last week we started with a plot Michael Broe had previously constructed\u0026hellip;\npenguins %\u0026gt;% drop_na() %\u0026gt;% ggplot(aes(x = bill_length_mm, y = bill_depth_mm, color = species)) + geom_point() + geom_smooth(method = \"lm\") #\u0026gt; `geom_smooth()` using formula 'y ~ x'   We then used facet_wrap() to present the data for the three species in separate panels, in place of using color\u0026hellip;\npenguins %\u0026gt;% drop_na() %\u0026gt;% ggplot(aes(x = bill_length_mm, y = bill_depth_mm)) + geom_point() + geom_smooth(method = \"lm\") + facet_wrap(vars(species)) #\u0026gt; `geom_smooth()` using formula 'y ~ x'   Then as part of the breakout rooms, we tried faceting on more than one variable - we subsetted the dataset for only Adelie penguins, then plotted the relationship between bill length and bill depth faceted across both island and sex\u0026hellip;\npenguins %\u0026gt;% drop_na() %\u0026gt;% filter(species == \"Adelie\") %\u0026gt;% ggplot(aes(x = bill_length_mm, y = bill_depth_mm)) + geom_point() + geom_smooth(method = \"lm\") + facet_wrap(vars(island, sex)) #\u0026gt; `geom_smooth()` using formula 'y ~ x'   3 \u0026ndash; facet_grid() While you can use facet_wrap() as above, facet_grid() is often a better option when faceting on two variables. Here\u0026rsquo;s what the example above looks like with facet_grid()\u0026hellip;\npenguins %\u0026gt;% drop_na() %\u0026gt;% filter(species == \"Adelie\") %\u0026gt;% ggplot(aes(x = bill_length_mm, y = bill_depth_mm)) + geom_point() + geom_smooth(method = \"lm\") + facet_grid(rows = vars(sex), cols = vars(island)) #\u0026gt; `geom_smooth()` using formula 'y ~ x'   Notice that with facet_grid() we specify which variable defines the rows and which variable defines the columns.\n Breakout Rooms I: Facet Grids Exercise 1  Try analyzing the relationship between Adelie penguin bill length and bill depth separately for each combination of year and sex. Make the columns represent male/female, and the rows represent the different years (in this case, 2007-2009).\n  Hint (click here)  \nUse filter() to select out Adelie penguins, then create a scatter plot similar to the one in the facet_grid() example. Assign the rows as year and the columns as sex.    Solution (click here)  penguins %\u0026gt;% drop_na() %\u0026gt;% filter(species == \"Adelie\") %\u0026gt;% ggplot(aes(x = bill_length_mm, y = bill_depth_mm)) + geom_point() + geom_smooth(method = \"lm\") + facet_grid(rows = vars(year), cols = vars(sex)) #\u0026gt; `geom_smooth()` using formula 'y ~ x'      Exercise 2  Now let\u0026rsquo;s modify the plot you just created a bit. Add the title \u0026ldquo;Bill Dimensions Of Adelie Penguins\u0026rdquo;, and move the year labels from the right side to the left side of the plot.\n  Hint (click here)  \nCheck out the switch option in the facet_grid() documentation for moving the year labels. For the title, consider labs() or ggtitle().    Solution (click here)  penguins %\u0026gt;% drop_na() %\u0026gt;% filter(species == \"Adelie\") %\u0026gt;% ggplot(aes(x = bill_length_mm, y = bill_depth_mm)) + geom_point() + geom_smooth(method = \"lm\") + facet_grid(rows = vars(year), cols = vars(sex), switch = \"y\") + ggtitle(\"Bill Dimensions Of Adelie Penguins\") #\u0026gt; `geom_smooth()` using formula 'y ~ x'       4 \u0026ndash; Multi-Panel Plots: Patchwork Faceting with facet_wrap() or facet_grid() works when you want to partition the plots based on one or more variables in the dataset. But if you want to arrange multiple plots into one figure, possibly even different types of plots, one good option is the patchwork package. Let\u0026rsquo;s install and load it\u0026hellip;\ninstall.packages(\"patchwork\")   library(patchwork)   With patchwork, you create and save each plot as a separate object. Then, once you\u0026rsquo;ve made the plots, you just tell patchwork how to arrange them. The syntax to define the layout is based on common mathematical operators.\nSome examples:\n plot1 + plot2 puts two plots side-by-side plot1 / plot2 stacks two plots vertically plot1 / (plot2 + plot3) gives plot1 on a top row, and plots 2 and 3 on a bottom row  In the examples above, plot1, plot2, and plot3 represent plots that have been saved as objects with those names.\nBelow is an example from palmerpenguins. First we create the plots, saving each as a new object\u0026hellip;\navg_island_lgth \u0026lt;- penguins %\u0026gt;% drop_na() %\u0026gt;% group_by(island) %\u0026gt;% summarize(\"mean_bill_length\" = mean(bill_length_mm)) %\u0026gt;% ggplot(aes(x = island, y = mean_bill_length)) + geom_col() + ggtitle(\"Average Penguin Bill Length\") mass_by_sex \u0026lt;- penguins %\u0026gt;% drop_na() %\u0026gt;% ggplot(aes(x = sex, y = body_mass_g)) + geom_boxplot() + ggtitle(\"Effect of Sex on Penguin Size\") lgth_by_depth \u0026lt;- penguins %\u0026gt;% drop_na() %\u0026gt;% ggplot(aes(x = bill_length_mm, y = bill_depth_mm)) + geom_point() + geom_smooth(method = \"lm\") + facet_wrap(\"species\") + ggtitle(\"Relationship Between Bill Length and Bill Depth\")   Then we simply use the patchwork syntax to define how these 3 plots will be arranged. In this case, the first (faceted) plot on top, with the other two side-by-side below it\u0026hellip;\nlgth_by_depth / (avg_island_lgth + mass_by_sex) #\u0026gt; `geom_smooth()` using formula 'y ~ x'    Breakout Rooms II: Combining Plots  Use the palmerpenguin data to try to create the plot below\u0026hellip;\n   Hint 1 (Boxplot) (click here)  \nFor the boxplot, use geom_boxplot().    Hint 2 (Boxplot) (click here)  \nNotice that R initially interprets the year variable as a continuous variable, but boxplots need a discrete x axis. Convert that variable to character or factor. You can use mutate along with as.character or as.factor.    Hint 3 (Plot Formatting) (click here)  \nFor the formatting, try theme_classic()    Hint 4 (Labels 1) (click here)  \nThe title and axis labels can be specified with labs(), among other options.    Hint 5 (Labels 2) (click here)  \nTo get the \u0026lsquo;A\u0026rsquo; and \u0026lsquo;B\u0026rsquo; plot annotations, check out the help page for the plot_annotation() function within patchwork.    Solution (click here)  bill_flipper \u0026lt;- penguins %\u0026gt;% drop_na() %\u0026gt;% ggplot(aes(x = bill_length_mm, y = flipper_length_mm)) + geom_point() + facet_wrap(\"species\") + geom_smooth(method = \"lm\") + theme_classic() + labs(title = \"Relationship Between Bill Length and Flipper Length\", x = \"Bill Length (mm)\", y = \"Flipper Length (mm)\") mass_yr \u0026lt;- penguins %\u0026gt;% drop_na() %\u0026gt;% mutate(\"year\" = as.character(year)) %\u0026gt;% ggplot(aes(x = year, y = body_mass_g)) + geom_boxplot() + theme_classic() + labs(title = \"Penguin Size Over Time\", x = \"Body Mass (g)\", y = \"Year\") bill_flipper / mass_yr + plot_annotation(tag_levels = 'A')       ","date":1635206400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635274684,"objectID":"b5bc6d9b231f1f1d1f663e1a2237ebdc","permalink":"https://biodash.github.io/codeclub/s02e09_multiple_plots_part2/","publishdate":"2021-10-26T00:00:00Z","relpermalink":"/codeclub/s02e09_multiple_plots_part2/","section":"codeclub","summary":"In a continuation from the previous session, we'll look at additional approaches for combining plots with a focus on `facet_grid()` and the *patchwork* package.","tags":null,"title":"Code Club S02E09: Combining Plots - Part 2","type":"codeclub"},{"authors":["Mike Sovic"],"categories":null,"content":"\nLearning objectives   Continue to practice creating plots with ggplot Use faceting to divide a plot into multiple panels according to some variable. Arrange multiple plots of different types on a single figure.    1 \u0026ndash; Intro We\u0026rsquo;ll continue with our theme on plotting by exploring some options for arranging multiple plots on a single figure. A couple scenarios where you might want to do this\u0026hellip;\n1.) You create a plot that needs to be subdivided according to some variable, possibly because accounting for that variable is important for the interpretation, or maybe there\u0026rsquo;s just too much on one plot and it helps to split the data up according to some factor.\n2.) You have a series of different plots that all address some related question, maybe each in a slightly different way, and you want to present them all in one figure.\nWe\u0026rsquo;ll take a couple approaches during this and next week\u0026rsquo;s sessions to deal with these two scenarios. Today we\u0026rsquo;ll look at some ggplot functions like facet_wrap() and facet_grid() that allow us to easily deal with scenario 1. Then in the next session we\u0026rsquo;ll try a separate package, patchwork, that offers one good option for scenario 2.\nLike in previous sessions, we\u0026rsquo;ll use some packages from the tidyverse and also the palmerpenguins dataset. If you haven\u0026rsquo;t installed either of those yet, you can do so with the following commands. If you installed them previously, you can just run the latter of the commands (library()) to load them for the current session.\ninstall.packages(\"tidyverse\") install.packages(\"palmerpenguins\")   library(palmerpenguins) library(tidyverse)   And now let\u0026rsquo;s preview/explore the penguins dataset just to remind ourselves of what\u0026rsquo;s in there\u0026hellip;\nhead(penguins) #\u0026gt; # A tibble: 6 x 8 #\u0026gt; species island bill_length_mm bill_depth_mm flipper_length_… body_mass_g sex  #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; #\u0026gt; 1 Adelie Torge… 39.1 18.7 181 3750 male  #\u0026gt; 2 Adelie Torge… 39.5 17.4 186 3800 fema… #\u0026gt; 3 Adelie Torge… 40.3 18 195 3250 fema… #\u0026gt; 4 Adelie Torge… NA NA NA NA NA  #\u0026gt; 5 Adelie Torge… 36.7 19.3 193 3450 fema… #\u0026gt; 6 Adelie Torge… 39.3 20.6 190 3650 male  #\u0026gt; # … with 1 more variable: year \u0026lt;int\u0026gt; summary(penguins) #\u0026gt; species island bill_length_mm bill_depth_mm  #\u0026gt; Adelie :152 Biscoe :168 Min. :32.10 Min. :13.10  #\u0026gt; Chinstrap: 68 Dream :124 1st Qu.:39.23 1st Qu.:15.60  #\u0026gt; Gentoo :124 Torgersen: 52 Median :44.45 Median :17.30  #\u0026gt; Mean :43.92 Mean :17.15  #\u0026gt; 3rd Qu.:48.50 3rd Qu.:18.70  #\u0026gt; Max. :59.60 Max. :21.50  #\u0026gt; NA's :2 NA's :2  #\u0026gt; flipper_length_mm body_mass_g sex year  #\u0026gt; Min. :172.0 Min. :2700 female:165 Min. :2007  #\u0026gt; 1st Qu.:190.0 1st Qu.:3550 male :168 1st Qu.:2007  #\u0026gt; Median :197.0 Median :4050 NA's : 11 Median :2008  #\u0026gt; Mean :200.9 Mean :4202 Mean :2008  #\u0026gt; 3rd Qu.:213.0 3rd Qu.:4750 3rd Qu.:2009  #\u0026gt; Max. :231.0 Max. :6300 Max. :2009  #\u0026gt; NA's :2 NA's :2   2 \u0026ndash; Faceting Let\u0026rsquo;s start by revisiting some plots Michael Broe created in his intro to ggplot a couple sessions ago. He was using the plots to investigate whether a relationship exists between the variables bill length and bill depth in these penguins. A scatterplot with a line of best fit from ggplot\u0026hellip;\npenguins %\u0026gt;% ggplot(aes(x = bill_length_mm, y = bill_depth_mm)) + geom_point() + geom_smooth(method = \"lm\") #\u0026gt; `geom_smooth()` using formula 'y ~ x' #\u0026gt; Warning: Removed 2 rows containing non-finite values (stat_smooth). #\u0026gt; Warning: Removed 2 rows containing missing values (geom_point).   As Michael pointed out previously, mapping an additional aesthetic (color) to the variable species helps us see a relationship a little more clearly\u0026hellip;\npenguins %\u0026gt;% ggplot(aes(x = bill_length_mm, y = bill_depth_mm, color = species)) + geom_point() + geom_smooth(method = \"lm\") #\u0026gt; `geom_smooth()` using formula 'y ~ x' #\u0026gt; Warning: Removed 2 rows containing non-finite values (stat_smooth). #\u0026gt; Warning: Removed 2 rows containing missing values (geom_point).   The color aesthetic partitions the data according to some variable (in this case, species), and here helps add important information to the visualization. An alternative might be to plot the data in separate panels, with each corresponding to a different species. We can do that with either of two functions from ggplot, facet_wrap() or facet_grid(). Let\u0026rsquo;s start with facet_wrap(). This is added as an additional layer to the plot, and indicates one or more variables that will be used to split the data into separate panels. I\u0026rsquo;ll facet here by species.\npenguins %\u0026gt;% ggplot(aes(x = bill_length_mm, y = bill_depth_mm)) + geom_point() + geom_smooth(method = \"lm\") + facet_wrap(\"species\") #\u0026gt; `geom_smooth()` using formula 'y ~ x' #\u0026gt; Warning: Removed 2 rows containing non-finite values (stat_smooth). #\u0026gt; Warning: Removed 2 rows containing missing values (geom_point).   The effect here is similar to what we did with adding a color aesthetic to the species variable earlier - it allows us to evaluate the relationship between bill length and bill depth for each species separately.\n Breakout Rooms: Faceting Exercise 1: Analyze Adelie Penguins By Island  Try analyzing the relationship between bill length and bill depth for just the Adelie penguins (the only species with observations from each of the three islands). For this species, try faceting by island. Does the relationship seem to be consistent across all islands?\n  Hint (click here)  \nUse filter() to select out Adelie penguins, then create a plot similar to the one in the example, but facet on island instead of species    Solution (click here)  penguins %\u0026gt;% filter(species == \"Adelie\") %\u0026gt;% ggplot(aes(x = bill_length_mm, y = bill_depth_mm)) + geom_point() + geom_smooth(method = \"lm\") + facet_wrap(\"island\") #\u0026gt; `geom_smooth()` using formula 'y ~ x' #\u0026gt; Warning: Removed 1 rows containing non-finite values (stat_smooth). #\u0026gt; Warning: Removed 1 rows containing missing values (geom_point).      Exercise 2a: Multiple Facets  Now building on the plot you just created for Adelie Penguins, what if you wanted to facet on not just island, but a combination of island and sex? Give it a try.\n  Hint (click here)  \nfacet_wrap() accepts a character vector of column names. Use the c() function to provide a vector with two column names.    Solution (click here)  penguins %\u0026gt;% filter(species == \"Adelie\") %\u0026gt;% ggplot(aes(x = bill_length_mm, y = bill_depth_mm)) + geom_point() + geom_smooth(method = \"lm\") + facet_wrap(c(\"island\", \"sex\")) #\u0026gt; `geom_smooth()` using formula 'y ~ x' #\u0026gt; Warning: Removed 1 rows containing non-finite values (stat_smooth). #\u0026gt; Warning: Removed 1 rows containing missing values (geom_point).      Exercise 2b: Multiple Facets  There are some facets coming through in that last plot that are based on NA\u0026rsquo;s. Try getting rid of all observations that include missing data before creating the plot.\n  Hint (click here)  \nUse the drop_na() function to remove observations with NA before calling ggplot.    Solution (click here)  penguins %\u0026gt;% drop_na() %\u0026gt;% filter(species == \"Adelie\") %\u0026gt;% ggplot(aes(x = bill_length_mm, y = bill_depth_mm)) + geom_point() + geom_smooth(method = \"lm\") + facet_wrap(c(\"island\", \"sex\")) #\u0026gt; `geom_smooth()` using formula 'y ~ x'      Exercise 3: Axis Scales  Now let\u0026rsquo;s go back to the full dataset where we faceted by species. The code we used (with the drop_na function added), along with its associated plot, are below\u0026hellip;\npenguins %\u0026gt;% drop_na() %\u0026gt;% ggplot(aes(x = bill_length_mm, y = bill_depth_mm)) + geom_point() + geom_smooth(method = \"lm\") + facet_wrap(\"species\") #\u0026gt; `geom_smooth()` using formula 'y ~ x'   Use the help page for facet_wrap to look in to the scales option. Try changing the value of this option to see what effect it has on the plot.\n  Hint 1 (click here)  \nUse ?facet_wrap to get the help page for the function, and find information about the scales option.    Hint 2 (click here)  \nWithin the facet_wrap() function, set scales = \u0026ldquo;free_y\u0026rdquo;.    Solution (click here)  penguins %\u0026gt;% ggplot(aes(x = bill_length_mm, y = bill_depth_mm)) + geom_point() + geom_smooth(method = \"lm\") + facet_wrap(\"species\", scales = \"free\") #\u0026gt; `geom_smooth()` using formula 'y ~ x' #\u0026gt; Warning: Removed 2 rows containing non-finite values (stat_smooth). #\u0026gt; Warning: Removed 2 rows containing missing values (geom_point).       In next week\u0026rsquo;s session, we\u0026rsquo;ll use facet_grid(), which has some similarities to facet_wrap(), and then check out the patchwork package, which gives you more control over how multiple plots are combined in a single figure.\n","date":1634601600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635273735,"objectID":"803100179d0db591e042f5c7dc41eb7e","permalink":"https://biodash.github.io/codeclub/s02e08_multiple_plots/","publishdate":"2021-10-19T00:00:00Z","relpermalink":"/codeclub/s02e08_multiple_plots/","section":"codeclub","summary":"Now that we've gotten a feel for creating plots, we'll look at how they can be arranged to include multiple plots in a single figure.","tags":null,"title":"Code Club S02E08: Combining Plots","type":"codeclub"},{"authors":["Jelmer Poelstra"],"categories":null,"content":"\n New to Code Club?   First, check out the Code Club Computer Setup instructions, which also has pointers for if you\u0026rsquo;re new to R or RStudio.\n  Please open RStudio before Code Club starts to test things out \u0026ndash; and in case you run into issues, please join the Zoom call early and we\u0026rsquo;ll troubleshoot.\n  New to ggplot2?   Check out the last Code Club Session, which was the first of this two-part introduction to ggplot2.\n  You may find this ggplot2 cheat sheet useful!\n   Introduction Session goals   Get more familiar with building and layering plots using geoms, using a new geom, geom_boxplot(), as our starting point.\n  Learn to format plots using theme() and labs().\n  Getting set up We will continue to work with the data contained in the Palmer Penguins package. You only have to install it if you didn\u0026rsquo;t do so in a previous session:\ninstall.packages(\"palmerpenguins\")  If the package has been installed, you do need to always load it with the library() function \u0026ndash; and we\u0026rsquo;ll also load the tidyverse, which includes the ggplot2 package.\nlibrary(palmerpenguins) library(tidyverse)  Penguin bill length We are going to mostly be plotting bill_length_mm, which is the \u0026ldquo;horizontal\u0026rdquo; length of the bill: see the image below.\n  Artwork by Allison Horst    1 - A geom for boxplots Geom recap ggplot2\u0026rsquo;s \u0026ldquo;geoms\u0026rdquo; are basically plot types of which there are quite a few available (see the cheatsheet). Last week, we saw two geoms: geom_point() to plot individual data points, and geom_smooth() to fit a line to data points.\nWhile doing so, we also saw two other properties of ggplot2 and its geoms:\n  Geoms can be layered on top of each other.\n  Geoms can simply plot the data \u0026ldquo;as is\u0026rdquo; (geom_point()) or can perform computations under the hood, and show the results of those computations (geom_smooth()).\n  Let\u0026rsquo;s use a new geom to get a little more fluent with ggplot2 basics.\nBoxplots A boxplot is a very useful type of plot that shows you the median as well as the variation of a distribution. ggplot2 has the geom geom_boxplot() to create boxplots \u0026ndash; another example of a geom that does calculations for us prior to plotting.\nLet\u0026rsquo;s make a boxplot that shows the distribution of penguin bill length (column bill_length_mm in our penguins dataframe) along the y-axis \u0026ndash; recall that we use aes() to refer to a column in the data frame from which the data should be taken:\nggplot(data = penguins) + geom_boxplot(mapping = aes(y = bill_length_mm)) #\u0026gt; Warning: Removed 2 rows containing non-finite values (stat_boxplot).     Why do we get the warning shown above? (click here)  We got the following warning:\n #\u0026gt; Warning: Removed 2 rows containing non-finite values (stat_boxplot).\n We get this warning because 2 rows contain NAs for the variable we are plotting, bill_length_mm.\nWe could take a look at those rows as follows:\npenguins %\u0026gt;% filter(is.na(bill_length_mm)) #\u0026gt; # A tibble: 2 × 8 #\u0026gt; species island bill_length_mm bill_depth_mm flipper_length_… body_mass_g sex  #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; #\u0026gt; 1 Adelie Torge… NA NA NA NA NA  #\u0026gt; 2 Gentoo Biscoe NA NA NA NA NA  #\u0026gt; # … with 1 more variable: year \u0026lt;int\u0026gt;  And we could remove those rows as follows, saving the results in a new dataframe:\n## By negating the `is.na` condition: penguins_noNA \u0026lt;- penguins %\u0026gt;% filter(!is.na(bill_length_mm)) ## Or using the specialized `drop_na` function: penguins_noNA \u0026lt;- penguins %\u0026gt;% drop_na(bill_length_mm)     The nitty-gritty of boxplots: what is shown exactly? (click here)   Lower whisker = smallest observation greater than or equal to lower hinge - 1.5 * IQR Lower hinge/bottom line of box part of boxplot = 25% quantile Middle line = median = 50% quantile Upper hinge/top line of box part of boxplot = 75% quantile Upper whisker = largest observation less than or equal to upper hinge + 1.5 * IQR   That worked, but the plot shows the distribution of bill lengths across all 3 species together, which is not that informative. To separate species along the x-axis, we can map the species column to x:\nggplot(data = penguins) + geom_boxplot(mapping = aes(y = bill_length_mm, x = species)) #\u0026gt; Warning: Removed 2 rows containing non-finite values (stat_boxplot).   Great! We can see, at a glance, that Adelie Penguins tend to have considerably shorter bills than the other two species. Chinstrap\u0026rsquo;s bills are just a bit longer than those of Gentoos, but the longest-billed bird is a Gentoo.\n 2 - Adding a plot layer To get an even better sense of the distribution of bill lengths, and also of our sample sizes, we may want to add the raw data points to our boxplot using geom_point().\n(You may have noticed that in the previous plot, a Gentoo Penguin outlier was shown as a point. To prevent plotting that point twice, we will add outlier.shape = NA to the boxplot call, a somewhat roundabout way of saying that we don\u0026rsquo;t want to plot outliers.)\nggplot(data = penguins) + geom_boxplot(mapping = aes(y = bill_length_mm, x = species), outlier.shape = NA) + geom_point() #\u0026gt; Warning: Removed 2 rows containing non-finite values (stat_boxplot). #\u0026gt; Error: geom_point requires the following missing aesthetics: x and y  Why did this not work?\nWe had previously species the aesthetics mapping inside the geom_boxplot() call \u0026ndash; that is, we set it for that geom only (\u0026ldquo;local aesthetics\u0026rdquo;) and not for the entire plot (\u0026ldquo;global aesthetics\u0026rdquo;). To add a geom_point() layer with the same aesthetics, we can do one of two things:\n Set the aesthetic mapping globally, i.e. inside the ggplot() call, or Set a local aesthetic mapping also inside geom_point().  Let\u0026rsquo;s do the former, so we are not repeating ourselves:\nggplot(data = penguins, mapping = aes(y = bill_length_mm, x = species)) + geom_boxplot(outlier.shape = NA) + geom_point() #\u0026gt; Warning: Removed 2 rows containing non-finite values (stat_boxplot). #\u0026gt; Warning: Removed 2 rows containing missing values (geom_point).   This doesn\u0026rsquo;t look too good because many of the points are plotted on top of each other. We can use a few arguments to geom_point() to make some changes:\n  Add position = \u0026quot;jitter\u0026quot; to the geom_point() call to introduce a small amount of randomness to our points to make us able to see them better.\n  Add size = 1 to make the point size a little smaller (1.5 is the default).\n  ggplot(data = penguins, mapping = aes(y = bill_length_mm, x = species)) + geom_boxplot(outlier.shape = NA) + geom_point(position = \"jitter\", size = 1) #\u0026gt; Warning: Removed 2 rows containing non-finite values (stat_boxplot). #\u0026gt; Warning: Removed 2 rows containing missing values (geom_point).   Note that position = \u0026quot;jitter\u0026quot; and size = 1 are not specified as mappings (i.e., not inside mapping = aes()): here, we are not mapping data to the plot, but are just changing some \u0026ldquo;settings\u0026rdquo;.\n Because jittering is so common, there is also a specialized jittering geom available: geom_jitter() is shorthand for geom_point(position = \u0026quot;jitter\u0026quot;).\nSo, we could have also used the following code to create the same plot:\nggplot(data = penguins, mapping = aes(y = bill_length_mm, x = species)) + geom_boxplot(outlier.shape = NA) + geom_jitter(size = 1)     Breakout Rooms I  Exercise 1   Run the code below and figure out what the problem is.\n(And why do you think ggplot2 creates a legend with the item \u0026ldquo;blue\u0026rdquo;, instead of throwing an error?)\n  Modify the code to get the originally intended effect: blue points.\n  ggplot(data = penguins, aes(y = bill_length_mm, x = species)) + geom_boxplot(outlier.shape = NA) + geom_point(mapping = aes(color = \"blue\"), position = \"jitter\")    Hints (click here)   Here is the botched plot:  ggplot(data = penguins, aes(y = bill_length_mm, x = species)) + geom_boxplot(outlier.shape = NA) + geom_point(mapping = aes(color = \"blue\"), position = \"jitter\") #\u0026gt; Warning: Removed 2 rows containing non-finite values (stat_boxplot). #\u0026gt; Warning: Removed 2 rows containing missing values (geom_point).    Should color = \u0026quot;blue\u0026quot; be a mapping, that is, should it be part of the mapping = aes() argument?     Solution (click here)    The problem with the original code is that color = \u0026quot;blue\u0026quot; should not be a mapping.\n  Why ggplot2 does not throw an error: the mapping argument is used to map data to an aesthetic like point color. Normally, that data is a column in the dataframe, but because the code quotes \u0026ldquo;blue\u0026rdquo; (color = \u0026quot;blue\u0026quot; instead of color = blue), ggplot2 does not assume it is a column and instead creates a variable on the fly that just contains the value \u0026ldquo;blue\u0026rdquo;.\n  The correct code to color points blue:\n  ggplot(data = penguins, mapping = aes(y = bill_length_mm, x = species)) + geom_boxplot(outlier.shape = NA) + geom_point(color = \"blue\", position = \"jitter\") #\u0026gt; Warning: Removed 2 rows containing non-finite values (stat_boxplot). #\u0026gt; Warning: Removed 2 rows containing missing values (geom_point).      Exercise 2 Violin plots are somewhat similar to boxplots, but show a density distribution. Using Google, find out which ggplot2 geom creates a violin plot, and then make one plotting bill length by species like we have done for boxplots.\n  Hints (click here)    geom_violin() is the geom that creates violin plots.\n  Other than the geom function, you can leave the code the same as in the previous examples.\n     Solution (click here)  ggplot(data = penguins, mapping = aes(y = bill_length_mm, x = species)) + geom_violin() #\u0026gt; Warning: Removed 2 rows containing non-finite values (stat_ydensity).       3 - Intro to formatting plots So far, we have mostly been concerned with what we are plotting, and haven\u0026rsquo;t paid much attention to how our plot looks. But I, for one, dislike that gray background to the plot, and perhaps the axis labels are a little small?\nggplot2 offers many options to modify the look of our plot. There are so many that it isn\u0026rsquo;t really possible to remember even the majority of them. Therefore, even for daily users of ggplot2, creating a publication-ready figure will usually involve some Googling or checking the ggplot2 documentation.\nLet\u0026rsquo;s have a look at some of the most commonly used options to change the look of ggplot2 plots.\nA starting plot We\u0026rsquo;ll start with the following plot, similar to one we have created before:\nggplot(data = penguins, mapping = aes(x = species, y = bill_length_mm)) + geom_boxplot(outlier.shape = NA) + geom_point(position = \"jitter\", size = 1, color = \"grey70\") #\u0026gt; Warning: Removed 2 rows containing non-finite values (stat_boxplot). #\u0026gt; Warning: Removed 2 rows containing missing values (geom_point).   (Note the addition of color = \u0026quot;grey70\u0026quot; to make the points less dominant in the plot.\nSee this PDF for an overview of named colors in R.)\n 4 - Formatting with theme() and more The quickest way to modify the overall look and feel of our plot is by using a different \u0026ldquo;complete theme\u0026rdquo;. The default theme is theme_gray(), which comes with that gray background and many other settings that control what the plot looks like.\nThe ggplot2 documentation has a list of complete themes that shows you what they look like.\nLet\u0026rsquo;s switch to a different theme, theme_classic(), for our penguin boxplot:\nggplot(data = penguins, mapping = aes(x = species, y = bill_length_mm)) + geom_boxplot(outlier.shape = NA) + geom_point(position = \"jitter\", size = 1, color = \"grey70\") + theme_classic() #\u0026gt; Warning: Removed 2 rows containing non-finite values (stat_boxplot). #\u0026gt; Warning: Removed 2 rows containing missing values (geom_point).   These complete theme functions (theme_\u0026lt;theme-name\u0026gt;) also take a few arguments \u0026ndash;\nbase_size is very useful if we want to simultaneously change the size of all text labels:\nggplot(data = penguins, mapping = aes(x = species, y = bill_length_mm)) + geom_boxplot(outlier.shape = NA) + geom_point(position = \"jitter\", size = 1, color = \"grey70\") + theme_classic(base_size = 14) #\u0026gt; Warning: Removed 2 rows containing non-finite values (stat_boxplot). #\u0026gt; Warning: Removed 2 rows containing missing values (geom_point).   This retains the relative sizes of different labels. For instance, note that in both plots, the \u0026ldquo;axis titles\u0026rdquo; (species on x, bill_lenth_mm on y) are larger than the \u0026ldquo;axis text\u0026rdquo; (the labels at the tick marks).\nIf we wanted to change individual theme components like those, we would need to use the theme() function (check its documentation page to see the many possible arguments).\nFor example, to make axis titles and axis text/labels the same size:\nggplot(data = penguins, mapping = aes(x = species, y = bill_length_mm)) + geom_boxplot(outlier.shape = NA) + geom_point(position = \"jitter\", size = 1, color = \"grey70\") + theme_classic(base_size = 14) + theme(axis.text = element_text(size = 14), axis.title = element_text(size = 14)) #\u0026gt; Warning: Removed 2 rows containing non-finite values (stat_boxplot). #\u0026gt; Warning: Removed 2 rows containing missing values (geom_point).    5 - Adding labels to our plot Right now, the axis titles are simply the names of the columns that we used in the mapping. The y-axis title in particular (bill_length_mm) could be improved. We might also want to add a title and even a subtitle to our plot.\nWe can do all of this with the labs() function as follows:\nggplot(data = penguins, mapping = aes(x = species, y = bill_length_mm)) + geom_boxplot(outlier.shape = NA) + geom_point(position = \"jitter\", size = 1, color = \"grey70\") + theme_classic(base_size = 14) + labs(title = \"Penguin Bill Length by Species\", subtitle = \"Collected at Palmer Station, Antarctica\", x = \"Penguin species\", # x-axis label y = \"Bill length (mm)\") # y-axis label #\u0026gt; Warning: Removed 2 rows containing non-finite values (stat_boxplot). #\u0026gt; Warning: Removed 2 rows containing missing values (geom_point).    Breakout Rooms II  Exercise 3   Modify the code used to produce the last plot (just above this exercise) to try several of the themes from the list of complete themes.\nDo you have a preference?\n  The list of complete themes also shows that these functions have a few more arguments than the base_size one we explored.\n  Bonus: Change the base_line_size. What does it do?\n  Bonus (may not work out of the box on Windows): Using a different font family can nicely shake things up \u0026ndash; this is the base_family argument. Most standard font family names (e.g. see this list) should work. For instance, you can try Optima, Verdana, Times New Roman, Courier, or cursive.\n    Example solution (click here)  With theme_bw() and:\n base_line_size = 1 (thicker axis lines) the cursive font family using base_family = cursive  ggplot(data = penguins, mapping = aes(x = species, y = bill_length_mm)) + geom_boxplot(outlier.shape = NA) + geom_point(position = \"jitter\", size = 1, color = \"grey70\") + theme_bw(base_size = 14, base_line_size = 1, base_family = \"cursive\") + labs(title = \"Penguin Bill Length by Species and Sex\", subtitle = \"Collected at Palmer Station, Antarctica\", x = \"Penguin Species\", y = \"Bill length (mm)\") #\u0026gt; Warning: Removed 2 rows containing non-finite values (stat_boxplot). #\u0026gt; Warning: Removed 2 rows containing missing values (geom_point).       Exercise 4  Modify your code from Exercise 3 to color the jittered points, but not the boxplots, according to sex.  As we also saw last week, a legend should have automatically appeared when mapping color to a variable. But what if we wanted to move the legend from the right to the top of the plot?\n Scroll through the theme() documentation and try and find the argument that controls the position of the legend. Then, use this argument to move the legend to the top.    Hints (click here)    To color points by sex without modifying the boxplots, add the mapping locally for geom_point() only.\n  To move the legend, use the legend.position argument of theme().\n     Solution (click here)   Color points by sex:  ggplot(data = penguins, mapping = aes(x = species, y = bill_length_mm)) + geom_boxplot(outlier.shape = NA) + geom_point(position = \"jitter\", size = 1, mapping = aes(color = sex)) + theme_classic(base_size = 14) + labs(title = \"Penguin Bill Length by Species and Sex\", subtitle = \"Collected at Palmer Station, Antarctica\", x = \"Penguin Species\", y = \"Bill length (mm)\") #\u0026gt; Warning: Removed 2 rows containing non-finite values (stat_boxplot). #\u0026gt; Warning: Removed 2 rows containing missing values (geom_point).    Move the legend to the top:  ggplot(data = penguins, mapping = aes(x = species, y = bill_length_mm)) + geom_boxplot(outlier.shape = NA) + geom_point(position = \"jitter\", size = 1, mapping = aes(color = sex)) + theme_classic(base_size = 14) + theme(legend.position = \"top\") + labs(title = \"Penguin Bill Length by Species and Sex\", subtitle = \"Collected at Palmer Station, Antarctica\", x = \"Penguin Species\", y = \"Bill length (mm)\") #\u0026gt; Warning: Removed 2 rows containing non-finite values (stat_boxplot). #\u0026gt; Warning: Removed 2 rows containing missing values (geom_point).       Exercise 5 (bonus)   Try to modify one of the previous plots to get separate boxes for each combination of species and sex, and no jittered points (don\u0026rsquo;t hesitate to look at the hints!).\n  Did you see the NA sex (i.e., missing data)? Recreate the plot without the missing data.\n    Hints (click here)    Map sex to color or fill (and continue to map species to x).\n  Use drop_na(sex) to remove rows with an NA in the sex column. You can save the result in a new dataframe and then plot that dataframe, or you can pipe (%\u0026gt;%) the result straight into the ggplot() function.\n     Solution (click here)  This example maps sex to fill, which will \u0026ldquo;fill\u0026rdquo; the box with colors (as we saw before, mapping to color colors the lines instead):\npenguins %\u0026gt;% drop_na(sex) %\u0026gt;% ggplot(mapping = aes(x = species, y = bill_length_mm, fill = sex)) + # Now mapping sex to \"fill\" geom_boxplot() + # We now do want to see the outliers! theme_classic(base_size = 14) + labs(title = \"Penguin Bill Length by Species and Sex\", subtitle = \"Collected at Palmer Station, Antarctica\", x = \"Penguin Species\", y = \"Bill length (mm)\")       Going further   One basic aspect of ggplot2 that we have not touched upon in this or the previous session are \u0026ldquo;scales\u0026rdquo;. For instance, to change the colors used in mappings (like our boxplot color), we would need to use scales. For an introduction, see the section on scales from the R for Data Science book.\n  We also didn\u0026rsquo;t talk about saving plots, which can be done with the ggsave() function (documentation page). Just be aware that if you don\u0026rsquo;t specify dimensions with the width and height arguments, they will be taken from the current size of the RStudio plotting window, which is not necessarily what you want.\n  ","date":1633392000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633628018,"objectID":"9447a0f876418349500f0f801001e795","permalink":"https://biodash.github.io/codeclub/s02e07_ggplot2_part2/","publishdate":"2021-10-05T00:00:00Z","relpermalink":"/codeclub/s02e07_ggplot2_part2/","section":"codeclub","summary":"During this second part of an introduction to ggplot2, we will get a better understanding of using geoms, with boxplots as an example, and will also learn about formatting and saving plots.","tags":null,"title":"S02E07: Intro to ggplot2 (part 2)","type":"codeclub"},{"authors":["Michael Broe"],"categories":null,"content":"\n New To Code Club?   First, check out the Code Club Computer Setup instructions, which also has some pointers that might be helpful if you\u0026rsquo;re new to R or RStudio.\n  Please open RStudio before Code Club to test things out \u0026ndash; if you run into issues, join the Zoom call early and we\u0026rsquo;ll troubleshoot.\n   Session Goals  Learn the philosophy of coding a graphic. Learn the basic template of a ggplot graphic, so you can reuse it for multiple chart types. Learn how you can quickly add visual information to a graphic using aesthetics and layers.   Intro: The ggplot philosophy We have seen that in R, instead of manually manipulating data frames with a mouse as you do when editing Excel spreadsheets, we code the operations we want to perform using dplyr verbs like select(), filter(), arrange(), and so on.\nIn a similar way when performing visualization, instead of clicking on a chart type in Excel and selecting options, we code the chart in R.\nAnd just as dplyr gives us efficient ways to manipulate data frames, ggplot2 (which is also part of the tidyverse) gives us efficient ways to manipulate charts/plots/graphics (we use these terms interchangeably).\nThe gg in ggplot2 stands for grammar of graphics, a systematic approach for designing statistical plots developed by Leland Wilkinson. The idea behind this is to think about \u0026lsquo;pulling apart\u0026rsquo; various plots into their component pieces, then provide code that could put those pieces together again in different ways to make new plots.\nThis notion of pulling apart a graphic leads to the idea of layers. You can build up a plot of any complexity by overlaying different views of the same data.\nThere\u0026rsquo;s a learning curve here for sure, but there are many advantages.\n First, every graphic shares a common template. Once you understand this structure you can \u0026ldquo;say\u0026rdquo; a lot of different things.\n(And I mean a lot. The ggplot cheat sheet lists over 40 plot-types, but because this is a language in its own right, users can create their own extensions that you can also utilize, adding over 80 more.) Second, because the graphic is now a piece of code, it is reusable. Once you have a little library of graphs you\u0026rsquo;ve made, you can tweak them in various ways to make more. You can also share your code with others, so they can apply the same approach to their data. Third, the way we put layers together is identical to the way we use pipes. You can read %\u0026gt;% as and then: \u0026ldquo;select() and then filter() and then mutate()\u0026rdquo;. In graphics, the \u0026ldquo;and then\u0026rdquo; symbol is +: \u0026ldquo;show this layer, and then add this layer, and then add this layer\u0026rdquo;. In this way you can gradually build up a graphic of increasing complexity; or on the other hand take someone else\u0026rsquo;s code and simplify it by deleting layers.   A First Example So how does this work in practice? We\u0026rsquo;ll work through visualizing aspects of the palmerpenguins package which was introduced in S02E04 Intro to the Tidyverse.\nIn addition, ggplot2 is part of the tidyverse package so we need to load that as well:\n# this assumes you've already installed tidyverse and palmerpenguins with install.packages() library(tidyverse) library(palmerpenguins)  And recall that the penguins dataset is included in that package:\nglimpse(penguins) #\u0026gt; Rows: 344 #\u0026gt; Columns: 8 #\u0026gt; $ species \u0026lt;fct\u0026gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel… #\u0026gt; $ island \u0026lt;fct\u0026gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse… #\u0026gt; $ bill_length_mm \u0026lt;dbl\u0026gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, … #\u0026gt; $ bill_depth_mm \u0026lt;dbl\u0026gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, … #\u0026gt; $ flipper_length_mm \u0026lt;int\u0026gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186… #\u0026gt; $ body_mass_g \u0026lt;int\u0026gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, … #\u0026gt; $ sex \u0026lt;fct\u0026gt; male, female, female, NA, female, male, female, male… #\u0026gt; $ year \u0026lt;int\u0026gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…  What is the correlation between bill-length and bill-width in these species?\nAre longer bills also deeper? We can explore this with a scatterplot.\nBut first let\u0026rsquo;s look at the ggplot() template.\n Note: the package is ggplot2, the command is just ggplot().\n ggplot(data = \u0026lt;DATA\u0026gt;) + \u0026lt;GEOM_FUNCTION\u0026gt;(mapping = aes(\u0026lt;MAPPINGS\u0026gt;))  These are the obligatory parts of any plot: data, geoms, and mappings. The first argument to ggplot() is the data frame:\nggplot(data = penguins)   This is not very interesting! but it\u0026rsquo;s notable that it is something. ggplot() has created a base coordinate system (a base layer) that we can add visual layers to. It\u0026rsquo;s completely uninfomative because we haven\u0026rsquo;t said how we want the data (or even which data) to be mapped to graphic elements.\nThe add a layer operator is \u0026ldquo;+\u0026rdquo;, which is the ggplot equivalent of the pipe symbol, and best practice is to place it at the end of the line, just like the pipe. This makes your layers more readable in the code (and can lead to problems if you put it some other places).\nThe next argument specifies the kind plot we want: scatterplot, bar chart, fitted line, boxplot, violin plot, etc, etc. ggplot2 refers to these as geoms: each different plot-type uses a different geometrical object to represent data.\nYou can see an overview of many of these geoms in the cheat sheet.The geom for a scatterplot is geom_point().\nBut we also require a mapping argument, which maps the variables or columns in the dataset we want to focus on to their visual representation in the plot.\nAnd finally we need to specify aesthetics for the geometric objects in the plot, which will control things like shape, color, transparency, etc. Perhaps surprisingly, the x and y coordinates of the points are aesthetics, since these control, not the shape or color, but the relative position of the points in the plot. (After all, flipping the x for y in some plots will flip the plot 90 degrees.)\nSounds like a lot when you spell it all out, but it\u0026rsquo;s actually pretty simple in practice. Here then is our complete first plot:\nggplot(data = penguins) + geom_point(mapping = aes(x = bill_length_mm, y = bill_depth_mm))   Hmmm\u0026hellip; the relationship is not very clear, but I guess if you squint your eyes you can maybe see some local positive correlations? We\u0026rsquo;ll explore this relationship in more detail below using extra aesthetics.\nBut let\u0026rsquo;s get coding!\nExercise 1 Create a scatterplot comparing body mass and flipper length. Does it look like there is any correlation?\n  Hints (click here)  Check the output of the glimpse() function to see what the precise variable names for body mass and flipper length are. It's up to you which you assign to x and y.    Solution (click here)  ggplot(data = penguins) + geom_point(mapping = aes(x = body_mass_g, y = flipper_length_mm))      The power of aesthetics We saw above that the relationship between bill length and bill depth is not particularly clear; but that there might be some local patterns. How can we explore this? Is it possible that these clusters correspond somehow to the three different species: Adelie, Chinstrap, and Gentoo? Part of the problem with our original plot may be that we are lumping all the data from all three species together. We can quickly explore this by adding an extra aesthetic.\nOur current plot uses two numeric variables: bill_length_mm and bill_depth_mm. We can add a third categorical variable, like species, to a two dimensional scatterplot by mapping it to a different visual aesthetic. We\u0026rsquo;ve mapped length and depth to x,y coordinates. Now we\u0026rsquo;ll simultaneously map species to color by expanding our list of aesthetics.\nBut before we do that, let\u0026rsquo;s improve our plotting commands a little.\nFirst, our template includes a general mapping = directive, which in turn includes a bunch of specific mappings from variables to aesthetics. While I think it\u0026rsquo;s quite useful to be explicit that this is a mapping component (which connects columns with graphical objects) it turns out we can drop the mapping = syntax completely (and many people do). The very fact we have specific mappings inside aes() makes the mapping = redundant. (I\u0026rsquo;ve come to think of it as \u0026lsquo;syntactic sugar\u0026rsquo;).\nSecond, we can actually pipe the dataset into the ggplot() command! We then set the mappings, and then choose our geom in a new layer. This is very common. Our new syntax looks like this:\npenguins %\u0026gt;% ggplot(aes(x = bill_length_mm, y = bill_depth_mm)) + geom_point()   Now let\u0026rsquo;s add an extra aesthetic for the species variable. For this example we\u0026rsquo;ll use the color aesthetic:\npenguins %\u0026gt;% ggplot(aes(x = bill_length_mm, y = bill_depth_mm, color = species)) + geom_point()   We get a neat legend on the right hand side for free. It\u0026rsquo;s much clearer now that, within each species, there does seem to be a positive correlation between length and depth. It\u0026rsquo;s just that the absolute values for each species fall in different regions of \u0026ldquo;bill space\u0026rdquo;.\n Notice: We are piping the entire raw dataset into the plot here, while only graphing part of it. The elegance of the pipe syntax is that we can first pass the dataset through a series of dplyr operations - filtering and mutating etc. - and then pipe that modified dataset directly into our plotting commands. We\u0026rsquo;ll see more of that in coming Code Clubs (and Exercise 3)!\n Exercise 2 Go back to your body_mass/flipper data and color by species, also using our new syntax.\nThere are various aesthetics you can use for a categorical variable (see help for geom_point). One is shape. Try this instead of color and see which you prefer.\n  Solutions (click here)  penguins %\u0026gt;% ggplot(aes(x = body_mass_g, y = flipper_length_mm, color = species)) + geom_point()   penguins %\u0026gt;% ggplot(aes(x = body_mass_g, y = flipper_length_mm, shape = species)) + geom_point()      The fitted-line geom: geom_smooth() In the scatterplot for bill length vs. bill depth, there wasn\u0026rsquo;t a very clean overall positive relationship. We can make this even more apparent visually by fitting a line to the data: overlaying another geom in the same plot.\npenguins %\u0026gt;% ggplot(mapping = aes(x = bill_length_mm, y = bill_depth_mm)) + geom_point() + geom_smooth() #\u0026gt; `geom_smooth()` using method = 'loess' and formula 'y ~ x'    Technical note: by default, the geom_smooth() function doesn\u0026rsquo;t use classical linear regression to fit the data. Instead it uses locally estimated scatterplot smoothing: loess. This snakes around and tries to find a more local best fit to the data. You can request different fit algorithms with the method option. See help for geom_smooth for details.\n But again, this is messy since we are trying to analyze all the data at once. What if we set the species aesthetic before we apply any geom, so that the aesthetic is inherited by both geoms?\npenguins %\u0026gt;% ggplot(aes(x = bill_length_mm, y = bill_depth_mm, color = species)) + geom_point() + geom_smooth() #\u0026gt; `geom_smooth()` using method = 'loess' and formula 'y ~ x'   Here we can see there does seem to be a strong positive correlation if we analyze the data species-by-species.\nExercise 3 Revisit your flipper/body_mass data, and fit lines to each species.\nThere is a lot of overlap between two of the species in the scatterplot, which makes it hard to see the overall patterns for those two species. Does the fitted line overlay help make sense of this?\n  Solution (click here)  penguins %\u0026gt;% ggplot(aes(x = body_mass_g, y = flipper_length_mm, color = species)) + geom_point() + geom_smooth() #\u0026gt; `geom_smooth()` using method = 'loess' and formula 'y ~ x'    Perhaps you think this plot is too noisy, with the scatterplot data messing up the basic linear relationships. Modify your plot so it just shows the linear layer without the scatterplot layer.\n  Hints (click here)  Remember you can delete layers just as easily as you can add layers.    Solution (click here)  penguins %\u0026gt;% ggplot(aes(x = body_mass_g, y = flipper_length_mm, color = species)) + geom_smooth() #\u0026gt; `geom_smooth()` using method = 'loess' and formula 'y ~ x'    (Bonus: if you have time\u0026hellip;)\nPipe your dataset into a dplyr filter() command which pulls out just the Gentoo data; then pipe that data into the ggplot() command, to create a scatterplot, together with a smoothed-line overlay, for just that one species.\n  Hints (click here)  Remember two important things about the filter() function: (i) the check for equality is a double equals sign == (ii) the species name is a character string, not a number, so has to be quoted with \"...\".    Solution (click here)  penguins %\u0026gt;% filter(species == \"Gentoo\") %\u0026gt;% ggplot(aes(x = body_mass_g, y = flipper_length_mm)) + geom_point() + geom_smooth() #\u0026gt; `geom_smooth()` using method = 'loess' and formula 'y ~ x'     ","date":1632787200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633013561,"objectID":"11e737a4fdee25a356dbb58f23d00829","permalink":"https://biodash.github.io/codeclub/s02e06_ggplot2/","publishdate":"2021-09-28T00:00:00Z","relpermalink":"/codeclub/s02e06_ggplot2/","section":"codeclub","summary":"In this session of Code Club, we'll look at how to visualize data in R using the tidyverse package **ggplot2**.","tags":null,"title":"Code Club S02E06: Visualizing Data","type":"codeclub"},{"authors":null,"categories":null,"content":" Prep homework Basic computer setup   If you didn\u0026rsquo;t already do this, please follow the Code Club Computer Setup instructions, which also has pointers for if you\u0026rsquo;re new to R or RStudio.\n  If you\u0026rsquo;re able to do so, please open RStudio a bit before Code Club starts \u0026ndash; and in case you run into issues, please join the Zoom call early and we\u0026rsquo;ll troubleshoot.\n   Introduction What will we go over today\n We will continue using the dplyr package, which is part of the tidyverse and was introduced last week. Learn using arrange() - orders the rows of a data frame by the values of selected columns. Learn using mutate() - adds new variables and preserves existing ones.   1 - What is the dplyr package? dplyr is one of the tidyverse packages that are designed for data science. dplyr provides functions for data manipulation.\nFunctions for row-wise operations include:\n filter() - chooses rows based on column values. slice() - chooses rows based on location. arrange() - orders the rows of a data frame by the values of selected columns.  Functions for column-wise operations include:\n select() - changes whether or not a column is included. rename() - changes the name of columns. mutate() - changes the values of columns and creates new columns relocate() - changes the order of the columns.  Functions for groups of rows include:\n summarise() - collapses a group into a single row.  Last week, we got introduced to the tidyverse and covered the %\u0026gt;% pipe, select(), and filter. We saw that packages are basically R add-ons that contain additional functions or datasets we can use. Using the function install.packages(), we can install packages that are available at the Comprehensive R Archive Network, or CRAN.\nFor those who have not installed the tidyverse, let\u0026rsquo;s install it. We only need to do this once, so if you did this last week, you don\u0026rsquo;t need to now.\ninstall.packages(\"tidyverse\")  To use the dplyr package within the tidyverse, we need to call it up using library().\nlibrary(tidyverse) #\u0026gt; ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ── #\u0026gt; ✔ ggplot2 3.3.5 ✔ purrr  0.3.4 #\u0026gt; ✔ tibble  3.1.4 ✔ dplyr  1.0.7 #\u0026gt; ✔ tidyr  1.1.3 ✔ stringr 1.4.0 #\u0026gt; ✔ readr  2.0.1 ✔ forcats 0.5.1 #\u0026gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── #\u0026gt; ✖ dplyr::filter() masks stats::filter() #\u0026gt; ✖ dplyr::lag() masks stats::lag()   2 - Using the arrange() function We will learn how to use the arrange() function from dplyr to sort a data frame in multiple ways. First, we will sort a dataframe by values of a single variable, and then we will learn how to sort a dataframe by more than one variable in the dataframe. By default, dplyr\u0026rsquo;s arrange() sorts in ascending order (lowest values first).\nLet\u0026rsquo;s get set up and grab some data so that we have some material to work with.\nWe will use the same dataset palmerpenguins we used last week. To get this data, we need to install the palmerpenguins package (again, no need to do this if you already did so last week):\ninstall.packages(\"palmerpenguins\")  Then, to use the package, we need to use the function library() to load the package in R:\nlibrary(palmerpenguins)  The dataframe we will use today is called penguins. Let\u0026rsquo;s take a look at the structure of the data:\n# look at the first 10 rows and all columns head(penguins, 10) #\u0026gt; # A tibble: 10 × 8 #\u0026gt; species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 Adelie Torgersen 39.1 18.7 181 3750 #\u0026gt; 2 Adelie Torgersen 39.5 17.4 186 3800 #\u0026gt; 3 Adelie Torgersen 40.3 18 195 3250 #\u0026gt; 4 Adelie Torgersen NA NA NA NA #\u0026gt; 5 Adelie Torgersen 36.7 19.3 193 3450 #\u0026gt; 6 Adelie Torgersen 39.3 20.6 190 3650 #\u0026gt; 7 Adelie Torgersen 38.9 17.8 181 3625 #\u0026gt; 8 Adelie Torgersen 39.2 19.6 195 4675 #\u0026gt; 9 Adelie Torgersen 34.1 18.1 193 3475 #\u0026gt; 10 Adelie Torgersen 42 20.2 190 4250 #\u0026gt; # … with 2 more variables: sex \u0026lt;fct\u0026gt;, year \u0026lt;int\u0026gt;  # check the structure of penguins_data # glimpse() which is a part of dplyr functions  # similarly to str() and can be used interchangeably glimpse(penguins) #\u0026gt; Rows: 344 #\u0026gt; Columns: 8 #\u0026gt; $ species \u0026lt;fct\u0026gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel… #\u0026gt; $ island \u0026lt;fct\u0026gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse… #\u0026gt; $ bill_length_mm \u0026lt;dbl\u0026gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, … #\u0026gt; $ bill_depth_mm \u0026lt;dbl\u0026gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, … #\u0026gt; $ flipper_length_mm \u0026lt;int\u0026gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186… #\u0026gt; $ body_mass_g \u0026lt;int\u0026gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, … #\u0026gt; $ sex \u0026lt;fct\u0026gt; male, female, female, NA, female, male, female, male… #\u0026gt; $ year \u0026lt;int\u0026gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…  Okay, now we have a sense of what the penguins dataset is.\nNow we want to sort the penguins dataframe by body mass to quickly learn about the lightest penguin and its relations to other variables. We will use the pipe operator %\u0026gt;% to feed the data to the arrange() function. We then specify name of the variable that we want to sort the dataframe by.\nIn this example, we are sorting by variable body_mass_g, so we will see the lightest penguins at the top of the dataframe:\npenguins %\u0026gt;% # take the penguins_data arrange(body_mass_g) # sort the dataframe in ascending order based on body mass #\u0026gt; # A tibble: 344 × 8 #\u0026gt; species island bill_length_mm bill_depth_mm flipper_length_… body_mass_g #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 Chinstrap Dream 46.9 16.6 192 2700 #\u0026gt; 2 Adelie Biscoe 36.5 16.6 181 2850 #\u0026gt; 3 Adelie Biscoe 36.4 17.1 184 2850 #\u0026gt; 4 Adelie Biscoe 34.5 18.1 187 2900 #\u0026gt; 5 Adelie Dream 33.1 16.1 178 2900 #\u0026gt; 6 Adelie Torgersen 38.6 17 188 2900 #\u0026gt; 7 Chinstrap Dream 43.2 16.6 187 2900 #\u0026gt; 8 Adelie Biscoe 37.9 18.6 193 2925 #\u0026gt; 9 Adelie Dream 37.5 18.9 179 2975 #\u0026gt; 10 Adelie Dream 37 16.9 185 3000 #\u0026gt; # … with 334 more rows, and 2 more variables: sex \u0026lt;fct\u0026gt;, year \u0026lt;int\u0026gt;  If we wanted to sort descendingly, such that the heaviest penguins are in the first rows, we can add a - in front of the variable:\npenguins %\u0026gt;% # take the penguins_data arrange(-body_mass_g) # sort the dataframe in descending order based on body mass #\u0026gt; # A tibble: 344 × 8 #\u0026gt; species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 Gentoo Biscoe 49.2 15.2 221 6300 #\u0026gt; 2 Gentoo Biscoe 59.6 17 230 6050 #\u0026gt; 3 Gentoo Biscoe 51.1 16.3 220 6000 #\u0026gt; 4 Gentoo Biscoe 48.8 16.2 222 6000 #\u0026gt; 5 Gentoo Biscoe 45.2 16.4 223 5950 #\u0026gt; 6 Gentoo Biscoe 49.8 15.9 229 5950 #\u0026gt; 7 Gentoo Biscoe 48.4 14.6 213 5850 #\u0026gt; 8 Gentoo Biscoe 49.3 15.7 217 5850 #\u0026gt; 9 Gentoo Biscoe 55.1 16 230 5850 #\u0026gt; 10 Gentoo Biscoe 49.5 16.2 229 5800 #\u0026gt; # … with 334 more rows, and 2 more variables: sex \u0026lt;fct\u0026gt;, year \u0026lt;int\u0026gt;  We can also pipe the results into the filter() function covered last week, to select only penguins weighing more than 5000 g:\npenguins_new \u0026lt;- # assign the results to a dataframe `penguins_new` penguins %\u0026gt;% # take the penguins data arrange(bill_length_mm, bill_depth_mm) %\u0026gt;% # sort by bill length followed by bill depth filter(body_mass_g \u0026gt; 5000) # select species greater with mass \u0026gt; 5000 g.  head(penguins_new, 5) # look at the top 5  #\u0026gt; # A tibble: 5 × 8 #\u0026gt; species island bill_length_mm bill_depth_mm flipper_length_… body_mass_g sex  #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; #\u0026gt; 1 Gentoo Biscoe 44.4 17.3 219 5250 male  #\u0026gt; 2 Gentoo Biscoe 44.9 13.3 213 5100 fema… #\u0026gt; 3 Gentoo Biscoe 45 15.4 220 5050 male  #\u0026gt; 4 Gentoo Biscoe 45.1 14.5 207 5050 fema… #\u0026gt; 5 Gentoo Biscoe 45.2 14.8 212 5200 fema… #\u0026gt; # … with 1 more variable: year \u0026lt;int\u0026gt;  Let\u0026rsquo;s check the counts of different species and islands among our new dataset:\npenguins_new %\u0026gt;% count(species) #\u0026gt; # A tibble: 1 × 2 #\u0026gt; species n #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 Gentoo 61  penguins_new %\u0026gt;% count(island) #\u0026gt; # A tibble: 1 × 2 #\u0026gt; island n #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 Biscoe 61  You can see that we have only retained Gentoo Penguins from the island of Biscoe.\n Breakout session 1 - arrange()  Exercise 1 With the penguins dataset, answer the following questions:\n  Create a new dataset called penguins_shortflippers from the penguins dataset with the 20 penguins with the shortest flippers.\n  How many penguins of each species are found in penguins_shortflippers?\n  Which islands do they come from?\n    Hints    To create penguins_shortflippers, first use arrange() to sort by flipper lengths, and pipe the results into the head() function to get the top 20.\n  To get the species and island composition, use the count() function.\n     Solution (click here)   To create a dataframe with the 20 penguins with the shortest flippers:  penguins_shortflippers \u0026lt;- # assign the results penguins %\u0026gt;% # take penguins_data arrange(flipper_length_mm) %\u0026gt;% # sort the data by bill flipper length  head(20) # take the top 20   To see the species composition in penguins_shortflippers:  penguins_shortflippers %\u0026gt;% count(species) #\u0026gt; # A tibble: 2 × 2 #\u0026gt; species n #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 Adelie 17 #\u0026gt; 2 Chinstrap 3   To see the island composition in penguins_shortflippers:  penguins_shortflippers %\u0026gt;% count(island) #\u0026gt; # A tibble: 3 × 2 #\u0026gt; island n #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 Biscoe 7 #\u0026gt; 2 Dream 9 #\u0026gt; 3 Torgersen 4      3 - Using mutate() Besides selecting sets of existing columns, it\u0026rsquo;s often useful to add new columns that are derived from existing columns. The mutate() function create new variables, usually by manipulating existing variables.\nmutate() always adds new columns at the end of the dataframe. When you use mutate(), you need typically to specify 3 things:\n the name of the dataframe you want to modify the name of the new column that you\u0026rsquo;ll create the values to be inserted in the new column  We will be working with the penguins dataset to learn the mutate() function. We will create a new dataframe called mutate_penguins, with a new column called body_mass_g_new.\nThe first argument (dataset to be piped) is the dataframe we\u0026rsquo;re going to modify, penguins. After that, we have the name-value pair for our new variable.\nHere, the name of the new variable is size and the values are body_mass_g multiplied by flipper_length_mm:\nmutate_penguins \u0026lt;- # assign the results to a dataframe `mutate_penguins` penguins %\u0026gt;% # take the penguins_data mutate(size = body_mass_g * flipper_length_mm) # create a new column  head(mutate_penguins) %\u0026gt;% select(6:9) # show the first rows of columns 6-9 #\u0026gt; # A tibble: 6 × 4 #\u0026gt; body_mass_g sex year size #\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 3750 male 2007 678750 #\u0026gt; 2 3800 female 2007 706800 #\u0026gt; 3 3250 female 2007 633750 #\u0026gt; 4 NA NA 2007 NA #\u0026gt; 5 3450 female 2007 665850 #\u0026gt; 6 3650 male 2007 693500  You can see that we created data with a new column called size.\n Breakout session 2 - mutate()  Exercise 2   Create a new dataframe called penguins_bills with a new column called bill_shape by dividing bill length by bill depth.\n  What is the species composition of the 20 penguins with the largest values for bill_shape?\n    Hints (click here)  To get the species composition of the top 20, first use arrange() (think about the direction you need to sort in!), then head(), and then count().\n   Solution (click here)   New dataframe with a bill shape variable:  penguins_bills \u0026lt;- penguins %\u0026gt;% # take the penguins_data mutate(bill_shape = bill_length_mm / bill_depth_mm) # Create a new column `bill_shape`    Species composition of the 20 penguins with the largest bill_shape values:  penguins_bills %\u0026gt;% arrange(-bill_shape) %\u0026gt;% # sort by bill_shape in descending order head(20) %\u0026gt;% # take the top 20 count(species) # create a frequency table #\u0026gt; # A tibble: 1 × 2 #\u0026gt; species n #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 Gentoo 20  They are all Gentoo penguins!\n    Exercise 3 Create a new dataframe called penguins_year:\n with only penguins sampled after 2007, with a new column called year_nr which has a year number that starts counting from 2008 (i.e., 2008 = year 1, 2009 = year 2, etc.) sorted by year_nr.    Hints (click here)  Not all values you pass to mutate() need to be variables! You can subtract year by a fixed number.\n   Solution (click here)  penguins_year \u0026lt;- penguins %\u0026gt;% filter(year \u0026gt; 2007) %\u0026gt;% mutate(year_nr = year - 2007) %\u0026gt;% arrange(year_nr) penguins_year #\u0026gt; # A tibble: 234 × 9 #\u0026gt; species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 Adelie Biscoe 39.6 17.7 186 3500 #\u0026gt; 2 Adelie Biscoe 40.1 18.9 188 4300 #\u0026gt; 3 Adelie Biscoe 35 17.9 190 3450 #\u0026gt; 4 Adelie Biscoe 42 19.5 200 4050 #\u0026gt; 5 Adelie Biscoe 34.5 18.1 187 2900 #\u0026gt; 6 Adelie Biscoe 41.4 18.6 191 3700 #\u0026gt; 7 Adelie Biscoe 39 17.5 186 3550 #\u0026gt; 8 Adelie Biscoe 40.6 18.8 193 3800 #\u0026gt; 9 Adelie Biscoe 36.5 16.6 181 2850 #\u0026gt; 10 Adelie Biscoe 37.6 19.1 194 3750 #\u0026gt; # … with 224 more rows, and 3 more variables: sex \u0026lt;fct\u0026gt;, year \u0026lt;int\u0026gt;, #\u0026gt; # year_nr \u0026lt;dbl\u0026gt;      ","date":1632268800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632418264,"objectID":"e4af6add02aa3153a0b668e1f4c7f766","permalink":"https://biodash.github.io/codeclub/s02e05_tidyverse-intro-part2/","publishdate":"2021-09-22T00:00:00Z","relpermalink":"/codeclub/s02e05_tidyverse-intro-part2/","section":"codeclub","summary":"In this session of Code Club, we will be learning some more about data wrangling in the tidyverse.","tags":null,"title":"Code Club S02E05: Intro to the Tidyverse (Part 2)","type":"codeclub"},{"authors":["Jessica Cooperstone"],"categories":null,"content":"\n Prep homework Basic computer setup   If you didn\u0026rsquo;t already do this, please follow the Code Club Computer Setup instructions, which also has pointers for if you\u0026rsquo;re new to R or RStudio.\n  If you\u0026rsquo;re able to do so, please open RStudio a bit before Code Club starts \u0026ndash; and in case you run into issues, please join the Zoom call early and we\u0026rsquo;ll troubleshoot.\n   Getting started Now that you are familiar with the basics of RMarkdown season 1 and season 2, I put together a RMarkdown file you can download which has the content for today\u0026rsquo;s Code Club.\nDownload today\u0026rsquo;s content   Click here to get an Rmd (optional)  # directory for Code Club Session 2: dir.create(\"S02E04\") # directory for our script # (\"recursive\" to create two levels at once.) dir.create(\"S02E04/Rmds/\") # save the url location for today's script todays_Rmd \u0026lt;- \"https://raw.githubusercontent.com/biodash/biodash.github.io/master/content/codeclub/S02E04_tidyverse-intro-part1/intro-to-tidyverse1.Rmd\" # indicate the name of the new Rmd file intro_tidyverse1 \u0026lt;- \"S02E04/Rmds/intro-to-tidyverse1.Rmd\" # go get that file!  download.file(url = todays_Rmd, destfile = intro_tidyverse1)    What will we go over today\n What is the tidyverse and why would I want to use it? Understanding how to use \u0026ldquo;the pipe\u0026rdquo; %\u0026gt;% Using filter() - picks observations (i.e., rows) based on their values Using select() - picks variables (i.e., columns) based on their names     1 - What is the tidyverse, and how do I use it? The tidyverse is a collection of R packages that are designed for data science. You can certainly use R without using the tidyverse, but it has many packages that I think will make your life a lot easier. The popular package ggplot2 is a part of the core tidyverse, which we have talked about in previous Code Clubs (intro, intro2, maps, and ggplotly), and will talk about in future sessions as well.\nPackages contain shareable code, documentation, tests, and data. One way to download packages is using the function install.packages() which will allow you to download packages that exist within the Comprehensive R Archive Network, or CRAN. There are packages that exist outside CRAN but that is a story for another time.\nBefore we talk more about the tidyverse, let\u0026rsquo;s download it. We only need to do this once.\ninstall.packages(\"tidyverse\")  To use any of the packages within the tidyverse, we need to call them up using library() anytime we want to use the code embedded within them.\nlibrary(tidyverse) #\u0026gt; ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ── #\u0026gt; ✔ ggplot2 3.3.3 ✔ purrr  0.3.4 #\u0026gt; ✔ tibble  3.1.4 ✔ dplyr  1.0.7 #\u0026gt; ✔ tidyr  1.1.3 ✔ stringr 1.4.0 #\u0026gt; ✔ readr  1.4.0 ✔ forcats 0.5.1 #\u0026gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── #\u0026gt; ✖ dplyr::filter() masks stats::filter() #\u0026gt; ✖ dplyr::lag() masks stats::lag()  Let\u0026rsquo;s look at this message, we can see that there are eight \u0026ldquo;attaching packages\u0026rdquo; as part of the \u0026ldquo;core\u0026rdquo; set of tidyverse.\nWe see that there are some conflicts, for example, there is a function called filter() (which we will talk about today) that is part of dplyr (a tidyverse package) that is masking another function called filter() within the stats package (which loads with base R).\nThe conflict arises from the fact that there are now two functions named filter(). After loading the tidyverse, the default filter() will be that from dplyr. If we want explcitly to use the filter() function from stats, we can do that using the double colon operator :: like this: stats::filter().\nNow this is fine for us right now, so there is nothing to do, but it is a good habit to get into reading (and not ignoring) any warnings or messages that R gives you. (It is trying to help!)\nRemember, you can learn more about any package by accessing the help documentation. The help will pop up in the Help tab of the bottom right quadrant of RStudio when you execute the code below.\n?tidyverse  By Mine Çetinkaya-Rundel\nBelow is a quick description of what each package is used for.\n dplyr: for data manipulation ggplot2: a \u0026ldquo;grammar of graphics\u0026rdquo; for creating beautiful plots readr: for reading in rectangular data (i.e., Excel-style formatting) tibble: using tibbles as modern/better dataframes stringr: handling strings (i.e., text or stuff in quotes) forcats: for handling categorical variables (i.e., factors) (meow!) tidyr: to make \u0026ldquo;tidy data\u0026rdquo; purrr: for enhancing functional programming (also meow!)  If you\u0026rsquo;re not understanding what some of this means, that\u0026rsquo;s totally fine.\nThere are more tidyverse packages outside of these core eight, and you can see what they are below.\ntidyverse_packages() #\u0026gt; [1] \"broom\" \"cli\" \"crayon\" \"dbplyr\"  #\u0026gt; [5] \"dplyr\" \"dtplyr\" \"forcats\" \"googledrive\"  #\u0026gt; [9] \"googlesheets4\" \"ggplot2\" \"haven\" \"hms\"  #\u0026gt; [13] \"httr\" \"jsonlite\" \"lubridate\" \"magrittr\"  #\u0026gt; [17] \"modelr\" \"pillar\" \"purrr\" \"readr\"  #\u0026gt; [21] \"readxl\" \"reprex\" \"rlang\" \"rstudioapi\"  #\u0026gt; [25] \"rvest\" \"stringr\" \"tibble\" \"tidyr\"  #\u0026gt; [29] \"xml2\" \"tidyverse\"  tl;dr Tidyverse has a lot of packages that make data analysis easier. None of them are \u0026lsquo;required\u0026rsquo; to do data analysis, but many tidyverse approaches you\u0026rsquo;ll find easier than using base R.\nYou can find here some examples of comparing tidyverse and base R syntax.\n 2 - Using the pipe %\u0026gt;% The pipe operator %\u0026gt;% is a tool that is used for expressing a series of operations. It comes from the magrittr package, and is loaded automatically when you load the tidyverse.\nThe purpose of the pipe is to allow you to take the output of one operation and have it be the starting material of the next step. It also (hopefully) makes your code easier to read and interpret.\nLet\u0026rsquo;s get set up and grab some data so that we have some material to work with.\nIllustration by Allison Horst\nWe are going to use a package called palmerpenguins which has some fun 🐧 data for us to play with. To get this data, we need to install the palmerpenguins package.\ninstall.packages(palmerpenguins)  palmerpenguins is a package developed by Allison Horst, Alison Hill and Kristen Gorman, including a dataset collected by Dr. Kristen Gorman at the Palmer Station Antarctica, as part of the Long Term Ecological Research Network. It is a nice, relatively simple dataset to practice data exploration and visualization in R. Plus the penguins are v cute.\nThen, to use the package, we need to use the function library() to call the package up in R.\nlibrary(palmerpenguins)  The data we will use today is called penguins.\nIllustration by Allison Horst\n# look at the first 6 rows, all columns head(penguins) #\u0026gt; # A tibble: 6 × 8 #\u0026gt; species island bill_length_mm bill_depth_mm flipper_length_… body_mass_g sex  #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; #\u0026gt; 1 Adelie Torge… 39.1 18.7 181 3750 male  #\u0026gt; 2 Adelie Torge… 39.5 17.4 186 3800 fema… #\u0026gt; 3 Adelie Torge… 40.3 18 195 3250 fema… #\u0026gt; 4 Adelie Torge… NA NA NA NA NA  #\u0026gt; 5 Adelie Torge… 36.7 19.3 193 3450 fema… #\u0026gt; 6 Adelie Torge… 39.3 20.6 190 3650 male  #\u0026gt; # … with 1 more variable: year \u0026lt;int\u0026gt; # check the structure of penguins_data # glimpse() which is a part of dplyr functions  # similarly to str() and can be used interchangeably glimpse(penguins) #\u0026gt; Rows: 344 #\u0026gt; Columns: 8 #\u0026gt; $ species \u0026lt;fct\u0026gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel… #\u0026gt; $ island \u0026lt;fct\u0026gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse… #\u0026gt; $ bill_length_mm \u0026lt;dbl\u0026gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, … #\u0026gt; $ bill_depth_mm \u0026lt;dbl\u0026gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, … #\u0026gt; $ flipper_length_mm \u0026lt;int\u0026gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186… #\u0026gt; $ body_mass_g \u0026lt;int\u0026gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, … #\u0026gt; $ sex \u0026lt;fct\u0026gt; male, female, female, NA, female, male, female, male… #\u0026gt; $ year \u0026lt;int\u0026gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…  Okay now we have a sense of what the penguins dataset is.\nIf we want to know how many penguins there are of each species we can use the function count(). In the count() function, the first argument is the dataset, and the next argument is what you want to be counted. You can always learn more about the arguments and syntax of functions by using ?yourfunction() or googling for the documentation. This is the non-pipe approach.\ncount(penguins, species) #\u0026gt; # A tibble: 3 × 2 #\u0026gt; species n #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 Adelie 152 #\u0026gt; 2 Chinstrap 68 #\u0026gt; 3 Gentoo 124  Alternatively, we can use the pipe to send penguins forward through a series of steps. For example, we can use the function count() to figure out how many of each penguin species there are in our dataset.\npenguins %\u0026gt;% # take penguins_data count(species) # count how many of each species there is #\u0026gt; # A tibble: 3 × 2 #\u0026gt; species n #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 Adelie 152 #\u0026gt; 2 Chinstrap 68 #\u0026gt; 3 Gentoo 124  Comparing to base R A main benefit of the pipe is readability, and also the ability to \u0026ldquo;pipe\u0026rdquo; many things together (which we are not doing with count()).\nI want to stress that everything you can do with the tidyverse you can also do using base R. I tend to think the tidyverse is more intuitive than base R, which is why we have elected to teach it here first. Here you can find a bunch of examples comparing tidyverse to base R equivalent syntax. Here is an interesting blogpost on the topic if this is really keeping you up at night.\nI am going to show you an example of a place I think the pipe really shines, don\u0026rsquo;t worry if you don\u0026rsquo;t understand all the syntax, I just want you to see how the pipe can be used.\npenguins %\u0026gt;% drop_na() %\u0026gt;% # drop missing values listed as NA group_by(species) %\u0026gt;% # group by species summarize(mean_mass = mean(body_mass_g)) # summarize mass into new column called  #\u0026gt; # A tibble: 3 × 2 #\u0026gt; species mean_mass #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 Adelie 3706. #\u0026gt; 2 Chinstrap 3733. #\u0026gt; 3 Gentoo 5092.  We are going to continue to use the pipe %\u0026gt;% as we practice with some new dplyr functions.  Breakout session 1 - install tidyverse, use the pipe  In your breakout sessions, make sure that you each have the tidyverse installed and loaded.\n  Solution (click here)  install.packages(\"tidyverse\") install.packages(\"dplyr\") # this is the only one of the 8 tidyverse packages we will use today library(tidyverse)  Occasionally we see people who are having tidyverse install issues, if this happens to you, please read the warning that R gives you, you may need to download an additional package to get it to work. If you have trouble, first try restarting your R session and see if that helps, or reach out to one of the organizers or one of your fellow codeclubbers.\n    We will practice using the pipe. In S02E02, Mike introduced you to some new functions in Exercise 6. Take the dataset penguins and use the pipe to determine the dimensions of the dataframe.\n  Hints (click here)  \nUse dim() to determine the dimensions\n   Solution (click here)  penguins %\u0026gt;% dim() #\u0026gt; [1] 344 8  This means the dataframe is 344 rows and 8 columns in size.\n    Take the dataset penguins and use the pipe to determine the names of the columns of the dataframe.\n  Hints (click here)  \nUse names() or colnames() to pull the column names.\n   Solution (click here)  penguins %\u0026gt;% names() #\u0026gt; [1] \"species\" \"island\" \"bill_length_mm\"  #\u0026gt; [4] \"bill_depth_mm\" \"flipper_length_mm\" \"body_mass_g\"  #\u0026gt; [7] \"sex\" \"year\" # the same penguins %\u0026gt;% colnames() #\u0026gt; [1] \"species\" \"island\" \"bill_length_mm\"  #\u0026gt; [4] \"bill_depth_mm\" \"flipper_length_mm\" \"body_mass_g\"  #\u0026gt; [7] \"sex\" \"year\"  These are the names of our 8 columns.\n    3 - Using select() It has been estimated that the process of getting your data into the appropriate formats takes about 80% of the total time of analysis. I find that getting data into a format that enables analysis often trips people up more than doing the actual analysis. The tidyverse package dplyr has a number of functions that help in data wrangling.\nThe first one we will talk about is select(). Tidyverse is nice in that the functions are very descriptive and intuitive as to what they do: select() allows you to pick certain columns to be included in your data frame.\nLet\u0026rsquo;s try out both the %\u0026gt;% and select(). Let\u0026rsquo;s make a new dataframe from penguins that contains only the variables species, island and sex.\npenguins_only_factors \u0026lt;- penguins %\u0026gt;% select(species, island, sex)  Did it work?\nhead(penguins_only_factors) #\u0026gt; # A tibble: 6 × 3 #\u0026gt; species island sex  #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt;  #\u0026gt; 1 Adelie Torgersen male  #\u0026gt; 2 Adelie Torgersen female #\u0026gt; 3 Adelie Torgersen female #\u0026gt; 4 Adelie Torgersen NA  #\u0026gt; 5 Adelie Torgersen female #\u0026gt; 6 Adelie Torgersen male  Let\u0026rsquo;s check the dimensions of each dataframe to make sure we have what we would expect\n# what are the dimensions of penguins? dim(penguins) #\u0026gt; [1] 344 8 # what are the dimensions of penguins_only_factors? dim(penguins_only_factors) #\u0026gt; [1] 344 3  The output is ordered rows (first number) by columns (second number). Our output makes sense - we haven\u0026rsquo;t removed any observation (i.e., rows), we have only selected some of the columns that we want to work with.\nWhat if we want to pick just the first three columns? We can do that too.\nstr(penguins) # what are those first three columns? #\u0026gt; tibble [344 × 8] (S3: tbl_df/tbl/data.frame) #\u0026gt; $ species : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ... #\u0026gt; $ island : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 3 3 ... #\u0026gt; $ bill_length_mm : num [1:344] 39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ... #\u0026gt; $ bill_depth_mm : num [1:344] 18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ... #\u0026gt; $ flipper_length_mm: int [1:344] 181 186 195 NA 193 190 181 195 193 190 ... #\u0026gt; $ body_mass_g : int [1:344] 3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ... #\u0026gt; $ sex : Factor w/ 2 levels \"female\",\"male\": 2 1 1 NA 1 2 1 2 NA NA ... #\u0026gt; $ year : int [1:344] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ... penguins %\u0026gt;% select(species:bill_length_mm) %\u0026gt;% # pick columns species through bill_length_mm head() # you can add head() as part of your pipe! #\u0026gt; # A tibble: 6 × 3 #\u0026gt; species island bill_length_mm #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 Adelie Torgersen 39.1 #\u0026gt; 2 Adelie Torgersen 39.5 #\u0026gt; 3 Adelie Torgersen 40.3 #\u0026gt; 4 Adelie Torgersen NA  #\u0026gt; 5 Adelie Torgersen 36.7 #\u0026gt; 6 Adelie Torgersen 39.3  Note, in the above chunk, this new dataframe is not being saved because we have not assigned it to anything.\nYou could use slightly different syntax to get the same thing using an indexing approach.\npenguins %\u0026gt;% select(1:3) %\u0026gt;% # pick columns 1-3 head() #\u0026gt; # A tibble: 6 × 3 #\u0026gt; species island bill_length_mm #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 Adelie Torgersen 39.1 #\u0026gt; 2 Adelie Torgersen 39.5 #\u0026gt; 3 Adelie Torgersen 40.3 #\u0026gt; 4 Adelie Torgersen NA  #\u0026gt; 5 Adelie Torgersen 36.7 #\u0026gt; 6 Adelie Torgersen 39.3  There is also convenient shorthand for indicating what you don\u0026rsquo;t want (instead of what you do).\npenguins %\u0026gt;% select(-year) %\u0026gt;% # all the columns except year head() #\u0026gt; # A tibble: 6 × 7 #\u0026gt; species island bill_length_mm bill_depth_mm flipper_length_… body_mass_g sex  #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; #\u0026gt; 1 Adelie Torge… 39.1 18.7 181 3750 male  #\u0026gt; 2 Adelie Torge… 39.5 17.4 186 3800 fema… #\u0026gt; 3 Adelie Torge… 40.3 18 195 3250 fema… #\u0026gt; 4 Adelie Torge… NA NA NA NA NA  #\u0026gt; 5 Adelie Torge… 36.7 19.3 193 3450 fema… #\u0026gt; 6 Adelie Torge… 39.3 20.6 190 3650 male  Embedded within select() is the column order - you can change the order by denoting the order of your columns.\npenguins %\u0026gt;% select(bill_length_mm, island, flipper_length_mm) %\u0026gt;% head() #\u0026gt; # A tibble: 6 × 3 #\u0026gt; bill_length_mm island flipper_length_mm #\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 39.1 Torgersen 181 #\u0026gt; 2 39.5 Torgersen 186 #\u0026gt; 3 40.3 Torgersen 195 #\u0026gt; 4 NA Torgersen NA #\u0026gt; 5 36.7 Torgersen 193 #\u0026gt; 6 39.3 Torgersen 190    4 - Using filter()  Artwork by Allison Horst.  filter() allows you to pick certain observations (i.e, rows) based on their values to be included in your data frame. Let\u0026rsquo;s see it in action.\n Artwork by Allison Horst.  We will select only the \"Chinstrap\" penguins. penguins_chinstrap \u0026lt;- penguins %\u0026gt;% filter(species == \"Chinstrap\") # note the double equals # let's check that it worked penguins_chinstrap %\u0026gt;% count(species) #\u0026gt; # A tibble: 1 × 2 #\u0026gt; species n #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 Chinstrap 68  We can also check to see if we got what we would expect by looking at the dimensions of both penguins and penguins_chinstrap.\ndim(penguins) #\u0026gt; [1] 344 8 dim(penguins_chinstrap) #\u0026gt; [1] 68 8  Great, you can see we have kept all of the columns (denoted by the second number 8), but trimmed down the rows/observations to only the Chinstrap penguins.\nYou can use filter() in other useful ways too. Let\u0026rsquo;s make a new dataframe that has only the penguins that are over 5000 g.\nbig_penguins \u0026lt;- penguins %\u0026gt;% filter(body_mass_g \u0026gt; 5000) # did it work? big_penguins %\u0026gt;% select(body_mass_g) %\u0026gt;% range() #\u0026gt; [1] 5050 6300 # another faster non-tidyverse way to do this range(big_penguins$body_mass_g) #\u0026gt; [1] 5050 6300  You can start stacking qualifiers to get the exact penguins you want. Let\u0026rsquo;s say we are only interested in penguins that are female and on the island Dream.\npenguins %\u0026gt;% filter(sex == \"female\" \u0026amp; island == \"Dream\") #\u0026gt; # A tibble: 61 × 8 #\u0026gt; species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 Adelie Dream 39.5 16.7 178 3250 #\u0026gt; 2 Adelie Dream 39.5 17.8 188 3300 #\u0026gt; 3 Adelie Dream 36.4 17 195 3325 #\u0026gt; 4 Adelie Dream 42.2 18.5 180 3550 #\u0026gt; 5 Adelie Dream 37.6 19.3 181 3300 #\u0026gt; 6 Adelie Dream 36.5 18 182 3150 #\u0026gt; 7 Adelie Dream 36 18.5 186 3100 #\u0026gt; 8 Adelie Dream 37 16.9 185 3000 #\u0026gt; 9 Adelie Dream 36 17.9 190 3450 #\u0026gt; 10 Adelie Dream 37.3 17.8 191 3350 #\u0026gt; # … with 51 more rows, and 2 more variables: sex \u0026lt;fct\u0026gt;, year \u0026lt;int\u0026gt;  There are lots of useful generic R operators that you can use inside functions like filter() including:\n ==: exactly equals to \u0026gt;=: greater than or equals to, you can also use ≥ \u0026lt;=: less than or equals to, you can also use ≤ \u0026amp;: and |: or !=: not equal to !x: not x is.na(): is NA (i.e. missing data)  There is a longer list of helpful select() features here.\ntl;dr, select() picks columns/variables and filter() picks rows/observations.\nBreakout session 2 - pipe, filter, select Exercise 1  Make a new dataframe called penguins_new that includes only the columns with numeric or integer data.\n  Hints (click here)  \nUse str() or glimpse() to figure out which columns are numeric or integers. Then use select() to pick only the columns you want.\n   Solution (click here)  glimpse(penguins) #\u0026gt; Rows: 344 #\u0026gt; Columns: 8 #\u0026gt; $ species \u0026lt;fct\u0026gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel… #\u0026gt; $ island \u0026lt;fct\u0026gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse… #\u0026gt; $ bill_length_mm \u0026lt;dbl\u0026gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, … #\u0026gt; $ bill_depth_mm \u0026lt;dbl\u0026gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, … #\u0026gt; $ flipper_length_mm \u0026lt;int\u0026gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186… #\u0026gt; $ body_mass_g \u0026lt;int\u0026gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, … #\u0026gt; $ sex \u0026lt;fct\u0026gt; male, female, female, NA, female, male, female, male… #\u0026gt; $ year \u0026lt;int\u0026gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007… penguins_new \u0026lt;- penguins %\u0026gt;% select(bill_length_mm:body_mass_g, year) # check to see if it worked head(penguins_new) #\u0026gt; # A tibble: 6 × 5 #\u0026gt; bill_length_mm bill_depth_mm flipper_length_mm body_mass_g year #\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 39.1 18.7 181 3750 2007 #\u0026gt; 2 39.5 17.4 186 3800 2007 #\u0026gt; 3 40.3 18 195 3250 2007 #\u0026gt; 4 NA NA NA NA 2007 #\u0026gt; 5 36.7 19.3 193 3450 2007 #\u0026gt; 6 39.3 20.6 190 3650 2007  Getting fancy with some more advanced options\n# this works too penguins_new2 \u0026lt;- penguins %\u0026gt;% select(ends_with(\"mm\"), body_mass_g, year) # this works three penguins_new3 \u0026lt;- penguins %\u0026gt;% select(where(is.numeric) | where(is.integer)) # are they all the same? all.equal(penguins_new2, penguins_new3) #\u0026gt; [1] TRUE      Exercise 2  Make a new dataframe called penguins_adelie_female that includes only the female penguins that are of the species Adelie.\n  Hints (click here)  \nUse filter() to set which penguins you want to keep. Use the %\u0026gt;% and count() to make sure what you did worked.\n   Solution (click here)  penguins_adelie \u0026lt;- penguins %\u0026gt;% filter(species == \"Adelie\" \u0026amp; sex == \"female\") # check to see if it worked penguins_adelie %\u0026gt;% count(species, sex) #\u0026gt; # A tibble: 1 × 3 #\u0026gt; species sex n #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 Adelie female 73      Exercise 3  Make a new dataframe called penguins_dream_or_2007 that includes only the penguins on the island dream, or from the year 2007. Then make sure the dataframe only contains those variables you have filtered on.\n  Hints (click here)  \nUse filter() to set which penguins you want to keep. Use the %\u0026gt;% and select() to construct your new dataframe.\n   Solution (click here)  penguins_dream_or_2007 \u0026lt;- penguins %\u0026gt;% filter(island == \"Dream\" | year == \"2007\") %\u0026gt;% select(island, year) head(penguins_dream_or_2007) #\u0026gt; # A tibble: 6 × 2 #\u0026gt; island year #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 Torgersen 2007 #\u0026gt; 2 Torgersen 2007 #\u0026gt; 3 Torgersen 2007 #\u0026gt; 4 Torgersen 2007 #\u0026gt; 5 Torgersen 2007 #\u0026gt; 6 Torgersen 2007 # did it work? penguins_dream_or_2007 %\u0026gt;% count(island, year) #\u0026gt; # A tibble: 5 × 3 #\u0026gt; island year n #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 Biscoe 2007 44 #\u0026gt; 2 Dream 2007 46 #\u0026gt; 3 Dream 2008 34 #\u0026gt; 4 Dream 2009 44 #\u0026gt; 5 Torgersen 2007 20      Further reading There are many good (free) references for the tidyverse, including the book R for Data Science by Hadley Wickham and Garrett Grolemund.\nThe package dplyr, as part of the tidyverse has a number of very helpful functions that will help you get your data into a format suitable for your analysis.\nRStudio makes very useful cheatsheets, including ones on tidyverse packages like dplyr, ggplot2, and others.\n","date":1631664000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631724210,"objectID":"df085db1615b3713af0a446430dc8eb4","permalink":"https://biodash.github.io/codeclub/s02e04_tidyverse-intro-part1/","publishdate":"2021-09-15T00:00:00Z","relpermalink":"/codeclub/s02e04_tidyverse-intro-part1/","section":"codeclub","summary":"During this session of Code Club, we will be learning about what the tidyverse is, the pipe operator, and how to use some of the most popular dplyr one-table functions, including filter and select.","tags":null,"title":"Code Club S02E04: Intro to the Tidyverse (Part 1)","type":"codeclub"},{"authors":["Michael Broe"],"categories":null,"content":"\n Introduction R Markdown consists of an amazing ecosystem of R packages to produce many types of technical content. Its signature capability is that is can print formatted text, run R code, and display the results, all inside a single document. Furthermore, you can easily export this document in a large variety of formats, including HTML, PDF, Word, RTF, etc. The webpage you are looking at now was written (almost) completely in R Markdown.\nAt the most basic level, instead of using comments interleaved with your code in an R script:\n# This is a comment\nyou can insert formatted text around your code in an R Markdown file. You can structure your document with headings and subheadings. You can add tables of contents. You can even generate formatted bibliographies. And the R code you insert in the document runs inside the document and the results go to the document itself, not to the console or (in the case of plots) to the Plots pane in RStudio.\nThis makes RMarkdown a great computer lab notebook, since you can explain what you\u0026rsquo;re doing and why (to colleagues or your future self). It\u0026rsquo;s also an example of reproducible research since you share not just a Word file, say, with example code, but an active document in which the code actually runs and the results are reproduced.\nTo understand R Markdown, we need to learn about three new things:\n  Markdown, a very lightweight text formatting language.\n  Code chunks, which allow us to incorporate R code that can be executed and whose results we can display in text, figures, and tables.\n  The YAML header, which encodes metadata about the output, such as the desired output format and specific formatting features.\n  We\u0026rsquo;ll focus on HTML output, but will also glance at other possibilities for the output format: with R Markdown, it is possible to create not just technical reports, but also slide decks, websites, books, scientific articles, dissertations, and so on.\nSetup At the core of the R Markdown ecosystem is the rmarkdown package. We need to install this but don\u0026rsquo;t need to load it:\ninstall.packages(\"rmarkdown\")  Inside your directory for Code Club, create a folder for this week:\ndir.create('S02E03')  Select this folder in the Files pane. Then make this your working directory, using \u0026ldquo;Set as Working Directory\u0026rdquo; from the More options:\n(You don\u0026rsquo;t need to do this now: we\u0026rsquo;ll make it part of the first Breakout Room in just a moment\u0026hellip;)\nFirst, an example Before we go into details, let\u0026rsquo;s first see a quick demonstration of what we\u0026rsquo;re talking about. RStudio lets you create an example R Markdown document with a couple of clicks. Here are the instructions: I\u0026rsquo;ll run through them, and then we\u0026rsquo;ll open Breakout rooms so you can try it out yourself.\n  Go to File =\u0026gt; New File =\u0026gt; R Markdown, change the Title to \u0026ldquo;Markdown demo\u0026rdquo;, and click OK.\n  Take a look at the R Markdown document, and notice that there seems to be some sort of header bounded by three dashes (=\u0026gt; YAML), followed by R code wrapped in strange constructs with backticks and curly brackets (=\u0026gt; Code chunks), and formatted written text (=\u0026gt; Markdown).\n  Before we can render the output, we need to save the document. Click the Save button and save the files as demo.Rmd inside your newly created directory.\n  Now click the Knit button in one of the top bars, and a document should show up in a pop-up in the Viewer pane. This is the rendered output from the R Markdown document, which is translated from Markdown into HTML behind the scenes, and displayed using a built in web-browser.\n  Notice that the YAML header is not printed (at least not verbatim) while some of the code is printed (some is hidden), and we also see code output, including a plot!\nThis is what the raw and rendered output look side-by-side:\n Breakout Room 1 Work through the example above yourselves. Generate a sample R Markdown file, in the correct directory, look at the contents, and make sure you can render it on your system using the \u0026ldquo;Knit\u0026rdquo; to HTML button in the top command bar.\n  We\u0026rsquo;ll now talk about Markdown, code chunks, and the YAML header in turn.\nI: Markdown To fully appreciate the magic of Markdown and where it came from, it\u0026rsquo;s useful to just briefly visit the notion of a Markup language.\nThe original markup was done by a subeditor using a blue pencil on a handwritten or typed manuscript. This markup included typographic instructions for printing, like making a heading larger or text boldface, etc.\nComputer Markup languages have the same kind of annotations, which are included in the plain text, but are visually different from the content. This marked-up text is then sent to an interpreter (e.g. a web browser, a PDF viewer, an app like Word) which renders the final document.\nA large number of markup languages have been developed over the years. At current count there are about 60. This is how you would markup italic text in a small selection of them:\nHTML\n\u0026lt;i\u0026gt;This text is italic\u0026lt;/i\u0026gt;\nWord .docx\n\u0026lt;w:t\u0026gt;This is italic text.\u0026lt;/w:t\u0026gt;\nTeX\n\\textit{This text is italic}\nODF text document .odt\n\u0026lt;text:p text:style-name=\u0026quot;P1\u0026quot;\u0026gt;This text is italic.\u0026lt;/text:p\u0026gt;\nRich text format .rtf\n{\\rtlch\\ai \\ltrch\\loch\\i\\loch This text is italic.}\nSome of these are more readable than others. Some are never meant to be read by humans at all! But underneath the hood every format is actually Markup.\nThe aim of Markdown is to create lightweight Markup language, which is easy to read and easy to write in a text editor. It embodies the principle: \u0026ldquo;make common things easy, and rare things possible\u0026rdquo;. Then we let the computer do the work of translating Markdown into various markup languages and rendering them, so we don\u0026rsquo;t have to worry about the details:\nMarkdown\n*This text is italic.*\nThe \u0026ldquo;Swiss Army Knife\u0026rdquo; for letting the computer convert between Markup languages is Pandoc. The Pandoc site contains a graphic which shows what can be translated to what (included here just to give you a sense of the extent of this Markup world):\n  Click here to see the Pandoc figure   R Markdown uses Pandoc as its engine for translating Markdown to various Markup languages.\nAn overview of commonly used Markdown syntax    Syntax Result     # My Title Header level 1 (largest)   ## My Section Header level 2   ### My Subsection Header level 3 \u0026ndash; and so forth   *italic* or _italic_ italic   **bold** or __bold__ bold   [Markdown Guide](markdownguide.org) Markdown Guide (Link with custom text)   ![](path/to/figure.png) Figure   - List item Unordered (bulleted) list   1. List item Ordered (numbered) list   `inline code` inline code   ``` \u0026hellip;code\u0026hellip; ``` Generic code block (for formatting only) (Alternative syntax: 4 leading spaces.)   ```r \u0026hellip;code\u0026hellip; ``` r code block (for formatting only)   --- Horizontal rule (line)    Below you\u0026rsquo;ll see an examples of raw Markdown on the left, and its rendered (formatted) output on the right:\n \u0026ldquo;Plain\u0026rdquo; Markdown files have the extension .md, whereas R Markdown\nfiles have the extension .Rmd.\n   II: Integrating R code As we saw above, plain Markdown has syntax for code formatting, but the code is not actually being executed. In R Markdown, however, we are able run code! The syntax to do so is only slightly modified from what we saw above:\n  For inline code, we add r and a space before the R code that is to be executed, for example:\n   Raw Rendered     There are `365*24` hours in a year There are 365*24 hours in a year   There are `r 365*24` hours in a year There are 8760 hours in a year      To generate code blocks, which we call code chunks in an R Markdown context,\nwe add r inside curly braces: ```{r}\nWe can optionally add settings that we want to apply to that chunk:\n```{r, option1=value, ...}\nThese options control things like:\n do you want your code to be displayed in the document, or just the results? do you want alerts and warnings to be displayed or not? do you want to turn off results, temporarily or permanently?    and many others.\n RStudio keyboard shortcut to insert a code chunk: Cmd/Ctrl+Alt+I.\nThere is also an Insert Code Chunk Button in the top bar of RStudio.\n  Code chunk examples In these examples we\u0026rsquo;ll use the Palmer Penguins dataset. To access this dataset yourself, do:\ninstall.packages(\u0026#34;palmerpenguins\u0026#34;) library(palmerpenguins) The example code we\u0026rsquo;ll be using comes from the tidyverse package. If you don\u0026rsquo;t have that installed yet, you need to do:\ninstall.packages(\u0026#34;tidyverse\u0026#34;) library(tidyverse) (If you\u0026rsquo;ve participated in Code Club before you probably have these packages installed).\nDon\u0026rsquo;t worry at all if you don\u0026rsquo;t understand the example code. This is exactly what we\u0026rsquo;ll be moving onto in the coming weeks. The point is that the code is executed and displayed inside the document.\n  A code chunk with default options\u0026hellip;\n\u0026hellip;will be executed and shown followed by the output of the code:\nmean(penguins$bill_depth_mm, na.rm = TRUE) #\u0026gt; [1] 17.15117    As an example of using a code chunk option, we will disable printing the code using echo=FALSE (the code will still run and the output will still be shown):\n#\u0026gt; [1] 17.15117    Figures have some specific options, including captions:\nggplot(penguins) + geom_point(aes(x = bill_length_mm, y = bill_depth_mm, color = species)) + theme_bw() #\u0026gt; Warning: Removed 2 rows containing missing values (geom_point).  Fig. 1: Bill length and depth are correlated within species, and differ subtly between species.     We added a caption for the figure using the fig.cap option.\nCode chunk options There are huge number of options, and various options are specific to certain types of code chunks. Just learn the basic structure first, and if you ever wonder \u0026ldquo;Can I do X to modify the output?\u0026rdquo; just know that many, many people have wondered that before, and if it makes sense to do it, you can probably do it with options.\nHere is an overview of some the most commonly made changes to defaults for code chunk options. This quickly gets confusing, but you\u0026rsquo;ll get the hang of it after experimenting a bit.\n echo=FALSE \u0026ndash; Don\u0026rsquo;t print the code in the output file. eval=FALSE \u0026ndash; Don\u0026rsquo;t run (evaluate) the code. include=FALSE \u0026ndash; Run but don\u0026rsquo;t print the code, nor any of its results. results=\u0026quot;hide\u0026quot; \u0026ndash; Don\u0026rsquo;t print the text output of the code. fig.show=\u0026quot;hide\u0026quot; \u0026ndash; Don\u0026rsquo;t print figures produced by the code.  Furthermore, you can use message=FALSE and warning=FALSE to suppress any messages (like the output when loading packages) and warnings (like the warning for the penguin figure above), respectively, that R might produce.\nFor figures, the following options are especially useful:\n fig.cap=\u0026quot;My caption\u0026quot; \u0026ndash; Include a caption. fig.asp=0.6 \u0026ndash; Aspect ratio. fig.width=6 \u0026ndash; Width of in inches: same as sizing in regular R code. fig.height=9.6 \u0026ndash; Height in inches: same as sizing in regular R code. out.width=\u0026quot;70%\u0026quot; \u0026ndash; Figure width as printed in the document (in % or pixels, px). out.height=\u0026quot;500px\u0026quot; \u0026ndash; Figure height as printed in the document.   III: The YAML header YAML (\u0026ldquo;YAML Ain\u0026rsquo;t Markup Language\u0026rdquo;) is a simple format commonly used for configuration files, which allows you to provide key-value pairs such as author: John Doe.\nIn R Markdown files, it is used as a header which configures certain aspects of the output, especially the formatting. Put another way, the YAML header contains the metadata for the output.\nA basic YAML header Here is an example of a very basic YAML header:\n--- author: My name title: The document's title output: html_document ---  Note the lines which just contain three dashes, which mark the beginning and the end of the YAML header.\nAdding extra options Often, a value (like html_document) can itself be given key-value pairs to specify additional options \u0026ndash; see the example below where we include a Table of Contents (toc) and also set it to \u0026ldquo;float\u0026rdquo;:\n--- output: html_document: toc: true toc_float: true ---   Note that indentation in YAML uses two (or four) spaces (no tabs!) per indentation level, and it is sensitive to indentation errors. (Fortunately, RStudio inserts spaces for tabs by default \u0026ndash; check/set in Tools =\u0026gt; Global Options =\u0026gt; Code =\u0026gt; Editing.)   Some options for html_document output html_document is the most commonly used output format for R Markdown documents, and here are few particularly useful options to customize the output:\n code_download: true \u0026ndash; Include a button to download the code. code_folding: hide \u0026ndash; Using hide or show will enable the folding of code chunks, with hide hiding them by default. toc: true \u0026ndash; Include a table of contents (Also: toc_depth: 3 sets depth to 3, toc_float: true lets the TOC \u0026ldquo;float\u0026rdquo; as you scroll down the document). number_sections: true \u0026ndash; Number the section headings. df_print: paged \u0026ndash; Get nicely formatted and paged data frame printing (also try: df_print: kable). theme: cerulean \u0026ndash; Use a pre-built theme, controlling the overall look and feel of the document. See here for a visual overview.    Three HTML document theme options: darkly, flatly, and cerulean.      IV: R Markdown and RStudio The R Markdown ecosystem of packages is being developed by RStudio, so it should come as no surprise that the RStudio IDE has some nice R Markdown functionality.\nKnitting and previewing your document The process of rendering an R Markdown file into another format, as specified by the YAML header, is called knitting. We already saw the button to knit the current document (keyboard shortcut: Cmd/Ctrl+Shift+K).\n If you get preview pop-up windows in RStudio, click the cog wheel icon next to the Knit button, and then select \u0026ldquo;Preview in Viewer Pane\u0026rdquo;.\n  \nInstead of knitting the entire document, you can also run individual code chunks using the green \u0026ldquo;play button\u0026rdquo; (or Cmd/Ctrl+Shift+Enter), or all code chunks up until the current one (button to the left of the play button).\n Breakout Room 2 For this exercise, you\u0026rsquo;ll convert an R Script file into an R Markdown file.\nFirst you\u0026rsquo;ll download the script file to your working directory (using the code provided below). Then open it in R Studio, copy the contents, and paste the contents into your demo.Rmd file (after the YAML header). Then work through making adjustments, and Knit at various times to check your work.\nThere are various suggestions (in parentheses) for Markdown formatting throughout the script.\nAnd remember, you need to wrap the actual R code from the script inside R Markdown code chunks.\nYou\u0026rsquo;ll also download an example picture. Include this picture in your demo.Rmd file using the Markdown syntax shown above. Then experiment with the various figure options to get it how you want it.\nWe\u0026rsquo;ll download the files in a similar way to last week. Execute the following code, either by copying into the console, or by creating a new script file and copying the commands into it, and executing them one by one. To keep things tidy and readable, first we create objects that hold the URLs we\u0026rsquo;re downloading from:\nscript_URL \u0026lt;- \u0026#34;https://raw.githubusercontent.com/biodash/biodash.github.io/master/content/codeclub/S02E03_rmarkdown/demo.R\u0026#34; picture_URL \u0026lt;- \u0026#34;https://raw.githubusercontent.com/biodash/biodash.github.io/master/content/codeclub/S02E03_rmarkdown/picture.jpg\u0026#34; Then use the download.file() function with two arguments, the remote URL and the local file name (which must be in quotes):\ndownload.file(url = script_URL, destfile = \u0026#34;demo.R\u0026#34;) download.file(url = picture_URL, destfile = \u0026#34;picture.jpg\u0026#34;) You should end up with the two local files showing up in your Files pane.\n  V: A single source doc, many output formats! Because of the Pandoc backend, a great feature of R Markdown is that you can output to many formats. So from one source document, or very similar variants, you can create completely different output depending on what you need.\nBuilt-in output formats The built-in output formats, which can be used with just the rmarkdown package, are listed below. These include HTML, PDF, Word, PowerPoint, and different HTML slide show formats.\nExtension output formats It\u0026rsquo;s worth highlighting a few of the output formats that can be used with the following packages in the R Markdown ecosystem:\n  distill \u0026ndash; An output format geared towards technical content, e.g. with extended support for equations, citations, and footnotes. Can also create websites.\n  rticles \u0026ndash; R Markdown templates to format output for specific scientific journals.\n  flexdashboard \u0026ndash; Create interactive \u0026ldquo;dashboards\u0026rdquo; to present data.\n  bookdown \u0026ndash; A book format, the R Markdown book is an example.\n  xaringan \u0026ndash; Create fancier presentation slides thanks to a JavaScript library.\n  Starting to use these and other output formats is often as simple as changing the YAML header:\n---output:distill::distill_article---\n Further resources   Free online books by the primary creator of R Markdown and other authors:\n R Markdown \u0026ndash; The Definitive Guide R Markdown Cookbook    RStudio\u0026rsquo;s 5-page R Markdown Reference PDF\n  RStudio\u0026rsquo;s R Markdown Cheatsheet\n  RStudio R Markdown lessons\n  Markdown tutorial\n  ","date":1630540800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631210881,"objectID":"d66c3740a020d0474cb6ee94580d5f78","permalink":"https://biodash.github.io/codeclub/s02e03_rmarkdown/","publishdate":"2021-09-02T00:00:00Z","relpermalink":"/codeclub/s02e03_rmarkdown/","section":"codeclub","summary":"In this session of Code Club, we'll learn about Markdown syntax and some of the great functionality of R Markdown.","tags":["codeclub","markdown","rmarkdown"],"title":"Code Club S02E03: R Markdown","type":"codeclub"},{"authors":["Mike Sovic"],"categories":null,"content":"\nLearning objectives   Create objects in R Recognize and use R functions Differentiate between some common object classes and data structures in R Read in data from a file Install and load R packages    1 \u0026ndash; Intro Nearly everything you do in R will involve objects, functions, or (often) both. In this session, we\u0026rsquo;ll take a quick look at each of these fundamental components for working in R. In addition, we\u0026rsquo;ll get introduced to R packages (since they\u0026rsquo;ll provide many of the functions you\u0026rsquo;ll use), and also practice reading some data in to R.\n2 \u0026ndash; Objects Objects are things in R to which a name can be assigned. They\u0026rsquo;re created using the assignment operator \u0026ldquo;\u0026lt;-\u0026rdquo;, which can be thought of as an arrow (it\u0026rsquo;s actually two separate characters - the less than symbol and dash) that points whatever is on the right side to a name provided on the left side. For example, running the following three lines of code creates objects named \u0026ldquo;x\u0026rdquo;, \u0026ldquo;y\u0026rdquo;, and \u0026ldquo;z\u0026rdquo;, respectively\u0026hellip;\nx \u0026lt;- 3 + 3 y \u0026lt;- TRUE z \u0026lt;- \"cat\"   If you run these lines in RStudio, you\u0026rsquo;ll see the resulting objects listed in the top right panel. This is really helpful for keeping track of object names, as you\u0026rsquo;ll often create many objects during an R session. Calling the objects returns their values\u0026hellip;\nx #\u0026gt; [1] 6 y #\u0026gt; [1] TRUE z #\u0026gt; [1] \"cat\"   3 \u0026ndash; Functions We\u0026rsquo;ll return to objects shortly, but first, let\u0026rsquo;s take a very basic look at functions, which make up a second really important part of R. You can think of objects as being things in R, while functions do things in R. I\u0026rsquo;ll start by writing a very simple function\u0026hellip;\n#define the function cubed_plus10 \u0026lt;- function(number) \u0026#123; number^3+10 \u0026#125; #apply the function cubed_plus10(4) #\u0026gt; [1] 74   No need to get caught up in details of writing the function right now. A couple important things to recognize at this point\u0026hellip;\n We created a function that took some input - the \u0026lsquo;4\u0026rsquo; in the example above, did something to it, and returned some output. To run the function, we used its name, followed by a set of parentheses. All R functions have that general structure. The parentheses might contain 0, 1, or more items (often referred to as options or arguments).  It\u0026rsquo;s useful to be able to create your own functions. But if that sounds a little advanced to you at this point, you can still do a lot with R even without knowing how to write your own, as there are lots that have already been written for you. A number of commonly-used functions are available as soon as you start an R session - these are often referred to as \u0026ldquo;base R\u0026rdquo; functions.\n#some example base R functions date() #\u0026gt; [1] \"Tue Aug 31 10:24:39 2021\" getwd() #\u0026gt; [1] \"/Users/sovic.1/Desktop/docs/Desktop_clear_4-28-21/S18_dev/biodash.github.io/content/codeclub/S02E02_r-intro_part2\" sqrt(25) #\u0026gt; [1] 5   4 \u0026ndash; Object Classes and Data Structures Now that we have at least a basic idea of R functions, we\u0026rsquo;ll turn attention back to objects (and use functions along the way from here on out). The objects we\u0026rsquo;ve created so far have been pretty simple. Let\u0026rsquo;s revisit the three from above\u0026hellip;\nx #\u0026gt; [1] 6 y #\u0026gt; [1] TRUE z #\u0026gt; [1] \"cat\"   There are lots of different kinds, or classes of objects in R, and behind the scenes, each object that\u0026rsquo;s created is immediately assigned to a class (or possibly multiple classes). There\u0026rsquo;s no real limit to the number of classes that exist, as new ones can be created for specialized cases at any time. But there are a fairly small number of object classes you\u0026rsquo;ll encounter a lot in R, so we\u0026rsquo;ll take a look at some of those now. Let\u0026rsquo;s use the class() function to figure out what class R assigned each of our objects to\u0026hellip;\nclass(x) #\u0026gt; [1] \"numeric\" class(y) #\u0026gt; [1] \"logical\" class(z) #\u0026gt; [1] \"character\"   Some very common object classes you\u0026rsquo;ll encounter\u0026hellip;\n  integer double (numeric) logical character factor   I\u0026rsquo;m going to introduce a new term here that\u0026rsquo;s closely tied to object classes, and that\u0026rsquo;s data structures. Again, there are a small number of data structures you\u0026rsquo;ll work with a lot in R. They include\u0026hellip;\n  Vectors Matrices Data Frames Lists Arrays   Today we\u0026rsquo;ll focus in on two of these - vectors and data frames.\n5 \u0026ndash; Vectors Vectors in R share a couple important characteristics\u0026hellip;\n They\u0026rsquo;re one-dimensional. In other words, they can be defined by a length property, with length zero, one, or more. All elements of a vector must be of the same type, or class. Operations can be performed on vectors - vector recycling rules apply (we\u0026rsquo;ll see this in the breakout exercises next).  The c() function is useful for creating vectors in R. It stands for \u0026ldquo;combine\u0026rdquo;, and allows you to combine multiple items into a single vector object\u0026hellip;\nodds \u0026lt;- c(1,3,5,7,9) animals \u0026lt;- c(\"dog\", \"cat\", \"cow\") #view the objects odds #\u0026gt; [1] 1 3 5 7 9 animals #\u0026gt; [1] \"dog\" \"cat\" \"cow\" #check their class class(odds) #\u0026gt; [1] \"numeric\" class(animals) #\u0026gt; [1] \"character\"    Breakout Rooms I (~10 min.) Exercise 1: Create Vector Objects  Create two objects (vectors) named short_vec and long_vec. To short_vec, assign the integers 1 through 5, and to long_vec, assign the integers 1 through 10. View each of the objects and check their class.\n  Hint (click here)  \nThe colon can be used to define a sequence of integers in R, for example, 1:3 represents the vector 1,2,3.    Solution (click here)  short_vec \u0026lt;- 1:5 long_vec \u0026lt;- 1:10 short_vec #\u0026gt; [1] 1 2 3 4 5 long_vec #\u0026gt; [1] 1 2 3 4 5 6 7 8 9 10 class(short_vec) #\u0026gt; [1] \"integer\" class(long_vec) #\u0026gt; [1] \"integer\"      Exercise 2: Vector Operations/Recycling  Now try adding the two vectors, short_vec and long_vec, together. Assign the result to a new object named vec_sum. Think about and talk through what you expect the result might look like before you execute the code.\n  Hint (click here)  \nBefore R performs an operation on any vectors, the vectors involved must be the same length. If they aren\u0026rsquo;t, the shorter vector is \u0026ldquo;recycled\u0026rdquo; until it matches the length of longer vector. Then the operation is performed.    Solution (click here)  vec_sum \u0026lt;- short_vec + long_vec vec_sum #\u0026gt; [1] 2 4 6 8 10 7 9 11 13 15      Exercise 3: Vector Operations/Recycling II  Just to drive this point on vector recycling home a little more, let\u0026rsquo;s do more operation - this time, subtract 3 (itself a vector of length 1) from short_vec. Again, try to predict what will happen before you run it.\n  Solution (click here)  short_vec - 3 #\u0026gt; [1] -2 -1 0 1 2      Keep in mind that all elements of a vector in R have to be of the same class. This means you typically won\u0026rsquo;t see a vector that looks like\u0026hellip;\nmixed_vector \u0026lt;- c(1, \"cat\", 4, \"dog\", TRUE)   You can create such a vector, but in this case, R will view it as a character vector, meaning, for example, the 1 and 4 won\u0026rsquo;t be treated as numbers, but as characters. Such forcing of an element or object into a specific class is often referred to as coercion. Let\u0026rsquo;s go back to our object \u0026lsquo;x\u0026rsquo;\u0026hellip;\nx #\u0026gt; [1] 6 class(x) #\u0026gt; [1] \"numeric\"   Notice it\u0026rsquo;s assigned as numeric. Let\u0026rsquo;s say we wanted R to see it as an integer (just slightly different in R than numeric)\u0026hellip;\nx \u0026lt;- as.integer(x) class(x) #\u0026gt; [1] \"integer\"   We have used the as.integer() function to coerce x into class integer.\n 6 \u0026ndash; Data Frames Data frames are another data structure in R you\u0026rsquo;ll likely use a lot. Some characteristics of data frames\u0026hellip;\n They have two dimensions (rows, columns) Each variable (column) needs to have the same number of entries. All elements of any one column have to be of the same type/class, but different columns can be of different classes.  A good way to think about a data frame is as being analogous to an Excel spreadsheet, with the caveat that all the columns have the same number of entries, which isn\u0026rsquo;t a requirement in an Excel sheet.\nYou can create a data frame by hand with the data.frame() function, but in many cases, you\u0026rsquo;ll create a data frame in R by reading data in from a file with one of a number of functions that are available for that purpose. Let\u0026rsquo;s try it. Here\u0026rsquo;s a small example dataset I generated in Excel\u0026hellip;\nIt\u0026rsquo;s available at the following address\u0026hellip; https://raw.githubusercontent.com/biodash/biodash.github.io/master/assets/data/data_frame/example_df.tsv\n(In this case, the data set comes from online, but often it will be a file on your computer, and the path to the file works the same way as how you\u0026rsquo;ll use the web address below.)\n Breakout Rooms II (~10 min.) Exercise 4: Reading In A Data Frame  Create an object named \u0026ldquo;data_address\u0026rdquo; that stores the web address for the dataset.\n  Hint (click here)  \nUse the assignment operator and make sure to put the address in quotes to define it as a character string.    Solution (click here)  data_address \u0026lt;- \"https://raw.githubusercontent.com/biodash/biodash.github.io/master/assets/data/data_frame/example_df.tsv\"    Now use the read.table() function to read the dataset in as a data frame. Assign it as an object named \u0026lsquo;exp_data\u0026rsquo; and view it in R.\n  Hint (click here)  \nUse the data_address object as the first argument in the read.table() function.    Solution (click here)  exp_data \u0026lt;- read.table(data_address) exp_data #\u0026gt; V1 V2 V3 V4 #\u0026gt; 1 Age Height_cm Eye_Color Graduated #\u0026gt; 2 22 175.26 Blue TRUE #\u0026gt; 3 67 170.18 Green TRUE #\u0026gt; 4 53 165.1 Blue FALSE #\u0026gt; 5 13 134.62 Brown FALSE #\u0026gt; 6 19 147.32 Green TRUE #\u0026gt; 7 27 185.42 Gray TRUE #\u0026gt; 8 30 190.5 Blue TRUE #\u0026gt; 9 11 144.78 Brown FALSE      Exercise 5: Getting Info About Functions  The column names (header) were read in as the first observation, and default column names (i.e. V1, V2, etc) were added. Take a look at the help page for read.table() by typing ?read.table in your R console, or by searching for \u0026ldquo;read.table\u0026rdquo; in the search box in the Help tab of the lower right RStudio panel. Look through some of the options/arguments and make an adjustment to fix the column names/header, then view the data frame again.\n  Hint (click here)  \nRerun the command, setting \u0026ldquo;header\u0026rdquo; to TRUE, instead of the default FALSE.    Solution (click here)  exp_data \u0026lt;- read.table(data_address, header = TRUE) exp_data #\u0026gt; Age Height_cm Eye_Color Graduated #\u0026gt; 1 22 175.26 Blue TRUE #\u0026gt; 2 67 170.18 Green TRUE #\u0026gt; 3 53 165.10 Blue FALSE #\u0026gt; 4 13 134.62 Brown FALSE #\u0026gt; 5 19 147.32 Green TRUE #\u0026gt; 6 27 185.42 Gray TRUE #\u0026gt; 7 30 190.50 Blue TRUE #\u0026gt; 8 11 144.78 Brown FALSE      Exercise 6: Practicing With Some Functions  Spend a few minutes playing around with some of the following functions and try to figure out what they do by applying them to the exp_data object. If it\u0026rsquo;s not clear, use the help\u0026hellip;\n head() dim() nrow() ncol() names() str() summary()    Solution (click here)  head(exp_data) #\u0026gt; Age Height_cm Eye_Color Graduated #\u0026gt; 1 22 175.26 Blue TRUE #\u0026gt; 2 67 170.18 Green TRUE #\u0026gt; 3 53 165.10 Blue FALSE #\u0026gt; 4 13 134.62 Brown FALSE #\u0026gt; 5 19 147.32 Green TRUE #\u0026gt; 6 27 185.42 Gray TRUE #gives preview of object dim(exp_data) #\u0026gt; [1] 8 4 #returns dimensions (number of row, number of columns) of the object nrow(exp_data) #\u0026gt; [1] 8 #returns number of rows in data frame ncol(exp_data) #\u0026gt; [1] 4 #returns number of columns in data frame names(exp_data) #\u0026gt; [1] \"Age\" \"Height_cm\" \"Eye_Color\" \"Graduated\" #returns vector of column names str(exp_data) #\u0026gt; 'data.frame': 8 obs. of 4 variables: #\u0026gt; $ Age : int 22 67 53 13 19 27 30 11 #\u0026gt; $ Height_cm: num 175 170 165 135 147 ... #\u0026gt; $ Eye_Color: Factor w/ 4 levels \"Blue\",\"Brown\",..: 1 4 1 2 4 3 1 2 #\u0026gt; $ Graduated: logi TRUE TRUE FALSE FALSE TRUE TRUE ... #summarizes the structure of an object summary(exp_data) #\u0026gt; Age Height_cm Eye_Color Graduated  #\u0026gt; Min. :11.00 Min. :134.6 Blue :3 Mode :logical  #\u0026gt; 1st Qu.:17.50 1st Qu.:146.7 Brown:2 FALSE:3  #\u0026gt; Median :24.50 Median :167.6 Gray :1 TRUE :5  #\u0026gt; Mean :30.25 Mean :164.1 Green:2  #\u0026gt; 3rd Qu.:35.75 3rd Qu.:177.8  #\u0026gt; Max. :67.00 Max. :190.5 #provides a summary/summary statistics for individual variables/columns       7 \u0026ndash; R Packages I mentioned above when talking about functions that many have already been written for you, and some are available as soon as you open up R - those that are considered part of \u0026ldquo;base R\u0026rdquo;. All the functions we\u0026rsquo;ve used up to this point are included in that set. But there are lots of other functions available as part of additional packages you can install and load. The two most common places to get packages are the CRAN and Bioconductor repositories - I did a couple short videos on these as part of this Intro To R Playlist.\nAs one example, we used the function read.table() above to read in the example dataset. A similar function, read_tsv() is available as part of the readr package, which is available from CRAN, and so can be installed with the install.packages() function\u0026hellip;\ninstall.packages(\"readr\")   The installation should only have to be done once. Then we use the library() function to load the library in each R session where we want to use it\u0026hellip;\nlibrary(readr) #\u0026gt; Warning: package 'readr' was built under R version 3.6.2   Now we should have access to the readr package and all of the functions contained in it.\nexp_data2 \u0026lt;- read_tsv(data_address) #\u0026gt;  #\u0026gt; ── Column specification ──────────────────────────────────────────────────────── #\u0026gt; cols( #\u0026gt; Age = col_double(), #\u0026gt; Height_cm = col_double(), #\u0026gt; Eye_Color = col_character(), #\u0026gt; Graduated = col_logical() #\u0026gt; ) exp_data2 #\u0026gt; # A tibble: 8 x 4 #\u0026gt; Age Height_cm Eye_Color Graduated #\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;lgl\u0026gt;  #\u0026gt; 1 22 175. Blue TRUE  #\u0026gt; 2 67 170. Green TRUE  #\u0026gt; 3 53 165. Blue FALSE  #\u0026gt; 4 13 135. Brown FALSE  #\u0026gt; 5 19 147. Green TRUE  #\u0026gt; 6 27 185. Gray TRUE  #\u0026gt; 7 30 190. Blue TRUE  #\u0026gt; 8 11 145. Brown FALSE   You might notice that exp_data2 is a tibble, while exp_data is a data frame (try the class() function on each). This small difference in the types of objects that are returned is one of the differences in the functions read.table() and read_tsv(). While the class of the objects is different, the contents of the objects are the same.\nIn addition to functions like install.packages() and library() that help you manage packages in R, RStudio also provides some point-and-click ways to do these same things. Check out the packages tab in the bottom-right RStudio panel.\n Bonus: Breakout Rooms III (~10 min.) Exercise 7: Reading In Compressed Data  Let\u0026rsquo;s look at one more example for a bit more practice with packages and reading data in to R. This time, we\u0026rsquo;ll try reading in a compressed (gzipped) version of the same example dataset. This one\u0026rsquo;s available from\u0026hellip;\nhttps://github.com/biodash/biodash.github.io/raw/master/assets/data/data_frame/example_df.tsv.gz\nLet\u0026rsquo;s read this dataset in as an object named zip_data. First try using the read.table() function just like before.\n  Hint (click here)  Try replacing the previous address with the updated address for the compressed file. Remember to set the header argument to TRUE.\n   Solution (click here)  #create an object storing the web address zip_address \u0026lt;- \"https://github.com/biodash/biodash.github.io/raw/master/assets/data/data_frame/example_df.tsv.gz\"   zip_data \u0026lt;- read.table(zip_address, header = TRUE)    read.table() isn\u0026rsquo;t able to uncompress a file directly from online, so you probably got an error message. However, it can automatically uncompress a file when reading it in locally (from your computer). So, see if you can use the download.file() function to get the compressed file and then read it in in a second step.\n  Hint (click here)  download.file() requires that two arguments are defined: url and destfile. Check ?download.file for details.\n   Solution (click here)  download.file(zip_address, destfile = \"example_zip_file.tsv.gz\") zip_data \u0026lt;- read.table(\"example_zip_file.tsv.gz\", header = TRUE)    Alternatively, you could install the data.table package and try its fread() function, which is able to download and automatically uncompress a file from online all in one step (though doing so requires another package, R.utils, so you\u0026rsquo;ll also have to get that one first if you don\u0026rsquo;t already have it)\u0026hellip;\ninstall.packages(\"R.utils\") install.packages(\"data.table\")   library(R.utils) library(data.table) data.table::fread(zip_address) #\u0026gt; Age Height_cm Eye_Color Graduated #\u0026gt; 1: 22 175.26 Blue TRUE #\u0026gt; 2: 67 170.18 Green TRUE #\u0026gt; 3: 53 165.10 Blue FALSE #\u0026gt; 4: 13 134.62 Brown FALSE #\u0026gt; 5: 19 147.32 Green TRUE #\u0026gt; 6: 27 185.42 Gray TRUE #\u0026gt; 7: 30 190.50 Blue TRUE #\u0026gt; 8: 11 144.78 Brown FALSE     Not only does this offer a little more practice with objects, functions, and reading in data, but it also provides one small example of the value in having multiple functions available that do similar things. In this specific case, when reading in data directly from online, you might find read.table() to be a little easier to work with if the data are uncompressed, but fread() might make things a bit easier (one less step) if the file is compressed. And we actually saw a third function, read_tsv() earlier (from the readr package), which offers yet another option for reading in data-frame-like data. This kind of thing is common in R - there are typically multiple ways of doing things, and as you work in R, you\u0026rsquo;ll continue to pick up more and more efficient ways of doing what you want to do.\n\n","date":1630281600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630419925,"objectID":"05140758ca36d0c2bef7cd0d6b392018","permalink":"https://biodash.github.io/codeclub/s02e02_r-intro_part2/","publishdate":"2021-08-30T00:00:00Z","relpermalink":"/codeclub/s02e02_r-intro_part2/","section":"codeclub","summary":"Part 2 of hitting some of the important basics for working in R. We'll take a look at functions, packages, and pick up from last week with a bit more on R objects.","tags":null,"title":"Code Club S02E02: An introduction to R (Part 2)","type":"codeclub"},{"authors":["Jelmer Poelstra"],"categories":null,"content":"\nLearning objectives   Learn what Code Club is all about Get some basic familiarity with R and RStudio Understand a bit about R objects and how to use them    To do beforehand Before the Code Club Zoom session, please follow the Code Club Computer Setup instructions.\nIn brief, you should have R and RStudio installed on your computer or you should be set up to run RStudio Server at the Ohio Supercomputer Center (OSC). (As a bonus, you can try to install and load the tidyverse package as the setup page suggests, but no sweat if you can\u0026rsquo;t get that to work yet.)\nIn case you run into issues, contact Jelmer or for last-minute troubleshooting, you can join the Zoom call 15-30 minutes early.\n (Re)Introducing Code Club OSU Code Club is a regularly occurring online gathering to improve coding skills, now in its second year.\nThis Code Club was inspired by a paper in PLoS Computational Biology (\u0026ldquo;Ten simple rules to increase computational skills among biologists with Code Clubs\u0026rdquo;), and here are some of the underlying ideas:\n  Coding is best learned by doing, so Code Club is interactive and hands-on.\n  Ongoing exposure and practice also helps when learning.\n  We aim to keep it informal and maybe even fun.\n  We have a core group of 5 organizers that do most of the presenting, but we also encourage participants to present, and will have a couple of participant-led sessions at the end of this semester (see the schedule).\n  Organizers  Jelmer Poelstra (Molecular and Cellular Imaging Center (MCIC), Wooster Campus) Jessica Cooperstone (Dept. of Horticulture and Crop Science \u0026amp; Dept. of Food Science and Technology) Michael Broe (Dept. of Evolution, Ecology and Organismal Biology) Mike Sovic (Center for Applied Plant Sciences) Stephen Opiyo (MCIC, Columbus Campus)  Session structure Each session consists of an instructional part where you can code along or listen, some exercises in breakout rooms with 3-4 people, and exercise recaps with the entire group.\nZoom guidelines   We very much welcome questions at any time, so please either unmute yourself and speak, or post in the chat whenever you have a question!\n  Having your camera on helps, especially in breakout rooms. We will record the whole-group part of each session, so we understand if some of you prefer to have their cameras off during that part. (But note that we will only share the recordings with other Code Clubbers.)\n  You can use the icons under the \u0026ldquo;Participants\u0026rdquo; menu in Zoom when we ask for a \u0026ldquo;show of hands\u0026rdquo; or if you are having problems.\n  In breakout rooms:\n Briefly introduce yourselves. Have someone share their screen, preferably one of the least experienced people. Be friendly and patient, keep everyone aboard. The Zoom Ask for help button will alert us, and one of the organizers will come into the breakout room. (Raise hand is not seen by us outside of the room.)    Otherwise   We need your feedback! Always feel free to email one of the organizers, and for suggestions for a topic to cover in a future Code Club, you can fill out this form!\n  I will quickly show the Code Club menu and BioDASH website during the session.\n  Zoom polling question: are you working at OSC or locally?\n   1 \u0026ndash; Why R? R is a programming language that is most well-known for being excellent for statistical analysis and data visualization.\nWhile the learning curve is steeper than for most programs with graphical user interfaces (GUIs), it pays off to invest in learning R:\n  R gives you greater flexibility to do anything you want.\n  Writing computer instructions as code, like you have to do in R, is more reproducible than clicking around in a GUI. It also makes it much easier to redo analyses with slight modifications!\n  R is highly interdisciplinary and can be used with many different kinds of data. To just name two examples, R has a very strong ecosystem for bioinformatics analysis (\u0026ldquo;Bioconductor\u0026rdquo; project), and can be used to create maps and perform GIS analyses.\n  R is more than a platform to perform analysis and create figures. R Markdown combines R with a simple text markup language to produce analysis reports that integrate code, results, and text, and to create slide decks, data dashboards, websites, and even books! In the third session of Code Club, Michael Broe will talk more about R Markdown.\n  While not as versatile outside of data-focused topics as a language like Python, R can be used as a general programming language, for instance to automate tasks such as large-scale file renaming.\n  Finally, R:\n  Is open-source and freely available for all platforms (Windows, Mac, Linux).\n  Has a large and welcoming user community.\n   2 \u0026ndash; Exploring RStudio R simply provides a \u0026ldquo;console\u0026rdquo; (command-line interface) where you can type your commands.\nHowever, because you want to save your commands in scripts and see the graphics that you produce, it is more effective to work in an environment that provides all of this side-by-side. We will use RStudio, an excellent graphical environment (\u0026ldquo;Integrated Development Environment\u0026rdquo;, IDE) for R.\nI will now demonstrate how to start an RStudio Server session from the Ohio Supercomputer Center\u0026rsquo;s website following the steps from our Code Club Computer Setup page. If you have RStudio installed on your own computer, start it now, and otherwise, follow along with me to run RStudio in your browser.\nOnce you have a running instance of RStudio, create a new R script by clicking File \u0026gt; New File \u0026gt; R Script.\nNow, you should see all 4 \u0026ldquo;panes\u0026rdquo; that the RStudio window is divided into:\n Top-left: The Editor for your scripts and other documents (hidden when no file is open). Bottom-left: The R Console to interactively run your code (+ a tab with a Terminal). Top-right: Your Environment with R objects you have created (+ several other tabs). Bottom-left: Tabs for Files, Plots, Help, and others.  So, in RStudio, we have a single interface to write code in text files or directly in the console, visualize plots, navigate the files found on our computer, and inspect the data we are working with.\nRStudio has a lot of useful features and during the next few sessions of Code Club, we will introduce some tips and tricks for working with it.\n Breakout rooms I (~5 min.)  Introduce yourselves! We\u0026rsquo;ll return to the same breakout room configuration later in this session to do a few exercises, so please take a moment to introduce yourself to your breakout roommates. Make sure to also mention:\n  Your level of experience with R and other coding languages.\n  What you are aiming to use or are already using R for.\n  Check that everyone has RStudio working   Take a moment to explore the RStudio interface.\n  If you run into issues, click the Ask for help button in Zoom and one of us will come by.\n     3 \u0026ndash; Interacting with R R as a calculator The lower-left RStudio pane, i.e. the R console, is where you can interact with R directly.\nThe \u0026gt; sign is the R \u0026ldquo;prompt\u0026rdquo;. It indicates that R is ready for you to type something.\nLet\u0026rsquo;s start by performing a division:\n203 / 2.54 #\u0026gt; [1] 79.92126  R does the calculation and prints the result in the console as well. Afterwards, you get your \u0026gt; prompt back. (The [1] may look a bit weird when there is only one output element; this is how you can keep count of output elements when there are many.)\nWith the expected set of symbols, you can use R as a general calculator:\n203 * 2.54 # Multiplication #\u0026gt; [1] 515.62 203 + 2.54 # Addition #\u0026gt; [1] 205.54  Note that pressing the up arrow key will put your previous command back on the prompt, and you can press the up arrow again to go further back (as well as the down arrow to go in the other direction).\nExperimenting a bit\u0026hellip; What if we add spaces around our values?\n203 - 2.54 #\u0026gt; [1] 200.46  This works: as it turns out, R simply ignores any extra spaces.\nSimilarly, we could omit the single spaces around the mathematical operators that we used earlier (though we will keep using them for clarity):\n203/2.54 #\u0026gt; [1] 79.92126  How about:\n203 /  Now the prompt turned into a + instead of the usual \u0026gt;.\n  What is going on here? (Click for the answer)  R is waiting for you to finish the command, since you typed an incomplete command: something has to follow the division sign /.\nWhile it was obvious here that our command was incomplete, you will often type incomplete commands without realizing you did so. Just remember that when you see the + prompt, something has to be missing in your command: most commonly, you\u0026rsquo;ll have forgotten a closing parenthesis ) or you accidentally opened up an unwanted opening parenthesis (.\nIf you want to abort completing the incomplete command, you can press Esc.\n And if we just type a number:\n203 #\u0026gt; [1] 203  R will print the number back to us! It turns out that the default, implicit action that R will perform on anything you type is to print it back to us (under the hood, it is calling a function called print()).\nInstead of a number, what if we try to have R print some text (a character string) back to us?\nFantastic #\u0026gt; Error in eval(expr, envir, enclos): object 'Fantastic' not found  Code Club #\u0026gt; Error: \u0026lt;text\u0026gt;:1:6: unexpected symbol #\u0026gt; 1: Code Club #\u0026gt; ^    What seems to be going wrong here? (Click for the answer)  Whenever you type a character string, R expects to find an object with that name (we will get to what exactly objects are in a little bit!). When no object exists with that name, R will throw an error. We will learn some of the basics of objects in section 5 of today\u0026rsquo;s session.\n We can get R to print character strings back to us, and work with strings in other ways, as long as we quote them:\n\"Fantastic\" #\u0026gt; [1] \"Fantastic\"   4 \u0026ndash; Working with a script Need for scripts We can go along like this, typing commands directly into the R console. But to keep better track of what we\u0026rsquo;re doing, it\u0026rsquo;s a good idea to write code in plain text files, i.e. to write \u0026ldquo;scripts\u0026rdquo;.\n  You should have already created a script above (otherwise, click File \u0026gt; New File \u0026gt; R Script).\n  Click File \u0026gt; Save As to save the script; give it a descriptive name like intro-to-R.R.\n(You may want to put the script in a new subfolder for this Code Club session.)\n  Interacting with the R console from your script We recommend that you generally type your commands into a script and execute the commands from there, instead of typing directly into the console.\nWe want to make sure to save our division command, so start by typing the following into the R script in the top-left pane:\n203 / 2.54  With the cursor still on this line, press Ctrl + Enter. The command will be copied to the R console and executed, and then the cursor will move to the next line.\nNote that it doesn\u0026rsquo;t matter where on the line your cursor is: Ctrl + Enter will execute the entire line unless you have selected only part of it.\n(And when you have selected multiple lines of code, Ctrl + Enter will execute them all.)\nCommenting You can use # signs to annotate (comment) your code. Anything to the right of a # is ignored by R, meaning it won\u0026rsquo;t be executed. You can use # both at the start of a line or anywhere in a line following code.\nComments are a great way to describe what your code does within the code itself, so comment liberally in your R scripts! This is useful not only for others that you may share your code with, but perhaps especially for yourself when you look back at your code a day, a month, or a year later.\n# Divide by 2.54 to get the wingspan in inches: 203 / 2.54 # Original measurement was in cm   5 \u0026ndash; R Objects Assigning stuff to R objects We can assign any value, character, or set of values or characters to an object with the assignment operator, \u0026lt;-. (This is a smaller-than sign \u0026lt; followed by a dash -.)1\nFor example:\nwingspan_cm \u0026lt;- 203 conversion \u0026lt;- 2.54  Type that into your script, and use Ctrl + Enter to send it to the console.\nThe objects you create get added to your \u0026ldquo;workspace\u0026rdquo; or \u0026ldquo;environment.\u0026rdquo; RStudio shows this in the Environment tab in the topright panel \u0026ndash; check to see if wingspan_cm and conversion are indeed there.\nAfter you\u0026rsquo;ve assigned a number to an object, you can use it in other calculations:\nwingspan_inch \u0026lt;- wingspan_cm / conversion wingspan_inch #\u0026gt; [1] 79.92126  More generally speaking, the object name that you provide is substituted with its contents by R, so the object name is just a reference to the underlying value.\nOur objects so far contained just a single number and we may have also called them variables. Object is the more general name that encompasses R items of any size or complexity. As we see will see next week, R distinguishes between different types of objects.\nObject names Objects can be given any name such as x, current_temperature, or subject_id.\nSome pointers on object names:\n  Because R is case sensitive, wingspan_inch is different from Wingspan_inch.\n  An object name cannot contain a space, so for readability, separate words using:\n _ \u0026ndash; e.g. wingspan_inch (this is called \u0026ldquo;snake case\u0026rdquo;, which we will tend to use in Code Club instructional materials) . \u0026ndash; e.g. wingspan.inch capitalization \u0026ndash; e.g. wingspanInch or WingspanInch (\u0026ldquo;camel case\u0026rdquo;)    Object names can contain but cannot start with a number (2x is not valid, but x2 is)2.\n  Make object names descriptive yet not too long.\n  You will make things easier for yourself by naming objects in a consistent way, for instance by always sticking to your favorite case like \u0026ldquo;snake case.\u0026quot;3\n Objects, your workspace, and closing R When you close R, it will probably ask you whether you want to save your workspace (\u0026ldquo;Save workspace image to ~/.RData\u0026rdquo;). When you do so, then the next time you start R, you can reload everything the way it was, such as your previously created objects.\nWhile this may seem convenient, we recommend that you don\u0026rsquo;t do this.\n  Can you think of a reason why saving and reloading your workspace may not be a good idea? (Click for the answer)  The main reason why this is generally not considered good practice relates to the idea that you should be able to reproduce your workspace (and more broadly speaking, your analysis) from the code in your script.\nRemember that you can modify your workspace either by entering commands in the console directly, or by running them from a script \u0026ndash; or even from multiple different scripts. Also, in practice, you often run lines in the script out of order, or write lines in the script that you don\u0026rsquo;t execute.\nTherefore, if you \u0026ldquo;carry around\u0026rdquo; the same workspace across multiple different sessions, you run a greater risk of not having a reproducible set of steps in your script.\nTo make RStudio stop asking you about saving your workspace, click Tools \u0026gt; Global Options \u0026gt; General and set the options as follows:\nTaking these ideas a step further, it can be a good idea to occasionally restart R so you can check whether the code in your script is correct and complete, that you are not relying on code that is not in the script, and so on. To do so, you don\u0026rsquo;t need to close and reopen RStudio itself: under Session in the top menu bar, you can click Restart R (and you should also see the keyboard shortcut for it in the menu bar, which is Ctrl + Shift + F10 for Windows/Linux, and Cmd + Shift + F10 for Mac).\n    Breakout rooms II (5-10 min.) Note that in both of these exercises, the answers are not contained in what we just discussed. I would like you to think about your intuition for R\u0026rsquo;s behavior, and then see if R indeed works that way or not.\n Exercise 1: Object \u0026ldquo;linkage\u0026rdquo; What do you think the value of y will be after executing the following lines in R? 100 or 160, and why?\nx \u0026lt;- 50 # x is now 50 y \u0026lt;- x * 2 # y is now 100 x \u0026lt;- 80 # x is now 80, but what is y?    Solution (click here)  Objects don\u0026rsquo;t get linked to each other, so if you change one object, it won\u0026rsquo;t affect the values of other objects that were defined earlier.\nTherefore, y will continue to be 100.\n    Exercise 2: Errors\u0026hellip; (Bonus) In section 3, you might have noticed that we got a different error when typing one versus multiple unquoted words. Here are those examples again:\nFantastic #\u0026gt; Error in eval(expr, envir, enclos): object 'Fantastic' not found  Code Club #\u0026gt; Error: \u0026lt;text\u0026gt;:1:6: unexpected symbol #\u0026gt; 1: Code Club #\u0026gt; ^  Reproduce these errors for yourself: in Rstudio\u0026rsquo;s editor pane, type these or equivalent error-generating examples in a script saved with a .R extension, and send them to the console.\nWhy is the error in the second case different, and what does it mean?\n  Hints (click here)    Can you see how RStudio can \u0026ldquo;notice\u0026rdquo; errors already in the editor \u0026ndash; but only for the second of these two examples? The editor checks for syntax (\u0026ldquo;R grammar\u0026rdquo;) errors but not whether objects already exist.\n  If you hover over the red cross in the margin, you can see what RStudio is upset about.\n    What if we put a + or another operator between the two words?\nCode + Club #\u0026gt; Error in eval(expr, envir, enclos): object 'Code' not found       Solution (click here)  When typing a single unquoted word which is not an existing object, R will look for an object and then complain that it can\u0026rsquo;t find that object.\nWhen typing multiple unquoted words with a space between them, regardless of whether those are existing objects, R will notice a syntax (\u0026ldquo;R grammar\u0026rdquo;) error before it even gets around to checking objects.\nThe problem is that you are referring to two objects sequentially and without any mathematical operator in between them, or some other syntax to \u0026ldquo;join\u0026rdquo; them. In R, that\u0026rsquo;s not valid syntax. (You may think it would perhaps simply try to print both objects, but this is not the case.)\n A general lesson here is that you should always pay attention to the details of the error messages that you get. While the language may seem terse and odd at first, it usually holds important clues as to what is going wrong exactly.\n   In closing Where to go from here For a list of recommended resources for learning R, see our R Resources and Tips page page.\nAttribution This was modified after material from The Carpentries, especially from this Data Carpentry workshop and this \u0026ldquo;R for Ecology\u0026rdquo; workshop.\n\n  In RStudio, typing Alt + - will write \u0026lt;- in a single keystroke. You can also use = as assignment, but that symbol can have other meanings, so we recommend sticking with the \u0026lt;- combination. \u0026#x21a9;\u0026#xfe0e;\n There are some names that cannot be used because they are the names of fundamental keywords in R (e.g., if, else, for, see here for a complete list). In general, it\u0026rsquo;s best not to use other function names even if it\u0026rsquo;s \u0026ldquo;allowed\u0026rdquo; (e.g., c, T, mean, data, df, weights). If in doubt, check the Help to see if the name is already in use. \u0026#x21a9;\u0026#xfe0e;\n It is also recommended to use nouns for variable names, and verbs for function names. For more, two popular R style guides are Hadley Wickham\u0026rsquo;s and Google\u0026rsquo;s. \u0026#x21a9;\u0026#xfe0e;\n   ","date":1629158400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630001482,"objectID":"5342f37781dccc93340fc0b810b8f329","permalink":"https://biodash.github.io/codeclub/s02e01_r-intro-part1/","publishdate":"2021-08-17T00:00:00Z","relpermalink":"/codeclub/s02e01_r-intro-part1/","section":"codeclub","summary":"This first installment of Code Club in the Fall '21 semester is the first of two sessions where we will go through the basics of R.","tags":null,"title":"Code Club S02E01:\nAn introduction to R (Part 1)\n","type":"codeclub"},{"authors":null,"categories":null,"content":" Resources to get started with R The basics   Code Club S02E01: Intro to R \u0026ndash; part 1\n  Code Club S02E02: Intro to R \u0026ndash; part 2\n  Mike Sovic\u0026rsquo;s Youtube playlist of short videos on R:\n    Lengthier material   A Tutorial Introduction to R.\n  The Carpentries have a number of great lessons on R, such as:\n  Data Analysis and Visualization in R for Ecologists, which includes data wrangling with tidyverse packages and plotting with ggplot2.\n  R for Reproducible Scientific Analysis\n    If you prefer material structured as a course, excellent free ones include the R Basics and Visualization courses by Rafael Irizarry (you do have to create an EdX account for access).\n  If you prefer a book, we would recommend Wickham \u0026amp; Grolemund\u0026rsquo;s \u0026ldquo;R for Data Science\u0026rdquo;, which is freely available on the web in a really nice format here.\n  Why R? R is a programming language that is most well-known for being excellent for statistical analysis and data visualization.\nWhile the learning curve is steeper than for most programs with graphical user interfaces (GUIs), it pays off to invest in learning R:\n  R gives you greater flexibility to do anything you want.\n  Writing computer instructions as code, like you have to do in R, is more reproducible than clicking around in a GUI. It also makes it much easier to redo analyses with slight modifications!\n  R is highly interdisciplinary and can be used with many different kinds of data. To just name two examples, R has a very strong ecosystem for bioinformatics analysis (\u0026ldquo;Bioconductor\u0026rdquo; project), and can be used to create maps and perform GIS analyses.\n  R is more than a platform to perform analysis and create figures. R Markdown combines R with a simple text markup language to produce analysis reports that integrate code, results, and text, and to create slide decks, data dashboards, websites, and even books!\n  While not as versatile outside of data-focused topics as a language like Python, R can be used as a general programming language, for instance to automate tasks such as large-scale file renaming.\n  Finally, R:\n  Is open-source and freely available for all platforms (Windows, Mac, Linux).\n  Has a large and welcoming user community.\n   Miscellaneous R tips Installing R packages CRAN packages To install an R package that is available at CRAN, the default R package repository, from within R (e.g. in the R console in RStudio), use the install.packages() function.\nThe install.packages() function will handle dependencies within R \u0026ndash; i.e., it will install other R packages that your package depends on. Occasionally, when the install function needs to compile a package from source, errors arise that relate to missing system dependencies (i.e. software outside of R).\nOn Mac and Linux, these system dependencies are best installed outside of R, such as with homebrew on Mac or apt on Ubuntu. The error message you got when trying to install an R package should tell you which system dependencies are needed.\nOn Windows, you can use the installr package to install such dependencies or other software from within R \u0026ndash; for example:\ninstall.packages(\u0026#34;installr\u0026#34;) # Install the installr package first installlr::install.RStudio() # Install RStudio installr::install.python() # Install Python \nSystem setup to installing packages \u0026ldquo;from source\u0026rdquo; Sometimes you need to install a package from source, that is, you need to compile the package rather than simply installing a pre-existing binary. (On Linux, where installing from source is often needed, this should work without additional steps.) On Windows and Mac, installing from source is generally only needed when you install a package from outside of CRAN (such as from Github, see below), but you will need to make sure you have the following non-R software:\nOn Windows, you will need Rtools (Rtools installation instructions).\nOn a Mac, you will need Xcode (which can be installed from the Mac App store).\nYou can test whether or not you are able to install packages from source using the devtools package:\ninstall.packages(\u0026#34;devtools\u0026#34;) # Install the devtools package devtools::has_devel() # Check whether you can install packages from source For a bit more info, see this page.\nInstalling packages from GitHub To install a package from GitHub, use either the devtools or the remotes package \u0026ndash; for example:\ninstall.packages(\u0026#34;remotes\u0026#34;) # Install the remotes package remotes::install_github(\u0026#34;kbroman/broman\u0026#34;) # Install from a repository using \u0026#34;\u0026lt;username\u0026gt;/\u0026lt;repo-name\u0026gt;\u0026#34; This will install the package from source, so you will need to make sure you are able to do so by following the instructions in the section right above this one.\nInstalling packages from Bioconductor If you\u0026rsquo;re doing bioinformatic analyses in R, you will probably run into packages that are not on CRAN but on Bioconductor. To install a package from Bioconductor, use the BiocManager package \u0026ndash; for example:\ninstall.packages(\u0026#34;BiocManager\u0026#34;) # Install the BiocManager package BiocManager::install(\u0026#34;edgeR\u0026#34;) # Install the edgeR package from Bioconductor \n Updating R Consider updating R if you have an older version of R installed \u0026ndash; as of June 2022, the current version is R 4.2 and we would certainly recommend updating R if the version is below R 4.0.\nYou can check which version of R you have by looking at the first line of output when running the following command inside R:\nsessionInfo() To update:   Windows: You can update R from within R. The updateR() function will also take care of updating your packages:\ninstall.packages(\u0026#34;installr\u0026#34;) installr::updateR() If this doesn\u0026rsquo;t work, download the Windows R installer as if you were installing it for the first time.\n  Mac: Download and install the latest .pkg file as if you were installing it for the first time.\n  Linux: In Ubuntu, if you installed R with apt or apt-get, you can use apt-get upgrade in a terminal. Otherwise, download and install the latest version after removing the old one.\n  Re-installing your packages after updating (Mac and Linux) While the installr::updateR() function for Windows users takes care of re-installing your packages along with updating R, Mac and Linux users will have to manually re-install their packages. Some people prefer to re-install these packages on the fly, which can end up being a way to get rid of packages you no longer use.\nBut if you want immediately re-install all your packages, run this before you upgrade:\nmy_packages \u0026lt;- installed.packages() saveRDS(my_packages, \u0026#34;my_packages.rds\u0026#34;) Then, after you\u0026rsquo;ve installed the latest R version:\nmy_packages \u0026lt;- readRDS(\u0026#34;CurrentPackages.rds\u0026#34;) install.packages(my_packages[1, ]) This will only work for packages available on CRAN. Of course, you can check your list for Github-only and Bioconductor packages and then install those with their respective commands (see below). Yes, this can be a bit of a hassle!\n  \n ","date":1629158400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1654879816,"objectID":"1b8835e02d17d12c605c4ae1b9cc35e8","permalink":"https://biodash.github.io/tutorials/r-resources-tips/","publishdate":"2021-08-17T00:00:00Z","relpermalink":"/tutorials/r-resources-tips/","section":"tutorials","summary":"Resources to get started with R The basics   Code Club S02E01: Intro to R \u0026ndash; part 1\n  Code Club S02E02: Intro to R \u0026ndash; part 2","tags":null,"title":"R Resources and Tips","type":"tutorials"},{"authors":["Jessica Cooperstone"],"categories":null,"content":"\n Prep homework Basic computer setup   If you didn\u0026rsquo;t already do this, please follow the Code Club Computer Setup instructions, which also has pointers for if you\u0026rsquo;re new to R or RStudio.\n  If you\u0026rsquo;re able to do so, please open RStudio a bit before Code Club starts \u0026ndash; and in case you run into issues, please join the Zoom call early and we\u0026rsquo;ll help you troubleshoot.\n  You can use R locally, or at OSC. You can find instructions if you are having trouble here.\n   Getting Started RMarkdown for today\u0026rsquo;s session # directory for Code Club Session 20: dir.create(\"S20\") # directory for our RMarkdown # (\"recursive\" to create two levels at once.) dir.create(\"S20/Rmd/\") # save the url location for today's script todays_Rmd \u0026lt;- 'https://raw.githubusercontent.com/biodash/biodash.github.io/master/content/codeclub/20_cleaning-up/CleaningUp.Rmd' # indicate the name of the new script file Session20_Rmd \u0026lt;- \"S20/Rmd/CleaningUp.Rmd\" # go get that file!  download.file(url = todays_Rmd, destfile = Session20_Rmd)   1 - Using regexs for wrangling Artwork by Allison Horst\nNow that we have gone through a mini-series on regular expressions, with the basics, some next level helpers, and using tidytext to make word clouds, I thought I\u0026rsquo;d talk today about some applications of this information to cleaning up your data.\nTo do this, we are going to practice with the palmerpenguins dataset, and get back to the bakeoff for our practice exercises.\n 2 - Accessing our data First load your libraries. We will be using stringr and tidyr but those are both part of core tidyverse. We are also using a new package today called janitor which helps you \u0026ldquo;clean up\u0026rdquo; your data.\nIf you don\u0026rsquo;t have the package janitor, please install it.\ninstall.packages(\"janitor\")  library(tidyverse) library(janitor) # for cleaning up column names library(palmerpenguins) # for penguins data library(bakeoff) # for bakeoff data  Then we will use the package palmerpenguins and the dataset penguins_raw, which has a bit more info than penguins, which we have used previously.\nArtwork by Allison Horst\n 3 - Variable names There are many instances where you may have variables names and/or sample names that are messy. For example, variable names that include characters like white spaces, special characters like symbols, or begin with a number are going to give you problems with some R coding. I\u0026rsquo;ll say that you can have these non-standard variable names, but occasionally they will give you a big headache and so I\u0026rsquo;d recommend to just avoid them.\nR variable \u0026ldquo;rules\u0026rdquo;:\n can contain letters, numbers, underscores (_) and periods (.) cannot start with a number or underscore shouldn\u0026rsquo;t be a \u0026ldquo;reserved\u0026rdquo; word, like if, else, function, TRUE, FALSE etc. (if you want to see them all, execute ?reserved in your console)  You can read about the tidyverse style guide if you want to learn more.\nLets look at the variable names in penguins_raw.\nglimpse(penguins_raw) #\u0026gt; Rows: 344 #\u0026gt; Columns: 17 #\u0026gt; $ studyName \u0026lt;chr\u0026gt; \"PAL0708\", \"PAL0708\", \"PAL0708\", \"PAL0708\", \"PAL… #\u0026gt; $ `Sample Number` \u0026lt;dbl\u0026gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1… #\u0026gt; $ Species \u0026lt;chr\u0026gt; \"Adelie Penguin (Pygoscelis adeliae)\", \"Adelie P… #\u0026gt; $ Region \u0026lt;chr\u0026gt; \"Anvers\", \"Anvers\", \"Anvers\", \"Anvers\", \"Anvers\"… #\u0026gt; $ Island \u0026lt;chr\u0026gt; \"Torgersen\", \"Torgersen\", \"Torgersen\", \"Torgerse… #\u0026gt; $ Stage \u0026lt;chr\u0026gt; \"Adult, 1 Egg Stage\", \"Adult, 1 Egg Stage\", \"Adu… #\u0026gt; $ `Individual ID` \u0026lt;chr\u0026gt; \"N1A1\", \"N1A2\", \"N2A1\", \"N2A2\", \"N3A1\", \"N3A2\", … #\u0026gt; $ `Clutch Completion` \u0026lt;chr\u0026gt; \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"No\", … #\u0026gt; $ `Date Egg` \u0026lt;date\u0026gt; 2007-11-11, 2007-11-11, 2007-11-16, 2007-11-16,… #\u0026gt; $ `Culmen Length (mm)` \u0026lt;dbl\u0026gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34… #\u0026gt; $ `Culmen Depth (mm)` \u0026lt;dbl\u0026gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18… #\u0026gt; $ `Flipper Length (mm)` \u0026lt;dbl\u0026gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190,… #\u0026gt; $ `Body Mass (g)` \u0026lt;dbl\u0026gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 34… #\u0026gt; $ Sex \u0026lt;chr\u0026gt; \"MALE\", \"FEMALE\", \"FEMALE\", NA, \"FEMALE\", \"MALE\"… #\u0026gt; $ `Delta 15 N (o/oo)` \u0026lt;dbl\u0026gt; NA, 8.94956, 8.36821, NA, 8.76651, 8.66496, 9.18… #\u0026gt; $ `Delta 13 C (o/oo)` \u0026lt;dbl\u0026gt; NA, -24.69454, -25.33302, NA, -25.32426, -25.298… #\u0026gt; $ Comments \u0026lt;chr\u0026gt; \"Not enough blood for isotopes.\", NA, NA, \"Adult…  What you can see is that there are variable names here that don\u0026rsquo;t comply with the \u0026ldquo;rules\u0026rdquo; I just indicated. How can that be?! You can see for the variable Sample Number that it is surrounded by backticks. This is how R know that this is a variable name.\nOkay, so who cares? If you want to call that particular variable, you will have to put it in backticks. For example:\n# this doesn't work penguins_raw %% select(Sample Number)  # this works but is clunky penguins_raw %\u0026gt;% select(`Sample Number`) #\u0026gt; # A tibble: 344 × 1 #\u0026gt; `Sample Number` #\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 1 #\u0026gt; 2 2 #\u0026gt; 3 3 #\u0026gt; 4 4 #\u0026gt; 5 5 #\u0026gt; 6 6 #\u0026gt; 7 7 #\u0026gt; 8 8 #\u0026gt; 9 9 #\u0026gt; 10 10 #\u0026gt; # … with 334 more rows  And, this is using tidyverse functions - there will be other situations where you will get non-solvable errors because of your variable names.\ntl;dr just make your variable names R compliant, there are lots of other harder things you\u0026rsquo;re going to be doing with coding, so just make this easier for yourself.\nUsing clean_names() Artwork by Allison Horst\nYou may be thinking now, okay but what happens if someone else gives me data that has unclean variable names?\nDon\u0026rsquo;t worry too much, you can easily fix it. My favorite, and the simplest way to do this is using the package janitor, and the function clean_names(). Certainly you could clean your variable names manually, but why? This is really easy.\npenguins_clean \u0026lt;- clean_names(penguins_raw) glimpse(penguins_clean) #\u0026gt; Rows: 344 #\u0026gt; Columns: 17 #\u0026gt; $ study_name \u0026lt;chr\u0026gt; \"PAL0708\", \"PAL0708\", \"PAL0708\", \"PAL0708\", \"PAL0708… #\u0026gt; $ sample_number \u0026lt;dbl\u0026gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1… #\u0026gt; $ species \u0026lt;chr\u0026gt; \"Adelie Penguin (Pygoscelis adeliae)\", \"Adelie Pengu… #\u0026gt; $ region \u0026lt;chr\u0026gt; \"Anvers\", \"Anvers\", \"Anvers\", \"Anvers\", \"Anvers\", \"A… #\u0026gt; $ island \u0026lt;chr\u0026gt; \"Torgersen\", \"Torgersen\", \"Torgersen\", \"Torgersen\", … #\u0026gt; $ stage \u0026lt;chr\u0026gt; \"Adult, 1 Egg Stage\", \"Adult, 1 Egg Stage\", \"Adult, … #\u0026gt; $ individual_id \u0026lt;chr\u0026gt; \"N1A1\", \"N1A2\", \"N2A1\", \"N2A2\", \"N3A1\", \"N3A2\", \"N4A… #\u0026gt; $ clutch_completion \u0026lt;chr\u0026gt; \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"No\"… #\u0026gt; $ date_egg \u0026lt;date\u0026gt; 2007-11-11, 2007-11-11, 2007-11-16, 2007-11-16, 200… #\u0026gt; $ culmen_length_mm \u0026lt;dbl\u0026gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, … #\u0026gt; $ culmen_depth_mm \u0026lt;dbl\u0026gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, … #\u0026gt; $ flipper_length_mm \u0026lt;dbl\u0026gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186… #\u0026gt; $ body_mass_g \u0026lt;dbl\u0026gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, … #\u0026gt; $ sex \u0026lt;chr\u0026gt; \"MALE\", \"FEMALE\", \"FEMALE\", NA, \"FEMALE\", \"MALE\", \"F… #\u0026gt; $ delta_15_n_o_oo \u0026lt;dbl\u0026gt; NA, 8.94956, 8.36821, NA, 8.76651, 8.66496, 9.18718,… #\u0026gt; $ delta_13_c_o_oo \u0026lt;dbl\u0026gt; NA, -24.69454, -25.33302, NA, -25.32426, -25.29805, … #\u0026gt; $ comments \u0026lt;chr\u0026gt; \"Not enough blood for isotopes.\", NA, NA, \"Adult not…  You can see that Sample Number became sample_number, Culmen Length (mm) became culmen_length_mm.\nThe default is to parse with \u0026ldquo;snake\u0026rdquo; case, which would look like snake_case. You could also set the argument case to:\n \u0026quot;lower_camel\u0026quot; or \u0026quot;small_camel\u0026quot; to get lowerCamel \u0026quot;upper_camel\u0026quot; or \u0026quot;big_camel\u0026quot; to get UpperCamel \u0026quot;screaming_snake\u0026quot; or \u0026quot;all_caps\u0026quot; to get SCREAMING_SNAKE (stop yelling please) \u0026quot;lower_upper\u0026quot; to get lowerUPPER (I don\u0026rsquo;t know why you\u0026rsquo;d want this) \u0026quot;upper_lower\u0026quot; to get UPPERlower (I also don\u0026rsquo;t know why you\u0026rsquo;d want this)  Artwork by Allison Horst\n 4 - Unite character columns There will be times when you\u0026rsquo;d like to take a variable, and combine it with another variable. For example, you might want a column called region_island which contains a combination of the region and island that each penguin is from. We can do this with the function unite(). The function unite() allows you to paste together multiple columns to become one column.\nThe arguments to unite work like this:\nunite(data, col, ..., sep = \u0026quot;_\u0026quot;, remove = TRUE, na.rm = FALSE)\npenguins_clean_unite \u0026lt;- penguins_clean %\u0026gt;% unite(col = \"region_island\", region:island, # indicate the columns to unite remove = FALSE) # don't remove region and island  Did it work?\nhead(penguins_clean_unite) #\u0026gt; # A tibble: 6 × 18 #\u0026gt; study_name sample_number species region_island region island stage  #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt;  #\u0026gt; 1 PAL0708 1 Adelie Penguin… Anvers_Torger… Anvers Torge… Adult, … #\u0026gt; 2 PAL0708 2 Adelie Penguin… Anvers_Torger… Anvers Torge… Adult, … #\u0026gt; 3 PAL0708 3 Adelie Penguin… Anvers_Torger… Anvers Torge… Adult, … #\u0026gt; 4 PAL0708 4 Adelie Penguin… Anvers_Torger… Anvers Torge… Adult, … #\u0026gt; 5 PAL0708 5 Adelie Penguin… Anvers_Torger… Anvers Torge… Adult, … #\u0026gt; 6 PAL0708 6 Adelie Penguin… Anvers_Torger… Anvers Torge… Adult, … #\u0026gt; # … with 11 more variables: individual_id \u0026lt;chr\u0026gt;, clutch_completion \u0026lt;chr\u0026gt;, #\u0026gt; # date_egg \u0026lt;date\u0026gt;, culmen_length_mm \u0026lt;dbl\u0026gt;, culmen_depth_mm \u0026lt;dbl\u0026gt;, #\u0026gt; # flipper_length_mm \u0026lt;dbl\u0026gt;, body_mass_g \u0026lt;dbl\u0026gt;, sex \u0026lt;chr\u0026gt;, #\u0026gt; # delta_15_n_o_oo \u0026lt;dbl\u0026gt;, delta_13_c_o_oo \u0026lt;dbl\u0026gt;, comments \u0026lt;chr\u0026gt;  This is a silly example since there is only one region, but I think you can see how this function is used.\n 5 - Separate character columns There will be times that you have a column that has two variables embedded within it, and you will want to separate or parse the column to become two separate columns. You can do this with the function separate().\nThe arguments to separate look like this:\nseparate(data, col, into, sep = \u0026quot;yourregex\u0026quot;, remove = TRUE, extra = \u0026quot;warn\u0026quot;, fill = \u0026quot;warn\u0026quot;)\nLet\u0026rsquo;s look at the column stage.\npenguins_clean$stage[1:5] #\u0026gt; [1] \"Adult, 1 Egg Stage\" \"Adult, 1 Egg Stage\" \"Adult, 1 Egg Stage\" #\u0026gt; [4] \"Adult, 1 Egg Stage\" \"Adult, 1 Egg Stage\"  We might want to separate the column stage into age and egg_stage. We can do this with separate().\npenguins_clean_stage \u0026lt;- penguins_clean %\u0026gt;% separate(col = stage, into = c(\"age\", \"egg_stage\"), sep = \",\", # the comma is the separator remove = FALSE)   Did it work?\npenguins_clean_stage %\u0026gt;% select(stage, age, egg_stage) %\u0026gt;% head() #\u0026gt; # A tibble: 6 × 3 #\u0026gt; stage age egg_stage  #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt;  #\u0026gt; 1 Adult, 1 Egg Stage Adult \" 1 Egg Stage\" #\u0026gt; 2 Adult, 1 Egg Stage Adult \" 1 Egg Stage\" #\u0026gt; 3 Adult, 1 Egg Stage Adult \" 1 Egg Stage\" #\u0026gt; 4 Adult, 1 Egg Stage Adult \" 1 Egg Stage\" #\u0026gt; 5 Adult, 1 Egg Stage Adult \" 1 Egg Stage\" #\u0026gt; 6 Adult, 1 Egg Stage Adult \" 1 Egg Stage\"   6 - Extract character columns We can use extract() to set up regular expressions to allow the separation of our variable species into a column with the common name, and a column with the genus species.\nWe will use str_view to figure out a regex that will work for us.\n# indicate our string string \u0026lt;- \"Adelie Penguin (Pygoscelis adeliae)\"  # to get Adelie Penguin str_view(string, \"\\\\w+\\\\s\\\\w+\")    \\\\w gives you anything that\u0026rsquo;s a word character the + indicates to match alphanumeric at least 1 time \\\\s indicates a space  # to get Pygoscelis adeliae str_view(string, \"(?\u0026lt;=\\\\()\\\\w+\\\\s\\\\w+\")    (?\u0026lt;=) is called the positive lookbehind, and has this general structure (?\u0026lt;=B)A which can be read like \u0026ldquo;find exprssion A which is preceeded by expression B.\u0026rdquo; In our example, expression B is a parentheses (. But there is some additional complexity here because parentheses have their own meanings in regex so you need to use the \\\\ to escape them. The whole expression for this part of our regex is (?\u0026lt;=\\\\(). \\\\w gives you anything that\u0026rsquo;s a word character the + indicates to match alphanumeric at least 1 time \\\\s indicates a space  Ok our regexs work as desired! Now we can incorporate them into extract(). Here I am using .*? to match the characters between our two targeted regex which here is (.\npenguins_clean_extract \u0026lt;- penguins_clean %\u0026gt;% extract(col = species, into = c(\"common_name\", \"genus_species\"), regex = \"(\\\\w+\\\\s\\\\w+).*?((?\u0026lt;=\\\\()\\\\w+\\\\s\\\\w+)\", remove = FALSE)   penguins_clean_extract %\u0026gt;% select(species, common_name, genus_species) %\u0026gt;% head() #\u0026gt; # A tibble: 6 × 3 #\u0026gt; species common_name genus_species  #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt;  #\u0026gt; 1 Adelie Penguin (Pygoscelis adeliae) Adelie Penguin Pygoscelis adeliae #\u0026gt; 2 Adelie Penguin (Pygoscelis adeliae) Adelie Penguin Pygoscelis adeliae #\u0026gt; 3 Adelie Penguin (Pygoscelis adeliae) Adelie Penguin Pygoscelis adeliae #\u0026gt; 4 Adelie Penguin (Pygoscelis adeliae) Adelie Penguin Pygoscelis adeliae #\u0026gt; 5 Adelie Penguin (Pygoscelis adeliae) Adelie Penguin Pygoscelis adeliae #\u0026gt; 6 Adelie Penguin (Pygoscelis adeliae) Adelie Penguin Pygoscelis adeliae  Voila!\n 7 - Replacing with str_replace() The column individual_id has two parts: the letter N and then a number, and the letter A and then a number. Let\u0026rsquo;s split this column into two columns, one called id_n that contains the number after the N, and a second called id_a that contains the number after the A.\npenguins_clean_fixID \u0026lt;- penguins_clean %\u0026gt;% separate(col = individual_id, into = c(\"id_n\", \"id_a\"), sep = \"A\", # can also use regex \"[A]\" remove = FALSE)   Did it work?\npenguins_clean_fixID %\u0026gt;% select(individual_id, id_n, id_a) %\u0026gt;% head() #\u0026gt; # A tibble: 6 × 3 #\u0026gt; individual_id id_n id_a  #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; #\u0026gt; 1 N1A1 N1 1  #\u0026gt; 2 N1A2 N1 2  #\u0026gt; 3 N2A1 N2 1  #\u0026gt; 4 N2A2 N2 2  #\u0026gt; 5 N3A1 N3 1  #\u0026gt; 6 N3A2 N3 2  This worked to separate out the A, but the N is still linked with id_n. We can use a combination of mutate() and str_replace_all() to remove the N. You can learn more about str_replace() here.\npenguins_clean_fixID \u0026lt;- penguins_clean_fixID %\u0026gt;% mutate(id_n = str_replace_all(id_n, \"N\", \"\"))  penguins_clean_fixID %\u0026gt;% select(individual_id, id_n, id_a) %\u0026gt;% head() #\u0026gt; # A tibble: 6 × 3 #\u0026gt; individual_id id_n id_a  #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; #\u0026gt; 1 N1A1 1 1  #\u0026gt; 2 N1A2 1 2  #\u0026gt; 3 N2A1 2 1  #\u0026gt; 4 N2A2 2 2  #\u0026gt; 5 N3A1 3 1  #\u0026gt; 6 N3A2 3 2   Exercises We will be doing our exercises today with a couple of datasets from the bakeoff package.\n Exercise 1  Using the dataset bakers, combine bakers_last with bakers_first to create a new column bakers_last_first which is indicated like this: Lastname, Firstname.\n  Hints (click here)  Use [`head()`](https://rdrr.io/r/utils/head.html) or `glimpse()` to see the structure of this data. Use `unite()` to combine columns. Don't forget to indicate the correct `sep`    Solutions (click here)  head(bakers) #\u0026gt; # A tibble: 6 × 8 #\u0026gt; series baker_full baker age occupation hometown baker_last baker_first #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt;  #\u0026gt; 1 1 \"Annetha Mi… Annet… 30 Midwife Essex Mills Annetha  #\u0026gt; 2 1 \"David Cham… David 31 Entrepreneur Milton K… Chambers David  #\u0026gt; 3 1 \"Edward \\\"E… Edd 24 Debt collec… Bradford Kimber Edward  #\u0026gt; 4 1 \"Jasminder … Jasmi… 45 Assistant C… Birmingh… Randhawa Jasminder  #\u0026gt; 5 1 \"Jonathan S… Jonat… 25 Research An… St Albans Shepherd Jonathan  #\u0026gt; 6 1 \"Lea Harris\" Lea 51 Retired Midlothi… Harris Lea bakers_2 \u0026lt;- bakers %\u0026gt;% unite(col = \"bakers_last_first\", c(baker_last, baker_first), sep = \", \") # did it work? head(bakers_2) #\u0026gt; # A tibble: 6 × 7 #\u0026gt; series baker_full baker age occupation hometown bakers_last_fir… #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt;  #\u0026gt; 1 1 \"Annetha Mills\" Annetha 30 Midwife Essex Mills, Annetha  #\u0026gt; 2 1 \"David Chambers\" David 31 Entrepren… Milton … Chambers, David  #\u0026gt; 3 1 \"Edward \\\"Edd\\\" Kimber\" Edd 24 Debt coll… Bradford Kimber, Edward  #\u0026gt; 4 1 \"Jasminder Randhawa\" Jasminder 45 Assistant… Birming… Randhawa, Jasmi… #\u0026gt; 5 1 \"Jonathan Shepherd\" Jonathan 25 Research … St Alba… Shepherd, Jonat… #\u0026gt; 6 1 \"Lea Harris\" Lea 51 Retired Midloth… Harris, Lea      Exercise 2  Using the dataset bakers, convert the column hometown to two columns, where whatever comes before the comma is in a column called city and whatever comes after is in a column called locale.\n  Hints (click here)  Try using `separate()`.    Solutions (click here)  head(bakers) #\u0026gt; # A tibble: 6 × 8 #\u0026gt; series baker_full baker age occupation hometown baker_last baker_first #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt;  #\u0026gt; 1 1 \"Annetha Mi… Annet… 30 Midwife Essex Mills Annetha  #\u0026gt; 2 1 \"David Cham… David 31 Entrepreneur Milton K… Chambers David  #\u0026gt; 3 1 \"Edward \\\"E… Edd 24 Debt collec… Bradford Kimber Edward  #\u0026gt; 4 1 \"Jasminder … Jasmi… 45 Assistant C… Birmingh… Randhawa Jasminder  #\u0026gt; 5 1 \"Jonathan S… Jonat… 25 Research An… St Albans Shepherd Jonathan  #\u0026gt; 6 1 \"Lea Harris\" Lea 51 Retired Midlothi… Harris Lea bakers_hometown \u0026lt;- bakers %\u0026gt;% separate(col = hometown, into = c(\"city\", \"locale\"), sep = \", \") #\u0026gt; Warning: Expected 2 pieces. Additional pieces discarded in 1 rows [71]. #\u0026gt; Warning: Expected 2 pieces. Missing pieces filled with `NA` in 65 rows [1, 2, 3, 4, 5, 7, 8, 11, 12, 15, 19, 20, 23, 25, 27, 28, 31, 34, 38, 41, ...]. # did it work? head(bakers_hometown) #\u0026gt; # A tibble: 6 × 9 #\u0026gt; series baker_full baker age occupation city locale baker_last baker_first #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt;  #\u0026gt; 1 1 \"Annetha M… Annet… 30 Midwife Essex NA Mills Annetha  #\u0026gt; 2 1 \"David Cha… David 31 Entrepren… Milt… NA Chambers David  #\u0026gt; 3 1 \"Edward \\\"… Edd 24 Debt coll… Brad… NA Kimber Edward  #\u0026gt; 4 1 \"Jasminder… Jasmi… 45 Assistant… Birm… NA Randhawa Jasminder  #\u0026gt; 5 1 \"Jonathan … Jonat… 25 Research … St A… NA Shepherd Jonathan  #\u0026gt; 6 1 \"Lea Harri… Lea 51 Retired Midl… Scotl… Harris Lea      Exercise 3  Using the dataset bakers add a column nickname which indicates the bakers nickname, if they have one.\n  Hints (click here)  Think about how to make a regex that would pull out the nickname. Try using `str_view_all()` to get your regex working before you apply it to `bakers`. Try using the lookahead syntax.    Solutions (click here)  baker_full \u0026lt;- bakers$baker_full  # note I used single quotes because there were double quotes in the regex str_view_all(baker_full, '(?\u0026lt;=\\\\\").*(?=\\\\\")')    bakers_nickname \u0026lt;- bakers %\u0026gt;% extract(col = baker_full, into = \"nickname\", regex = '((?\u0026lt;=\\\\\")\\\\w+(?=\\\\\"))') bakers_nickname %\u0026gt;% arrange(nickname) %\u0026gt;% head() #\u0026gt; # A tibble: 6 × 8 #\u0026gt; series nickname baker age occupation hometown baker_last baker_first #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt;  #\u0026gt; 1 1 Edd Edd 24 Debt collector… Bradford Kimber Edward  #\u0026gt; 2 2 Jo Joanne 41 Housewife Ongar, E… Wheatley Joanne  #\u0026gt; 3 7 Val Val 66 Semi-retired, … Yeovil Stones Valerie  #\u0026gt; 4 8 Yan Yan 46 Laboratory res… North Lo… Tsou Chuen-Yan  #\u0026gt; 5 1 NA Annetha 30 Midwife Essex Mills Annetha  #\u0026gt; 6 1 NA David 31 Entrepreneur Milton K… Chambers David      Exercise 4  Using the dataset challenge_results, write a regex to find any signature that contains chocolate. Remove all observations that contain NA for the signature. How many of the signature bakes contain chocolate? What percentage of the total signature bakes (for which we have bake names) does this represent?\n  Hints (click here)  You can get rid of NAs with `drop_na()`. Try using `str_count()` to see how many occurances you have of chocolate in the signatures.    Solutions (click here)  # select only signatures, drop NAs signatures \u0026lt;- challenge_results %\u0026gt;% select(signature) %\u0026gt;% drop_na() # check dimensions  dim(signatures) #\u0026gt; [1] 703 1 # regex for chocolate (or Chocolate, or Chocolatey) str_count(signatures, \"[Cc]hocolat[ey]\") #\u0026gt; Warning in stri_count_regex(string, pattern, opts_regex = opts(pattern)): argument is not an atomic vector; coercing #\u0026gt; [1] 75 # what percent of signatures contain chocolate (str_count(signatures, \"[Cc]hocolat[ey]\")/count(signatures))*100 #\u0026gt; Warning in stri_count_regex(string, pattern, opts_regex = opts(pattern)): argument is not an atomic vector; coercing #\u0026gt; n #\u0026gt; 1 10.66856      ","date":1619740800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1628697543,"objectID":"15be695bc3bed9ca7458afe8a992c490","permalink":"https://biodash.github.io/codeclub/20_cleaning-up/","publishdate":"2021-04-30T00:00:00Z","relpermalink":"/codeclub/20_cleaning-up/","section":"codeclub","summary":"During this  session of Code Club, we will be learning to clean up variable names, combine and separate columns, and extract data with regular expressions.","tags":null,"title":"Session 20: Cleaning up variables names, and other wrangling","type":"codeclub"},{"authors":["Michael Broe"],"categories":null,"content":"\n New To Code Club?   First, check out the Code Club Computer Setup instructions, which also has some pointers that might be helpful if you\u0026rsquo;re new to R or RStudio.\n  Please open RStudio before Code Club to test things out \u0026ndash; if you run into issues, join the Zoom call early and we\u0026rsquo;ll troubleshoot.\n   Session Goals  Learn the fundamentals of text mining. Learn how to do text mining in a tidyverse setting. Reuse some of our dplyr and ggplot skills on text. Learn how to very simply create word cloud visualizations.   Setup This is another in our current series on text processing. We\u0026rsquo;ll be using the following previously used packages which you should load first (install them if you haven\u0026rsquo;t already):\nlibrary(tidyverse) library(bakeoff)   We\u0026rsquo;ll also be using the following packages, which you should install and load:\n# Uncomment the following line to install: # install.packages(c(\"tidytext\", \"gutenbergr\", \"wordcloud\")) library(tidytext) library(gutenbergr) library(wordcloud)   Introduction In this CodeClub session we\u0026rsquo;ll see how to create word clouds (also known as tag clouds) from text, using the tidytext and wordcloud packages. A word cloud is a visualization of word frequencies, graphically highlighting the most common words.\nWe need to get some text from somewhere, so first let\u0026rsquo;s do it in the simplest possible way. Here we manually enter a quote, line by line, as a vector of five character strings. This is the first stanza from Robert Lowell\u0026rsquo;s Skunk Hour:\nlowell \u0026lt;- c(\"Nautilus Island's hermit\", \"heiress still lives through winter in her Spartan cottage;\", \"her sheep still graze above the sea.\", \"Her son's a bishop. Her farmer is first selectman in our village;\", \"she's in her dotage.\")   In textual analysis we distinguish between word types, and word tokens (multiple instances of those words in text). For example there are two tokens of the word-type \u0026ldquo;still\u0026rdquo; in this stanza:\n heiress still lives through winter\nher sheep still graze above the sea\n And slightly more abstractly there are four tokens of \u0026ldquo;her\u0026rdquo;, modulo capitalization:\n her Spartan cottage\nher sheep still graze\nHer son\u0026rsquo;s a bishop.\nHer farmer\n Formally, it\u0026rsquo;s the token frequency of the word types we are ultimately interested in capturing. So: two tasks, extract the word tokens, and count them! Done!\nThe reason this is tricky is that natural language text is messy: the task of extracting a clean set of tokens to count is termed text mining or tokenization. We would also like to get the output into a tidyverse compliant data frame, so we can use familiar dplyr and ggplot functions to analyze it.\nWe could imagine attacking this using stingr functions:\nlowell_tokens \u0026lt;- lowell %\u0026gt;% # convert upper to lower case; returns a character vector. str_to_lower() %\u0026gt;% # remove punctuation with a character class; returns a list. str_extract_all(\"[a-z]+\") %\u0026gt;% # flatten that list unlist() %\u0026gt;% # stick it in a data frame as_tibble() print(lowell_tokens, n = 38) #\u0026gt; # A tibble: 38 x 1 #\u0026gt; value  #\u0026gt; \u0026lt;chr\u0026gt;  #\u0026gt; 1 nautilus  #\u0026gt; 2 island  #\u0026gt; 3 s  #\u0026gt; 4 hermit  #\u0026gt; 5 heiress  #\u0026gt; 6 still  #\u0026gt; 7 lives  #\u0026gt; 8 through  #\u0026gt; 9 winter  #\u0026gt; 10 in  #\u0026gt; 11 her  #\u0026gt; 12 spartan  #\u0026gt; 13 cottage  #\u0026gt; 14 her  #\u0026gt; 15 sheep  #\u0026gt; 16 still  #\u0026gt; 17 graze  #\u0026gt; 18 above  #\u0026gt; 19 the  #\u0026gt; 20 sea  #\u0026gt; 21 her  #\u0026gt; 22 son  #\u0026gt; 23 s  #\u0026gt; 24 a  #\u0026gt; 25 bishop  #\u0026gt; 26 her  #\u0026gt; 27 farmer  #\u0026gt; 28 is  #\u0026gt; 29 first  #\u0026gt; 30 selectman #\u0026gt; 31 in  #\u0026gt; 32 our  #\u0026gt; 33 village  #\u0026gt; 34 she  #\u0026gt; 35 s  #\u0026gt; 36 in  #\u0026gt; 37 her  #\u0026gt; 38 dotage   This is a good start: it gets rid of the capitalization issue, and also gets rid of the punctuation. But there\u0026rsquo;s a problem. The regular expression pattern [a-z]+ doesn\u0026rsquo;t recognize possessives or contractions: it just strips anything that\u0026rsquo;s not a letter, so it messes up with Island's, son's, and she's: welcome to the subtleties of processing natural language text algorithmically! Exceptions, exceptions!!\nWe could fiddle about with our regex, but\u0026hellip; there\u0026rsquo;s a package for that! This kind of text mining is exactly what the tidytext package was built for. It will simultaneously strip punctuation intelligently and \u0026lsquo;unnest\u0026rsquo; lines into word tokens.\nTidytext functions need a dataframe to operate on. So first we need to get the poem into a data frame; here we\u0026rsquo;ll use the column name text.\nlowell_df \u0026lt;- tibble(text = lowell) lowell_df #\u0026gt; # A tibble: 5 x 1 #\u0026gt; text  #\u0026gt; \u0026lt;chr\u0026gt;  #\u0026gt; 1 Nautilus Island's hermit  #\u0026gt; 2 heiress still lives through winter in her Spartan cottage;  #\u0026gt; 3 her sheep still graze above the sea.  #\u0026gt; 4 Her son's a bishop. Her farmer is first selectman in our village; #\u0026gt; 5 she's in her dotage.   Each string in the character vector becomes a single row in the data frame.\nAgain we want one word-token per row, to \u0026lsquo;tidy\u0026rsquo; our data. This is what tidytext::unnest_tokens() does. We\u0026rsquo;re going to unnest words in this case (we can unnest other things, like characters, sentences, regexes, even tweets) and we need to specify the variable in the dataframe we are unnesting (in this case just text). This will create a new word-token data frame, and we\u0026rsquo;ll name the variable in the data frame word. This is important (see later on stop words).\nlowell_tidy \u0026lt;- lowell_df %\u0026gt;% unnest_tokens(word, text) print(lowell_tidy, n = 35) #\u0026gt; # A tibble: 35 x 1 #\u0026gt; word  #\u0026gt; \u0026lt;chr\u0026gt;  #\u0026gt; 1 nautilus  #\u0026gt; 2 island's  #\u0026gt; 3 hermit  #\u0026gt; 4 heiress  #\u0026gt; 5 still  #\u0026gt; 6 lives  #\u0026gt; 7 through  #\u0026gt; 8 winter  #\u0026gt; 9 in  #\u0026gt; 10 her  #\u0026gt; 11 spartan  #\u0026gt; 12 cottage  #\u0026gt; 13 her  #\u0026gt; 14 sheep  #\u0026gt; 15 still  #\u0026gt; 16 graze  #\u0026gt; 17 above  #\u0026gt; 18 the  #\u0026gt; 19 sea  #\u0026gt; 20 her  #\u0026gt; 21 son's  #\u0026gt; 22 a  #\u0026gt; 23 bishop  #\u0026gt; 24 her  #\u0026gt; 25 farmer  #\u0026gt; 26 is  #\u0026gt; 27 first  #\u0026gt; 28 selectman #\u0026gt; 29 in  #\u0026gt; 30 our  #\u0026gt; 31 village  #\u0026gt; 32 she's  #\u0026gt; 33 in  #\u0026gt; 34 her  #\u0026gt; 35 dotage   Punctuation has been stripped and all words are lower case, but possessives and contractions are preserved (fancy usage of str_ regular expression functions under the hood\u0026hellip;).\nBakeoff! Now that we have the basic idea, let\u0026rsquo;s look at a more interesting data set, from the bakeoff package.\nFirst we\u0026rsquo;ll create a data frame with just the signature column from the bakes data set:\nsignature_df \u0026lt;- select(bakes, signature) signature_df #\u0026gt; # A tibble: 548 x 1 #\u0026gt; signature  #\u0026gt; \u0026lt;chr\u0026gt;  #\u0026gt; 1 \"Light Jamaican Black Cakewith Strawberries and Cream\"  #\u0026gt; 2 \"Chocolate Orange Cake\"  #\u0026gt; 3 \"Caramel Cinnamon and Banana Cake\"  #\u0026gt; 4 \"Fresh Mango and Passion Fruit Hummingbird Cake\"  #\u0026gt; 5 \"Carrot Cake with Lime and Cream Cheese Icing\"  #\u0026gt; 6 \"Cranberry and Pistachio Cakewith Orange Flower Water Icing\"  #\u0026gt; 7 \"Carrot and Orange Cake\"  #\u0026gt; 8 \"Sticky Marmalade Tea Loaf\"  #\u0026gt; 9 \"Triple Layered Brownie Meringue Cake\\nwith Raspberry Cream\"  #\u0026gt; 10 \"Three Tiered Lemon Drizzle Cakewith Fresh Cream and freshly made Lemon Curd\" #\u0026gt; # … with 538 more rows   Next we tokenize by word on the signature column:\nsignature_tidy \u0026lt;- signature_df %\u0026gt;% unnest_tokens(word, signature) signature_tidy #\u0026gt; # A tibble: 2,762 x 1 #\u0026gt; word  #\u0026gt; \u0026lt;chr\u0026gt;  #\u0026gt; 1 light  #\u0026gt; 2 jamaican  #\u0026gt; 3 black  #\u0026gt; 4 cakewith  #\u0026gt; 5 strawberries #\u0026gt; 6 and  #\u0026gt; 7 cream  #\u0026gt; 8 chocolate  #\u0026gt; 9 orange  #\u0026gt; 10 cake  #\u0026gt; # … with 2,752 more rows   Now we want to count those tokens: i.e. we want to collapse all duplicate word tokens into a single word type, with the corresponding frequency. Since we now have tidy data, dplyr to the rescue!\n dplyr count() lets you quickly count the unique values of one or more variables. The option sort, if TRUE, will show the largest groups at the top.\n signature_count \u0026lt;- signature_tidy %\u0026gt;% count(word, sort = TRUE) signature_count #\u0026gt; # A tibble: 806 x 2 #\u0026gt; word n #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 and 321 #\u0026gt; 2 cake 66 #\u0026gt; 3 chocolate 61 #\u0026gt; 4 orange 42 #\u0026gt; 5 with 42 #\u0026gt; 6 pie 37 #\u0026gt; 7 apple 34 #\u0026gt; 8 ginger 30 #\u0026gt; 9 lemon 29 #\u0026gt; 10 biscuits 26 #\u0026gt; # … with 796 more rows   We\u0026rsquo;re way more interested in cake than and: this is an example of a stop word:\n In computing, stop words are words which are filtered out before or after processing of natural language data (text). \u0026ldquo;stop words\u0026rdquo; usually refers to the most common words in a language.\n  One of our major performance (search) optimizations\u0026hellip; is removing the top 10,000 most common English dictionary words (as determined by Google search). It\u0026rsquo;s shocking how little is left of most posts once you remove the top 10k English dictionary words\u0026hellip;\n The tidytext package has a database of just over a thousand of these words, including \u0026lsquo;and\u0026rsquo;:\nprint(stop_words, n = 30) #\u0026gt; # A tibble: 1,149 x 2 #\u0026gt; word lexicon #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt;  #\u0026gt; 1 a SMART  #\u0026gt; 2 a's SMART  #\u0026gt; 3 able SMART  #\u0026gt; 4 about SMART  #\u0026gt; 5 above SMART  #\u0026gt; 6 according SMART  #\u0026gt; 7 accordingly SMART  #\u0026gt; 8 across SMART  #\u0026gt; 9 actually SMART  #\u0026gt; 10 after SMART  #\u0026gt; 11 afterwards SMART  #\u0026gt; 12 again SMART  #\u0026gt; 13 against SMART  #\u0026gt; 14 ain't SMART  #\u0026gt; 15 all SMART  #\u0026gt; 16 allow SMART  #\u0026gt; 17 allows SMART  #\u0026gt; 18 almost SMART  #\u0026gt; 19 alone SMART  #\u0026gt; 20 along SMART  #\u0026gt; 21 already SMART  #\u0026gt; 22 also SMART  #\u0026gt; 23 although SMART  #\u0026gt; 24 always SMART  #\u0026gt; 25 am SMART  #\u0026gt; 26 among SMART  #\u0026gt; 27 amongst SMART  #\u0026gt; 28 an SMART  #\u0026gt; 29 and SMART  #\u0026gt; 30 another SMART  #\u0026gt; # … with 1,119 more rows   Note that the name of the stop word column is word, and the name we used in our tokenized column is word (now you will see why we used that name) so we can use dplyr\u0026rsquo;s anti_join() to filter the word tokens!\n anti_join() returns all rows from x without a match in y (where x are the word tokens, and y are the stop words)\n signature_count \u0026lt;- signature_tidy %\u0026gt;% count(word, sort = TRUE) %\u0026gt;% anti_join(stop_words) #\u0026gt; Joining, by = \"word\" signature_count #\u0026gt; # A tibble: 762 x 2 #\u0026gt; word n #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 cake 66 #\u0026gt; 2 chocolate 61 #\u0026gt; 3 orange 42 #\u0026gt; 4 pie 37 #\u0026gt; 5 apple 34 #\u0026gt; 6 ginger 30 #\u0026gt; 7 lemon 29 #\u0026gt; 8 biscuits 26 #\u0026gt; 9 loaf 22 #\u0026gt; 10 walnut 22 #\u0026gt; # … with 752 more rows   Since we are in the tidyverse, we can pipe our results into ggplot. First we filter on counts above a certain threshold (here 12, just for visualization purposes):\nsignature_count %\u0026gt;% filter(n \u0026gt; 12) %\u0026gt;% ggplot(aes(n, word)) + geom_col() + theme_minimal() + labs(y = NULL)   This is ordered alphabetically by default, bottom to top; but we can reorder by count (n) using dplyr mutate():\nsignature_count %\u0026gt;% filter(n \u0026gt; 12) %\u0026gt;% mutate(word = reorder(word, n)) %\u0026gt;% ggplot(aes(n, word)) + geom_col() + theme_minimal() + labs(y = NULL)   We now have everything we need for a word cloud: word types and their token frequencies:\nThe only obligatory arguments to wordcloud() are the first two: the rest just let you tweak the graphic:\nwordcloud(words = signature_count$word, freq = signature_count$n, min.freq = 12, random.order=FALSE, rot.per=0.3, colors=brewer.pal(8, \"Dark2\"))   min.freq lets you filter on a frequency threshold. random.order=FALSE plots words in decreasing frequency (highest most central); rot.per is the proportion of words with 90 degree rotation; colors=brewer.pal(8, \u0026quot;Dark2\u0026quot;) lets you choose an RColorBrewer color palette of your choice.\n Lemmatization\nIf you create a count data frame of signature_tidy without the sort = TRUE option, the words are sorted alphabetically. And if you look through that table you will see many instances such as apple, apples; apricot, apricots; cake, cakes etc. Arguably, these are the same word type (think \u0026ldquo;dictionary word\u0026rdquo;) just grammatical variations. Properly collapsing these into a single type is called lemmatization: a very difficult problem which would take us far afield into the morphology of words. Again in general there are many exceptions, only partly due to English borrowing so many words from other languages: besides apple, apples there is mouse, mice; self, selves; bacillus, bacilli; basis, bases. etc. These are known as irregular plurals.\nVerbs are worse! Perhaps you would also consider the inflectional forms run, runs, ran, running as the same type, just as a dictionary does. How do you reduce those algorithmically? And if you consider inflectional forms as the same dictionary word, how would you tackle Ancient Greek, which has hundreds of inflected forms for the same verb? Here are just a few, there are pages and pages of them\u0026hellip;\nCurrently machine learning has been unleashed on this problem, with limited success. The traditional computational linguists' algorithms are still winning\u0026hellip;\n  The gutenbergr package Say we wanted to do a word cloud for a more substantive text like Darwin\u0026rsquo;s Origin of Species.\nProject Gutenberg is a volunteer effort to digitize and archive cultural works and is the oldest digital library. It has over 60,000 books in the public domain (including Darwin\u0026rsquo;s works).\nThe gutenbergr package allows you to download any of these works directly into a data frame using just the Project Gutenberg ID. This is then perfect input for tidytext. The package provides all the metadata to search for author and work IDs inside R (you can also just find the ID by searching on the Project Gutenberg website):\ndarwins_works \u0026lt;- gutenberg_metadata %\u0026gt;% filter(author == \"Darwin, Charles\") darwins_works #\u0026gt; # A tibble: 40 x 8 #\u0026gt; gutenberg_id title author gutenberg_autho… language gutenberg_books… rights #\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt;  #\u0026gt; 1 944 \"The… Darwi… 485 en Travel/Harvard … Publi… #\u0026gt; 2 1227 \"The… Darwi… 485 en NA Publi… #\u0026gt; 3 1228 \"On … Darwi… 485 en Harvard Classic… Publi… #\u0026gt; 4 2009 \"The… Darwi… 485 en Harvard Classic… Publi… #\u0026gt; 5 2010 \"The… Darwi… 485 en NA Publi… #\u0026gt; 6 2087 \"Lif… Darwi… 485 en NA Publi… #\u0026gt; 7 2088 \"Lif… Darwi… 485 en NA Publi… #\u0026gt; 8 2300 \"The… Darwi… 485 en NA Publi… #\u0026gt; 9 2355 \"The… Darwi… 485 en NA Publi… #\u0026gt; 10 2485 \"The… Darwi… 485 en Botany Publi… #\u0026gt; # … with 30 more rows, and 1 more variable: has_text \u0026lt;lgl\u0026gt;   An inspection of the results of Origin of Species on the website reveals that the latest edition is ID 2009. Let\u0026rsquo;s grab it:\nOoS \u0026lt;- gutenberg_download(2009) #\u0026gt; Determining mirror for Project Gutenberg from http://www.gutenberg.org/robot/harvest #\u0026gt; Using mirror http://aleph.gutenberg.org   In the breakout rooms, we\u0026rsquo;ll work through inspecting the frequencies and creating a word cloud for this text.\nThe gutenbergr package is extremely useful, but as long as you can read a document into R, you can then convert it to a data frame as we did in the very first example above, and then the tidytext pipeline will work. The readtext package can import a variety of formats, including PDFs and Microsoft Word documents.\nBreakout rooms Exercise 1 Run the command:\nOoS \u0026lt;- gutenberg_download(2009) and inspect the data frame. Identify the name of the column you want to tokenize.\nThen use the unnest_tokens() command to create a data frame of word tokens.\n  Hints (click here)  It's the text column you want. gutenbergr includes the gutenberg_ID in case you download multiple texts into the same data frame. Remember to name the column in the new data frame word so we can filter any stop words later on.    Solution (click here)  OoS \u0026lt;- gutenberg_download(2009) OoS #\u0026gt; # A tibble: 21,556 x 2 #\u0026gt; gutenberg_id text  #\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt;  #\u0026gt; 1 2009 \"1228 1859, First Edition\"  #\u0026gt; 2 2009 \"22764 1860, Second Edition\"  #\u0026gt; 3 2009 \"2009 1872, Sixth Edition, considered the definitive edition… #\u0026gt; 4 2009 \"\"  #\u0026gt; 5 2009 \"\"  #\u0026gt; 6 2009 \"\"  #\u0026gt; 7 2009 \"\"  #\u0026gt; 8 2009 \"On the Origin of Species\"  #\u0026gt; 9 2009 \"\"  #\u0026gt; 10 2009 \"BY MEANS OF NATURAL SELECTION,\"  #\u0026gt; # … with 21,546 more rows   OoS_tidy \u0026lt;- OoS %\u0026gt;% unnest_tokens(word, text) OoS_tidy #\u0026gt; # A tibble: 209,048 x 2 #\u0026gt; gutenberg_id word  #\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt;  #\u0026gt; 1 2009 1228  #\u0026gt; 2 2009 1859  #\u0026gt; 3 2009 first  #\u0026gt; 4 2009 edition #\u0026gt; 5 2009 22764  #\u0026gt; 6 2009 1860  #\u0026gt; 7 2009 second  #\u0026gt; 8 2009 edition #\u0026gt; 9 2009 2009  #\u0026gt; 10 2009 1872  #\u0026gt; # … with 209,038 more rows      Exercise 2 Count and sort the tokens into a new data frame. Inspect the output. Are there any stop words?\n  Hints (click here)  Pipe the word column of the data frame into the dplyr count() function with the sort = TRUE option.    Solution (click here)  OoS_count \u0026lt;- OoS_tidy %\u0026gt;% count(word, sort = TRUE) OoS_count #\u0026gt; # A tibble: 9,233 x 2 #\u0026gt; word n #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 the 14570 #\u0026gt; 2 of 10438 #\u0026gt; 3 and 5853 #\u0026gt; 4 in 5414 #\u0026gt; 5 to 4753 #\u0026gt; 6 a 3368 #\u0026gt; 7 that 2749 #\u0026gt; 8 as 2230 #\u0026gt; 9 have 2114 #\u0026gt; 10 be 2099 #\u0026gt; # … with 9,223 more rows      Exercise 3 Remove the stop words from the output and inspect the results.\n  Hints (click here)  Use antijoin() with the tidytext stop_words data frame:    Solution (click here)  OoS_count \u0026lt;- OoS_tidy %\u0026gt;% count(word, sort = TRUE) %\u0026gt;% anti_join(stop_words) #\u0026gt; Joining, by = \"word\" OoS_count #\u0026gt; # A tibble: 8,678 x 2 #\u0026gt; word n #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 species 1921 #\u0026gt; 2 forms 565 #\u0026gt; 3 selection 561 #\u0026gt; 4 natural 535 #\u0026gt; 5 varieties 486 #\u0026gt; 6 plants 471 #\u0026gt; 7 animals 436 #\u0026gt; 8 distinct 357 #\u0026gt; 9 life 350 #\u0026gt; 10 nature 325 #\u0026gt; # … with 8,668 more rows      Exercise 4 Visualize the counts using ggplot(), from highest frequency to lowest, using a frequency cutoff of 200. Does any one word stand out in any way?\nDoes the tidytext package perform lemmatization? Are there any irregular plurals in this result?\n  Hints (click here)  Use a dplyr filter() command on the n column, and, well just look at the examples in the presentation for the details of piping it into ggplot()!    Solution (click here)  OoS_count %\u0026gt;% filter(n \u0026gt; 200) %\u0026gt;% mutate(word = reorder(word, n)) %\u0026gt;% ggplot(aes(n, word)) + geom_col() + theme_minimal() + labs(y = NULL)   tidytext does not lemmatize. There are many plurals in this list, so undoubtedly there are corresponding singulars of lower frequency. Indeed we see both forms and form. And of course the irregular genera is the plural of genus.\n   Exercise 5 Create a word cloud of this data frame, with the same frequency cut off as the ggplot() (200). Otherwise use the same settings as in the presentation. Tweak those settings, especially the frequency threshold and rotation proportion. See what happens when you set random.order=TRUE.\n  Hints (click here)  The option for the the frequency threshold is min.freq = 200.    Solution (click here)  wordcloud(words = OoS_count$word, freq = OoS_count$n, min.freq = 200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, \"Dark2\"))      ","date":1618963200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1626742536,"objectID":"3d438db44d6a4c848a548ed0236c3266","permalink":"https://biodash.github.io/codeclub/19_wordclouds/","publishdate":"2021-04-21T00:00:00Z","relpermalink":"/codeclub/19_wordclouds/","section":"codeclub","summary":"Visualizing text frequency distributions","tags":null,"title":"Session 19: Word Clouds via Tidytext","type":"codeclub"},{"authors":["Mike Sovic"],"categories":null,"content":"\n New to Code Club?   If you didn\u0026rsquo;t already do this, please follow the Code Club Computer Setup instructions, which also has pointers for if you\u0026rsquo;re new to R or RStudio.\n  If you\u0026rsquo;re able to do so, please open RStudio a bit before Code Club starts. If you run into issues, please join the Zoom call early and we\u0026rsquo;ll help you troubleshoot.\n   Getting Set Up Like last week, we\u0026rsquo;re going to be working with functions from the stringr package, which is one of the core tidyverse packages, so let\u0026rsquo;s get that loaded first\u0026hellip;\n## If needed, install the tidyverse: # install.packages(\"tidyverse\") ## Load the tidyverse -- this will include loading \"stringr\".  library(tidyverse)   stringr has lots of useful functions for working with strings. Our focus here is on regular expressions, though, which represent just one component of working with strings. So, we\u0026rsquo;ll limit the number of stringr functions we work with to try to focus more on the regular expressions themselves. In the previous session, we worked primarily with the str_view_all() function. We\u0026rsquo;ll see a lot of that one again this week, and maybe a couple more as needed. Maybe we\u0026rsquo;ll do a stringr-focused session down the road to see more of what it offers, but in the meantime, you can check out the stringr cheatsheet here, which also includes some useful info on regular expressions, if you\u0026rsquo;re interested.\nIntro In case you missed last week, or could use a refresher, here\u0026rsquo;s a quick summary\u0026hellip;\n Regular expressions allow you to match specific patterns in text. These can be simple patterns, like matching \u0026lsquo;choco\u0026rsquo; in \u0026lsquo;chocolate\u0026rsquo;, or more complicated patterns, like matching only lines that end with any three or four digits. More complicated matches are possible because there are characters, or in some cases, combinations of characters, that have special meaning. These include the metacharacters, anchors, and quantifiers we saw last week, and they help us define more complicated patterns in text. Sometimes you want to turn the \u0026ldquo;special meaning\u0026rdquo; of things like metacharacters, anchors, and quantifiers off to instead interpret the characters literally. The backslash helps us in those cases, and in R, you typically need two of them.  Session Goals If regular expressions were new to you last week, it might have felt like a lot to digest. And this week we\u0026rsquo;re just going to add more. But don\u0026rsquo;t worry \u0026ndash; the idea really isn\u0026rsquo;t that you remember all of the special characters (metacharacters) and rules for matching patterns. Personally, I use regular expressions fairly often, and still don\u0026rsquo;t remember all of them all the time \u0026ndash; it\u0026rsquo;s easy enough to grab a cheat sheet and look up what you need to know. The more important thing for now is to just get a sense of the types of things regular expressions allow you to do. With that in mind, this week we\u0026rsquo;ll consider the following additional features/uses of regular expressions\u0026hellip;\n Character classes/Bracket expressions Alternation Grouping Backreferences Greediness/Non-Greediness  As we go through these, we\u0026rsquo;ll use the following string that contains made-up counts of 100 peoples' favorite ice cream flavor to see some examples of how they work\u0026hellip;\nour_string \u0026lt;- \"Chocolate-48,Vanilla-27,Strawberry-25\"   Character Classes/Bracket Expressions Regular expressions allow you to match certain classes of characters \u0026ndash; uppercase, lowercase, digits, alphanumeric, punctuation, etc. We actually saw some examples of character classes last week \u0026ndash; we just didn\u0026rsquo;t call them that. For example, the \u0026lsquo;\\d\u0026rsquo; metacharacter that matched any digit, and the \u0026lsquo;\\w\u0026rsquo; that matched any word character each represented character classes. Another way of defining character classes/sets is with bracket expressions. These can work in several ways. Basically, any characters defined inside square brackets are matched. So, if you wanted to match any digit, you could use\u0026hellip;\nstr_view_all(our_string, \"[01234556789]\", match = TRUE)    Ranges can also be defined inside the square brackets with a dash, so this would be equivalent to the expression above\u0026hellip;\nstr_view_all(our_string, \"[0-9]\", match = TRUE)    And if you want to match the dash, put it at the beginning\u0026hellip;\nstr_view_all(our_string, \"[-0-9]\", match = TRUE)    Many character classes have a descriptive term that can also be used if it\u0026rsquo;s bracketed by a colon on each side inside the brackets\u0026hellip;\nstr_view_all(our_string, \"[:digit:]\", match = TRUE)    Finally, the \u0026lsquo;^\u0026rsquo; can be used inside the brackets to negate the match. Notice the difference in how this character is interpreted here as compared to when we previously used it as an anchor (outside of the square brackets) \u0026ndash; CONTEXT MATTERS!\u0026hellip;\nstr_view_all(our_string, \"[^0-9]\", match = TRUE)    Alternation Alternation allows you to search for any of two or more patterns. This is achieved with the pipe symbol/vertical bar |, which is usually just above the Return key. Within regular expressions, it can be read as \u0026ldquo;or\u0026rdquo;. So, the expression \u0026ldquo;Chocolate|Vanilla\u0026rdquo; finds matches to either of these flavors\u0026hellip;\nstr_view_all(our_string, \"Chocolate|Vanilla\", match = TRUE)    And you can chain more than two of these together, as in \u0026ldquo;Chocolate|Vanilla|Strawberry\u0026rdquo;\u0026hellip;\nstr_view_all(our_string, \"Chocolate|Vanilla|Strawberry\", match = TRUE)    Grouping Grouping serves a couple main purposes in regular expressions. We\u0026rsquo;ll consider one here, and then a second in the context of backreferences in the next section. The quantifiers Jelmer introduced last week define the number of times the preceding character must occur. But what if you want to match a set of characters a specific number of times? They can be grouped by wrapping them in parentheses, so the quantifier applies to the entire set. We\u0026rsquo;ll use a new example string for this one - one from the DNA world. Sometimes strings of DNA contain short sequences of repeats, like the \u0026lsquo;ATC\u0026rsquo; repeat in the middle of this string\u0026hellip; GTACGGGATCATCATCATCATCGGATCCCAGT\ndna_string \u0026lt;- \"GTACGGGATCATCATCATCATCGGATCCCAGT\"   What if we wanted to find places where \u0026ldquo;ATC\u0026rdquo; was repeated at least 3 times in sequence?\nThis doesn\u0026rsquo;t give us what we want, since the quantifier is only being applied to the \u0026lsquo;C\u0026rsquo;\u0026hellip;\nstr_view_all(dna_string, \"ATC\u0026#123;3,\u0026#125;\", match = TRUE)    Instead, we can group the \u0026lsquo;ATC\u0026rsquo; with a set of parentheses to get the result we want\u0026hellip;\nstr_view_all(dna_string, \"(ATC)\u0026#123;3,\u0026#125;\", match = TRUE)    Backreferences Another place grouping comes in handy is with backreferences. But before we get to those, let\u0026rsquo;s get comfortable with a new function from stringr. So far, we\u0026rsquo;ve focused on using regular expressions just to search for patterns. But sometimes we want to not only find a pattern, but then replace it with something else. The str_replace() function can be thought of as an extension of str_view() that takes a third argument - the string that will be used to replace any match identified. So, say we had a mistake in the data, and \u0026ldquo;Strawberry\u0026rdquo; was actually supposed to be \u0026ldquo;Caramel\u0026rdquo;\u0026hellip;\nour_string #\u0026gt; [1] \"Chocolate-48,Vanilla-27,Strawberry-25\" str_replace(our_string, \"Strawberry\", \"Caramel\") #\u0026gt; [1] \"Chocolate-48,Vanilla-27,Caramel-25\"   Backreferences allow us to use the matches to (grouped) regex patterns as replacements. The characters matching each grouped regex pattern are temporarily assigned to variables (sequential numbers - i.e. the first matched group is assigned as 1, the second as 2, and so on), and can then be recalled with those numbers. Let\u0026rsquo;s go back to our ice cream string and use backreferences to reverse the order of the flavors in the string\u0026hellip;\n#view the current string our_string #\u0026gt; [1] \"Chocolate-48,Vanilla-27,Strawberry-25\" str_replace(our_string, \"(Chocolate-48),(Vanilla-27),(Strawberry-25)\", \"\\\\3,\\\\2,\\\\1\") #\u0026gt; [1] \"Strawberry-25,Vanilla-27,Chocolate-48\"   The first grouped match (Chocolate-48) got assigned to the variable \u0026lsquo;1\u0026rsquo;, the second (Vanilla-27) to \u0026lsquo;2\u0026rsquo;, and the third (Strawberry-25) to \u0026lsquo;3\u0026rsquo;. For the replacement, we just called these variables in reverse order. The notation to call these variables (backreferences) is often the combination of a single backslash and the number, but as Jelmer pointed out last week, in R, we need two backslashes.\nWe could also use metacharacters to do it like this (or many other ways for that matter)\u0026hellip;\nstr_replace(our_string, \"(C.*),(V.*),(S.*)\", \"\\\\3,\\\\1,\\\\2\") #\u0026gt; [1] \"Strawberry-25,Chocolate-48,Vanilla-27\"   Greediness By default, regular expression matches will be greedy, as in this example\u0026hellip;\nstr_view_all(our_string, \"C.+\\\\d\\\\d\")    Notice there are three possible valid matches to the search pattern here \u0026ndash; \u0026ldquo;Chocolate-48\u0026rdquo;, \u0026ldquo;Chocolate-48,Vanilla-27\u0026rdquo;, and the full string which is actually what gets matched. This is called greedy behavior - the longest valid match will be identified by default. You can add the \u0026lsquo;?\u0026rsquo; after a quantifier to make the match non-greedy\u0026hellip;\nstr_view_all(our_string, \"C.+?\\\\d\\\\d\")    So, in summary for today,\n Bracket expressions (square brackets) allow you to match anything inside them. Ranges can be defined with a dash. Notation is also available to define and match character classes \u0026ndash; things like digits, lowercase letters, punctuation, etc. The | means \u0026ldquo;or\u0026rdquo; \u0026ndash; use it to match one of two or more patterns. Parentheses can be used to group a set of characters/metacharacters into a single regex pattern. When grouped patterns match, they are assigned to a temporary numeric variable that can be used to recall the match, usually to use it in a replacement. If there is more than one valid match to a regex search pattern, the longest one will be returned by default. This \u0026ldquo;greedy\u0026rdquo; behavior can be reversed by adding a \u0026lsquo;?\u0026rsquo; after the relevant quantifier.  Like last week, we\u0026rsquo;ll use data from the Great British Bakeoff to practice with some of these things. If you didn\u0026rsquo;t install that dataset last week, you can get it with the following code\u0026hellip;\n## If needed, first install the \"remotes\" package: # install.packages(\"remotes\") remotes::install_github(\"apreshill/bakeoff\")   Then (everybody), load it\u0026hellip;\nlibrary(bakeoff)    Breakout rooms  Exercise 1 For the first few exercises, we\u0026rsquo;re going to work with the signature bakes found in the \u0026ldquo;signature\u0026rdquo; column of the \u0026ldquo;bakes\u0026rdquo; data frame. Assign the data from this column to an object names \u0026ldquo;sigs\u0026rdquo;. Preview it by viewing its first 3 items and getting its length.\n  Hints  Use the \u0026lsquo;$\u0026rsquo; notation to pull out the single column from the data frame, or alternatively a combination of dplyr\u0026rsquo;s select() followed by unlist(). Use square brackets to index the vector, and the length() function to get its length.\n   Solution  sigs \u0026lt;- bakes$signature sigs[1:3] #\u0026gt; [1] \"Light Jamaican Black Cakewith Strawberries and Cream\" #\u0026gt; [2] \"Chocolate Orange Cake\"  #\u0026gt; [3] \"Caramel Cinnamon and Banana Cake\" length(sigs) #\u0026gt; [1] 548       Exercise 2 Find all signature bakes that contain either raspberries or blueberries. Make sure to try to cover all the ways those ingredients might be reflected in the names.\n  Hints    Use the pipe symbol for alternation (OR)\n  Include possible variants such as raspberry, raspberries, Raspberry, etc.\n     Solution  str_view_all(sigs, \"[Rr]aspberr.+|[Bb]lueberr.+\", match = TRUE)        Exercise 3 Even if you\u0026rsquo;re not a millionaire, you\u0026rsquo;d like to try to eat like one. First, find all signature bakes that have \u0026ldquo;Millionaire\u0026rdquo; in the name. Then do a second search and limit the results to just those that start with \u0026ldquo;Millionaire\u0026rdquo;.\n  Hints   Use the appropriate anchor to limit results to those with \u0026ldquo;Millionaire\u0026rdquo; at the beginning of the name.     Solution  str_view_all(sigs, \"Millionaire\", match=TRUE)    str_view_all(sigs, \"^Millionaire\", match=TRUE)        Exercise 4 You tried each of the three signature bakes that start with \u0026ldquo;Millionaire\u0026rdquo;, and weren\u0026rsquo;t that impressed. Save these three bakes' names in the object \u0026lsquo;not_good\u0026rsquo; and then change the names of each of the three by replacing \u0026ldquo;Millionaire\u0026rdquo; with \u0026ldquo;Poor Man\u0026rdquo;. Assign the three new names to the object \u0026lsquo;renamed\u0026rsquo;. Note you\u0026rsquo;ll need a new stringr function for this exercise.\n  Hints  Use the str_subset() function to pull out the matching strings. Then use the str_replace() function we used in the examples for the replacement.\n   Solution  not_good \u0026lt;- str_subset(sigs, \"^Millionaire\") not_good #\u0026gt; [1] \"Millionaires' Shortbread\" \"Millionaire Banoffee Bonus\" #\u0026gt; [3] \"Millionaire's Roulade\" renamed \u0026lt;- str_replace_all(not_good, \"Millionaire\", \"Poor Man\") renamed #\u0026gt; [1] \"Poor Mans' Shortbread\" \"Poor Man Banoffee Bonus\" #\u0026gt; [3] \"Poor Man's Roulade\"       Bonus 1 For the bonus, let\u0026rsquo;s work with a different part of the dataset. The \u0026lsquo;bakers\u0026rsquo; data frame includes a column named \u0026lsquo;baker_full\u0026rsquo; that has the full name of each baker. First, extract that column and save it as the object \u0026lsquo;baker_names\u0026rsquo;. Then preview the first 5 names in this vector.\n  Hints  Use the \u0026lsquo;$\u0026rsquo; and [ ] notations.\n   Solution  baker_names \u0026lt;- bakers$baker_full baker_names[1:5] #\u0026gt; [1] \"Annetha Mills\" \"David Chambers\" \"Edward \\\"Edd\\\" Kimber\" #\u0026gt; [4] \"Jasminder Randhawa\" \"Jonathan Shepherd\"       Bonus 2 Notice from the first 5 entries of \u0026lsquo;baker_names\u0026rsquo; that the names are ordered as first name then last name, with potentially a middle name, or nickname, in between. Try reordering the names so they read last name, comma, first (and middle, if applicable). Assign the new names to \u0026lsquo;baker_names_rev\u0026rsquo;.\n  Hints  Use grouping and backreferences.\n   Solution  baker_names_rev \u0026lt;- str_replace(baker_names, \"(.+)(\\\\s[:alpha:]+)\", \"\\\\2, \\\\1\") baker_names_rev[1:5] #\u0026gt; [1] \" Mills, Annetha\" \" Chambers, David\"  #\u0026gt; [3] \" Kimber, Edward \\\"Edd\\\"\" \" Randhawa, Jasminder\"  #\u0026gt; [5] \" Shepherd, Jonathan\"      ","date":1618185600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618512594,"objectID":"a85a2a284ea62b5cc185c9c5e88d0488","permalink":"https://biodash.github.io/codeclub/18_regex2/","publishdate":"2021-04-12T00:00:00Z","relpermalink":"/codeclub/18_regex2/","section":"codeclub","summary":"Building on last week, we'll take regular expressions to a second level.","tags":null,"title":"Session 18: Regular Expressions: Part II","type":"codeclub"},{"authors":["Jelmer Poelstra"],"categories":null,"content":"\n New to Code Club?   If you didn\u0026rsquo;t already do this, please follow the Code Club Computer Setup instructions, which also has pointers for if you\u0026rsquo;re new to R or RStudio.\n  If you\u0026rsquo;re able to do so, please open RStudio a bit before Code Club starts \u0026ndash; and in case you run into issues, please join the Zoom call early and we\u0026rsquo;ll help you troubleshoot.\n   1. Getting set up While base R also has functions to work with regular expressions (such as grep() and regexp()), we will work with the stringr package, one of the core tidyverse packages.\n## If needed, install the tidyverse: # install.packages(\"tidyverse\") ## Load the tidyverse -- this will include loading \"stringr\".  library(tidyverse)   To get access to some strings that we can match with regular expressions, we will use the bakeoff data package:\n## If needed, first install the \"remotes\" package: # install.packages(\"remotes\") remotes::install_github(\"apreshill/bakeoff\")   library(bakeoff)    2. Regular expressions: what and why? You would probably have no trouble recognizing internet and email addresses, most phone numbers, or a DNA sequence embedded in a piece of text. And you would do so even if these were presented without context, and even though you may have never seen that specific email address, DNA sequence, and so on.\nWe can recognize these things because they adhere to certain patterns: a DNA sequence, for instance, typically consists of a sequence of capital As, Cs, Gs, and Ts.\nRegular expressions provide a way to describe and match text that contains specific patterns to computers, with expressions that convey things like \u0026ldquo;any digit\u0026rdquo; and \u0026ldquo;one or more or the previous character or character type\u0026rdquo;. For example, \\d{5} is a regular expression that matches at least five consecutive digits and would be a good start to finding all US ZIP codes contained in some text.\nRegular expressions are extremely useful for a couple of related purposes:\n  Finding and extracting information that adheres to patterns\n  Finding addresses, citations, or identifiers such as accession numbers.\n  Finding degenerate primers (or the DNA sequence between them) or transcription factor binding sites, in which certain positions may vary.\n  Finding DNA repeats: you know that something is repeated, but not what is.\n  While we often generalize and constrain matches at the same time, we could also merely constrain them:\n  Only find instances of \u0026ldquo;chocolate\u0026rdquo; if it is the first or last word of a line/sentence/string.\n  Only find instances of \u0026ldquo;chocolate\u0026rdquo; which are followed by \u0026ldquo;cake\u0026rdquo;, \u0026ldquo;tart\u0026rdquo;, or \u0026ldquo;croissant\u0026rdquo;.\n        Sophisticated find-and-replace\n  Replace multiple variations of the same thing at once:\ne.g. change all DNA repeats to lowercase letters or Ns.\n  Change a date format from M/DD/YY to YYYY-MM-DD, or GPS coordinates in degrees/minutes/seconds format to decimal degrees (note that this needs a bit of conversion too).\n  Rename files: switch sample ID and treatment ID separated by underscores,\nor pad numbers (1-100 =\u0026gt; 001-100 for proper ordering).\n    Finally, regular expressions can be used to parse and convert file formats, though you generally don\u0026rsquo;t have to do this yourself unless you are dealing with highly custom file types.\nRegular expressions are used in nearly all programming languages. They are also widely used in text editors and therefore provide a first taste of programming for many people.\n 3. str_view() and strings Today, to get to know regular expressions, we will just use the str_view() function from the stringr package. Next week, we\u0026rsquo;ll get introduced to other stringr functions to search and also to replace strings.\nThe basic syntax is str_view(\u0026lt;target-string(s)\u0026gt;, \u0026lt;search-pattern\u0026gt;), for example:\nstr_view(\"chocolate\", \"cola\")    str_view() shows us which part of the target string was matched in the Viewer pane of RStudio. This particular match is rather obvious because we searched for a \u0026ldquo;literal string\u0026rdquo; without any special meaning. However, the visual representation will become useful when we start using special characters in our regular expressions: then, we know what pattern we should be matching, but not what exact string we actually matched.\nIf we want to see all matches, and not just the first one, we have to use str_view_all:\nstr_view(\"chocolate\", \"o\")    str_view_all(\"chocolate\", \"o\")    stringr functions are vectorized, so we can use them not just to match a single string but also to match a vector of strings:\nbakes \u0026lt;- c(\"plum pudding\", \"chocolate cake\", \"sticky toffee pudding\") str_view(bakes, \"pudding\")    Note that the non-matching string \u0026ldquo;chocolate cake\u0026rdquo; was displayed despite the lack of a match. If we only want to see strings that matched, we can set the match argument to TRUE:\n str_view(bakes, \"pudding\", match = TRUE)     Strings in R\nA \u0026ldquo;string\u0026rdquo; or \u0026ldquo;character string\u0026rdquo; is a contiguous sequence of characters. To indicate that something is a string in R, we put quotes around it: \u0026quot;Hello\u0026quot; and \u0026quot;9\u0026quot;. If you forget the quotes, R would interpret \u0026quot;Hello\u0026quot; as an object (because it starts with a letter) and \u0026quot;9\u0026quot; as a number (because it starts with a digit).\nThere is no difference between single quotes ('Hello') and double quotes (\u0026quot;Hello\u0026quot;), but double quotes are generally recommended.\nIf your string is itself supposed to contain a quote symbol of some kind, it is convenient to use the other type of quote to define the string:\n# The string contains a single quote, so we use double quotes to define it: \"This cake's 7th layer is particularly good.\" #\u0026gt; [1] \u0026ldquo;This cake\u0026rsquo;s 7th layer is particularly good.\u0026quot; \n Alternatively, a quote can be escaped using a backslash \\ to indicate that it does not end the string but represents a literal quote inside the string, which may be necessary if a string contains both single and double quotes:\n\"This cake is only 2'4\\\" tall - do better!\" #\u0026gt; [1] \u0026ldquo;This cake is only 2'4\u0026quot; tall - do better!\u0026quot; \n    4. Special characters Special characters and escaping them In regular expressions (regex), we need a way to succinctly convey descriptions such as \u0026ldquo;any character\u0026rdquo; or \u0026ldquo;any digit\u0026rdquo;. However, there are no characters exclusive to regular expressions: instead, we re-use normal characters. For instance:\n \u0026ldquo;Any digit\u0026rdquo; is represented by \\d, with the \\ basically preventing the d from being interpreted literally. \u0026ldquo;Any character\u0026rdquo; is represented by a period, .  How, then, do we indicate a literal period . in a regular expression? The solution is to escape it with a backslash: the regular expression \\. matches a period ..\n TLDR for the rest of this section When writing regular expressions as strings in R, we always need to add an extra backslash:\n The regex \\d matches a digit \u0026mdash; and we write it as \u0026quot;\\\\d\u0026quot; in R. The regex \\. matches a period \u0026mdash; and we write it as \u0026quot;\\\\.\u0026quot; in R.    The \u0026ldquo;escaping\u0026rdquo; described above also applies to backslashes, such that the regex \\\\ matches a \\.\nEscape sequences in regular strings Outside of regular expressions, R also uses backslashes \\ to form so-called \u0026ldquo;escape sequences\u0026rdquo;. This works similarly to how the regular expression \\d means \u0026ldquo;any digit\u0026rdquo; \u0026ndash; for example, when we use \\n in any string, it will be interpreted as a newline:\ncat(\"cho\\nco\") #\u0026gt; cho #\u0026gt; co   In fact, a single backslash \\ is never taken literally in any regular R string:\ncat(\"cho\\dco\") #\u0026gt; Error: '\\d' is an unrecognized escape in character string starting \"\"cho\\d\"   Because this is not a regular expression, and \\d does not happen to be an escape sequence like \\n was earlier, \\d doesn\u0026rsquo;t mean anything to R. But instead of assuming that the backslash is therefore a literal backslash, R throws an error, demonstrating that a backslash is always interpreted as the first character in an escape sequence.\nHow can we include a backslash in a string, then? Same as before: we \u0026ldquo;escape\u0026rdquo; it with another backslash:\ncat(\"bla\\\\dbla\") #\u0026gt; bla\\dbla   The backslash plague We saw that the regular expression \\d matches a digit, but also that using string \u0026quot;\\d\u0026quot; will merely throw an error!\nTherefore, to actually define a regular expression that contains \\d, we need to use the string \u0026quot;\\\\d\u0026quot;:\nstr_view(\"The cake has 8 layers\", \"\\d\") #\u0026gt; Error: '\\d' is an unrecognized escape in character string starting \"\"\\d\"   str_view(\"The cake has 8 layers\", \"\\\\d\")    So, to define any regular expression symbol that contains a backslash, we need to always use two backslashes!\nThis also applies when we want to match a literal character. For example, to match a literal period, we need the regex \\., which we have to write as \\\\. in an R string:\nstr_view(\"The cake has 8.5 layers\", \"\\\\.\")    Now to the worst case: what if we want to match a backslash? We need the regular expression \\\\, but to define that regex as a string, we have to escape each of the two backslashes \u0026ndash; only to end up with four backslashes!\nstr_view(\"C:\\\\Windows\", \"\\\\\") #\u0026gt; Error in stri_locate_first_regex(string, pattern, opts_regex = opts(pattern)): Unrecognized backslash escape sequence in pattern. (U_REGEX_BAD_ESCAPE_SEQUENCE, context=`\\`)   str_view(\"C:\\\\Windows\", \"\\\\\\\\\")    Welcome to the backslash plague! 1\n 5. The Great British Bake Off Let\u0026rsquo;s take a look at some of the data in the bakeoff package, which is about \u0026ldquo;The Great British Bake Off\u0026rdquo; (GBBO) television show.\nThe bakers dataframe contains some information about each participant (baker) in the show, and we will be matching names from the baker_full column:\nhead(bakers) #\u0026gt; # A tibble: 6 x 8 #\u0026gt; series baker_full baker age occupation hometown baker_last baker_first #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt;  #\u0026gt; 1 1 \"Annetha Mi… Annet… 30 Midwife Essex Mills Annetha  #\u0026gt; 2 1 \"David Cham… David 31 Entrepreneur Milton K… Chambers David  #\u0026gt; 3 1 \"Edward \\\"E… Edd 24 Debt collec… Bradford Kimber Edward  #\u0026gt; 4 1 \"Jasminder … Jasmi… 45 Assistant C… Birmingh… Randhawa Jasminder  #\u0026gt; 5 1 \"Jonathan S… Jonat… 25 Research An… St Albans Shepherd Jonathan  #\u0026gt; 6 1 \"Lea Harris\" Lea 51 Retired Midlothi… Harris Lea   The challenge_results dataframe contains \u0026ldquo;signature\u0026rdquo; and \u0026ldquo;showstopper\u0026rdquo; bakes made by each participant in each episode:\nhead(challenge_results) #\u0026gt; # A tibble: 6 x 7 #\u0026gt; series episode baker result signature technical showstopper  #\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt;  #\u0026gt; 1 1 1 Annet… IN Light Jamaican … 2 Red, White \u0026amp; Blue Cho… #\u0026gt; 2 1 1 David IN Chocolate Orang… 3 Black Forest Floor Ga… #\u0026gt; 3 1 1 Edd IN Caramel Cinnamo… 1 NA  #\u0026gt; 4 1 1 Jasmi… IN Fresh Mango and… NA NA  #\u0026gt; 5 1 1 Jonat… IN Carrot Cake wit… 9 Three Tiered White an… #\u0026gt; 6 1 1 Louise IN Carrot and Oran… NA Never Fail Chocolate …   The \u0026ldquo;signature\u0026rdquo; bakes are the first bakes presented in each GBBO episode, so we\u0026rsquo;ll start trying to match these bakes with regular expressions. Let\u0026rsquo;s save them in a vector for easy access later on:\nsignatures \u0026lt;- challenge_results$signature # Assign the column to a vector signatures \u0026lt;- signatures[!is.na(signatures)] # Remove NAs signatures[1:20] # Look at the first 20 items #\u0026gt; [1] \"Light Jamaican Black Cakewith Strawberries and Cream\"  #\u0026gt; [2] \"Chocolate Orange Cake\"  #\u0026gt; [3] \"Caramel Cinnamon and Banana Cake\"  #\u0026gt; [4] \"Fresh Mango and Passion Fruit Hummingbird Cake\"  #\u0026gt; [5] \"Carrot Cake with Lime and Cream Cheese Icing\"  #\u0026gt; [6] \"Carrot and Orange Cake\"  #\u0026gt; [7] \"Triple Layered Brownie Meringue Cake\\nwith Raspberry Cream\"  #\u0026gt; [8] \"Three Tiered Lemon Drizzle Cakewith Fresh Cream and freshly made Lemon Curd\" #\u0026gt; [9] \"Cranberry and Pistachio Cakewith Orange Flower Water Icing\"  #\u0026gt; [10] \"Sticky Marmalade Tea Loaf\"  #\u0026gt; [11] \"Cheddar Cheese and Fresh Rosemary Biscuits\"  #\u0026gt; [12] \"Oatmeal Raisin Cookie\"  #\u0026gt; [13] \"Millionaires' Shortbread\"  #\u0026gt; [14] \"Honey and Candied Ginger Cookies\"  #\u0026gt; [15] \"Fresh Vanilla Biscuits with Royal Icing\"  #\u0026gt; [16] \"Peanut Shortbread withSalted Peanut Caramel\"  #\u0026gt; [17] \"Rose Petal Shortbread\"  #\u0026gt; [18] \"Stained Glass Window Shortbread\"  #\u0026gt; [19] \"Chilli Bread\"  #\u0026gt; [20] \"Olive Bread\"    6. Components of regular expressions Literal characters Literal characters can be a part of regular expressions. In fact, as we saw in the first example, our entire search pattern for str_view() can perfectly well consist of only literal characters.\nBut the power of regular expressions comes with special characters, and below, we\u0026rsquo;ll go through several different categories of these.\nMetacharacters Metacharacters often represent a single instance of a character type: above, we already learned that . matches any single character.\nOther metacharacters are actually character combinations starting with a \\:\n   Symbol Matches Negation (\u0026ldquo;anything but\u0026rdquo;)     . Any single character.    \\d Any digit. \\D   \\s Any white space: space, tab, newline, carriage return. \\S   \\w Any word character: alphanumeric and underscore. \\W   \\n A newline.    \\t A tab.     Negated metacharacters match anything except that character type: \\D matches anything except a digit.\nSome examples:\n  Are there any digits (\\d) in the bake names?\nstr_view_all(signatures, \"\\\\d\", match = TRUE)        Let\u0026rsquo;s match 5-character strings that start with \u0026ldquo;Ma\u0026quot;:\nstr_view_all(signatures, \"Ma...\", match = TRUE)    Note that the only constraint we are setting with ... is that at least three characters should follow Ma \u0026ndash; we are not restricting matches to five-character words.\n    Let\u0026rsquo;s find the bakers whose (first or last) names contain at least 11 word characters \\w:\nstr_view_all(bakers$baker_full, \"\\\\w\\\\w\\\\w\\\\w\\\\w\\\\w\\\\w\\\\w\\\\w\\\\w\\\\w\", match = TRUE)    It\u0026rsquo;s not very convenient to have to repeat \\\\w so many times!\n  Or let\u0026rsquo;s say we wanted to get all three-part names: names that contain three sets of one or more word characters separated by non-word characters. How could we describe such a pattern? \u0026ldquo;Quantifiers\u0026rdquo; to the rescue!\nQuantifiers Quantifiers describe how many consecutive instances of the preceding character should be matched:\n   Quantifier Matches     * Preceding character any number of times (0 or more).   + Preceding character at least once (1 or more).   ? Preceding character at most once (0 or 1).   {n} Preceding character exactly n times.   {n,} Preceding character at least n times.   {n,m} Preceding character at least n and at most m times.    Some examples:\n  Names with at least 11 ({11,}) characters \u0026ndash; note that this matches the entire word:\nstr_view(bakers$baker_full, \"\\\\w\u0026#123;11,\u0026#125;\", match=TRUE)        Match names with 2 to 3 ({2,3}) consecutive \u0026ldquo;e\u0026rdquo; characters. Note that this match encompasses the full string (name), because we flank the pattern with .*.\nstr_view(bakers$baker_full, \".*e\u0026#123;2,3\u0026#125;.*\", match=TRUE)        Account for different spelling options with ? \u0026ndash; match \u0026ldquo;flavor\u0026rdquo; or \u0026ldquo;flavour\u0026quot;:\nstr_view_all(signatures, \"flavou?r\", match=TRUE)        Match all three-part names \u0026ndash; one or more word characters (\\w+) separated by a non-word character (\\W) at least two consecutive times:\nstr_view(bakers$baker_full, \"\\\\w+\\\\W\\\\w+\\\\W\\\\w+\", match=TRUE)        Match all three-letter names by looking for non-word characters (\\W) surrounding three word characters (\\w{3})?\nstr_view_all(bakers$baker_full, \"\\\\W\\\\w\u0026#123;3\u0026#125;\\\\W\", match = TRUE)      That last attempt didn\u0026rsquo;t really work \u0026ndash; note that we only got three-letter middle names, since we required our three-letter names to be flanked by non-word characters.\nTo get all three-letter names, we need to be able to \u0026ldquo;anchor\u0026rdquo; our regular expressions, e.g. demand that a pattern starts at the beginning of the string.\nAnchors    Anchor Matches     ^ Beginning of the string/line   $ End of the string/line   \\b A word boundary (beginning or end)    Some examples:\n  Match all three-letter first names, by anchoring the three word characters (\\w{3}) to the beginning of the string with ^, and including a space at the end:\nstr_view(bakers$baker_full, \"^\\\\w\u0026#123;3\u0026#125; \", match = TRUE)      Match all three-letter names \u0026ndash;whether first, middle, or last\u0026ndash; using three word-characters (\\w) surrounded by word-boundaries (\\b):\nstr_view_all(bakers$baker_full, \"\\\\b\\\\w\u0026#123;3\u0026#125;\\\\b\", match = TRUE)      Regex components for next week Next week, we\u0026rsquo;ll talk about:\n Character classes Alternation Grouping Backreferences Making quantifiers non-greedy   Regular expressions vs globbing\nDo not confuse regular expressions with globbing!\nIf you have worked in a terminal before, you may know that you can match file names using shell wildcards, which is known as \u0026ldquo;globbing\u0026rdquo;.\nThere are only a few characters used in shell wildcards, but their meanings differ from regular expressions in two instances!\n   Shell wildcard Equivalent regex Meaning     ? . Any single character   * .* Any number of any character   [] and [^] same! Match/negate match of character class     Note also that . is interpreted as a literal period in globbing. We will talk about \u0026ldquo;character classes\u0026rdquo; next week.     7. Breakout rooms  Exercise 1 Find all participant names in bakers$baker_full that contain at least 4 lowercase \u0026ldquo;e\u0026rdquo; characters. (That, the \u0026ldquo;e\u0026ldquo;s don\u0026rsquo;t need to be consecutive, but you should not disallow consecutive \u0026ldquo;e\u0026ldquo;s either.)\n  Hints  Use .* to allow for optional characters in between the \u0026ldquo;e\u0026quot;s.\n   Solution  str_view(bakers$baker_full, \"e.*e.*e.*e\", match = TRUE)        Exercise 2 In the signatures vector, match words of exactly five characters that start with \u0026ldquo;Ta\u0026rdquo;.\n  Hints    To describe the five-letter word you should include three word characters after \u0026ldquo;Ta\u0026rdquo;.\n  To exclusively match five-letter words, you should use the \u0026ldquo;word boundary\u0026rdquo; anchor before and after the part that should match the word.\n     Solution  str_view_all(signatures, \"\\\\bTa\\\\w\u0026#123;3\u0026#125;\\\\b\", match = TRUE)        Exercise 3 Match \u0026ldquo;Donut\u0026rdquo; as well as \u0026ldquo;Doughnut\u0026rdquo; in the signatures vector.\nUnfortunately, signatures only contains the spelling \u0026ldquo;Doughnut\u0026rdquo;. Therefore, you should separately test whether your regex would actually match \u0026ldquo;Donut\u0026rdquo;.\n  Hints  Since \u0026ldquo;donut\u0026rdquo; is contained within \u0026ldquo;doughnut\u0026rdquo;, you can build a single regex and use ? to indicate optional characters.\n   Solution  str_view_all(signatures, \"Dou?g?h?nut\", match=TRUE)    str_view_all(c(signatures, \"Donut\"), \"Dou?g?h?nut\", match=TRUE)        Exercise 4 Match both dates in the string: \u0026ldquo;The best cakes were baked between 2016-03-10 and 2017-08-31.\u0026rdquo;.\n  Hints  Make sure you use str_view_all() and not str_view()!\n   Solution  mystring \u0026lt;- \"The best cakes were baked between 2016-03-10 and 2017-08-31.\" str_view_all(mystring, \"\\\\d\u0026#123;4\u0026#125;-\\\\d\u0026#123;2\u0026#125;-\\\\d\u0026#123;2\u0026#125;\")        Bonus exercise You can use the list.files() function in R to list files on your computer. list.files() takes an argument pattern to which you can specify a regular expression in order to narrow down the results.\nFor example, the code below would find all files with \u0026ldquo;codeclub\u0026rdquo; in the name, from your current working directory (the default for the path argument) and downwards (due to recursive = TRUE):\nlist.files(pattern = \"codeclub\", recursive = TRUE)   You can also specify a path \u0026ndash; for instance, the code below would search your home or (on Windows) Documents directory and nothing below it:\nlist.files(path = \"~\", pattern = \"codeclub\") # \"~\" is your home dir list.files(path = \"C:/Users/myname/Documents\", pattern = \"codeclub\")   Use this function to list only R scripts, i.e. files ending in .R, in a directory of your choice.\n  Hints  Make sure to use the \u0026ldquo;end of string\u0026rdquo; anchor.\n   Solution  Here we are searching the the home dir and everything below it \u0026ndash; could take a while, but then you know how many R scripts you actually have!\nlist.files(path = \"~\", pattern = \"\\\\.R$\", recursive = TRUE)       8. Further resources   The chapter on strings in Hadley Wickham\u0026rsquo;s R for Data Science (freely abailable online!).\n  RStudio regex cheatsheet.\n  A course video by Roger Peng introducing regular expressions.\n  RegExplain, an RStudio add-in to visualize regex matches and help build regular expressions.\n    Since R 4.0, which was released last year, there is also a \u0026ldquo;raw string\u0026rdquo; or \u0026ldquo;raw character constant\u0026rdquo; construct, which circumvents some of these problems \u0026ndash; see this blogpost that summarizes this new syntax. Because many are not yet using R 4.x, and most current examples, vignettes, and tutorials on the internet don\u0026rsquo;t use this, we will stick to being stuck with all the backslashes for now. \u0026#x21a9;\u0026#xfe0e;\n   ","date":1617667200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617993294,"objectID":"7360016f393c20d38ce9f0d2a030289c","permalink":"https://biodash.github.io/codeclub/17_regex/","publishdate":"2021-04-06T00:00:00Z","relpermalink":"/codeclub/17_regex/","section":"codeclub","summary":"In the first in a mini-series on working with strings in R, we will learn some basics of regular expressions.","tags":null,"title":"Session 17: Introduction to regular expressions","type":"codeclub"},{"authors":["Stephen Opiyo"],"categories":null,"content":"\n New to Code Club?   If you didn\u0026rsquo;t already do this, please follow the Code Club Computer Setup instructions, which also has pointers for if you\u0026rsquo;re new to R or RStudio.\n  If you\u0026rsquo;re able to do so, please open RStudio a bit before Code Club starts \u0026ndash; and in case you run into issues, please join the Zoom call early and we\u0026rsquo;ll help you troubleshoot.\n   1. Getting set up R has a range of functions that allow you to work with dates and times. However, today we will discuss how to work with dates and times in R using the package \u0026ldquo;lubridate\u0026rdquo;.\nWhile lubridate is tidyverse-style, it is not part of the core tidyverse, so we need to install it.\nWe are also going to use the bird data that was first discussed in Code Club session 1, and we will need to download that.\n# If needed, install the lubridate package: # install.packages(\"lubridate\") # Load the tidyverse and lubridate: library(tidyverse) library(lubridate)   # Create a dir for our bird data (\"recursive\" to create two levels at once): dir.create(\"data/birds/\", recursive = TRUE) # The URL to our file: birds_file_url \u0026lt;- \"https://raw.githubusercontent.com/biodash/biodash.github.io/master/assets/data/birds/backyard-birds_Ohio.tsv\" # The path to the file that we want to download the data to: birds_file \u0026lt;- \"data/birds/backyard-birds_Ohio.tsv\" # Download: download.file(url = birds_file_url, destfile = birds_file) # Read the data: birds \u0026lt;- read_tsv(file = birds_file)     2. What is lubridate? lubridate provides tools that make it easier to parse and manipulate dates.\nWe will discuss the common use of lubridate under the following themes:\n  Parsing dates\n  Manipulating dates\na) Instants: Rounding dates, time zones\nb) Time spans: Durations, periods, intervals\n   3. Parsing dates lubridate\u0026rsquo;s parsing functions read strings into R as \u0026ldquo;date-time\u0026rdquo; objects. Year is represented by y, month by m, and day by d.\nStrings can be parsed using the following functions: dmy(), myd(), ymd(), ydm(), dym(), mdy(), ymd_hms().\nLet us look at some examples\n# parsing by year, month, day ymd(20170131) #\u0026gt; [1] \"2017-01-31\" # parsing by month, day, year mdy(\"December 1st, 2020\") #\u0026gt; [1] \"2020-12-01\" # parsing by day, month, year dmy(\"01-Dec-2020\") #\u0026gt; [1] \"2020-12-01\" dmy(\"01/Dec/2020\") #\u0026gt; [1] \"2020-12-01\" dmy(\"01Dec2020\") #\u0026gt; [1] \"2020-12-01\" # parsing by year, month, day, hour, minutes, and seconds ymd_hms(\"2020-01-31 20:11:59\") #\u0026gt; [1] \"2020-01-31 20:11:59 UTC\"    4. Manipulating dates: Instants lubridate distinguishes between moments in time (instants) and spans of time (time spans).\nInstants are specific moments of time. They are a combination of measurements on different units (i.e, years, months, days, etc.).\n now() returns the current system time. today() returns the current system date.  The individual values for now() and today() units can be extracted from an instant and set with the accessor functions second(), minute(), hour(), day(), yday(), mday(), wday(), week(), month(), and year().\nLet us look at some examples\n# Find the current system date using function today () today() #\u0026gt; [1] \"2021-07-19\" # Find the current system time using function now () Now \u0026lt;- now() Now #\u0026gt; [1] \"2021-07-19 20:54:27 EDT\" # Extract the day of the month from an object Now using function mday () mday(Now) #\u0026gt; [1] 19 # Extract the day of the week from an object Now using function wday() wday(Now) #\u0026gt; [1] 2 # Extract the week of the year from an object Now using function week () week(Now) #\u0026gt; [1] 29 # Extract the month from an object Now using function month () month(Now) #\u0026gt; [1] 7 # Extract the year from an object Now using function year () year(Now) #\u0026gt; [1] 2021   Rounding instants Instants can be rounded to a convenient unit using the functions ceiling_date(), floor_date(), and round_date().\n  ceiling_date() takes a date-time object and rounds it up to the nearest boundary of the specified time unit.\n  round_date() takes a date-time object and time unit, and rounds it to the nearest value of the specified time unit.\n  floor_date() takes a date-time object and rounds it down to the nearest boundary of the specified time unit.\n  Let us look at some examples\nceiling_date(Now, unit = \"minute\") #\u0026gt; [1] \"2021-07-19 20:55:00 EDT\" round_date(Now, unit = \"minute\") #\u0026gt; [1] \"2021-07-19 20:54:00 EDT\" floor_date(Now, unit = \"minute\") #\u0026gt; [1] \"2021-07-19 20:54:00 EDT\"   Time zones Naming time zones is challenging because everyday names of time zones tend to be ambiguous. For example, USA has EST, or Eastern Standard Time. However, both Australia and Canada also have EST!\nTo avoid confusion, R uses the international standard IANA time zones. These use a consistent naming scheme \u0026ldquo;/\u0026rdquo;, typically in the form \u0026lt;continent\u0026gt;/\u0026lt;city\u0026gt; (there are a few exceptions because not every country lies on a continent). Examples include America/New_York, Europe/Paris, and Pacific/Auckland.\nUnless otherwise specified, lubridate always uses UTC. UTC (Coordinated Universal Time) is the standard time zone used by the scientific community and roughly equivalent to its predecessor GMT (Greenwich Mean Time).\nExample: ymd_hms(\u0026quot;2021-03-27 11:54:54 EDT\u0026quot;, tz=\u0026quot;America/New_York\u0026quot;)\nTo find your current time zone, use the Sys.timezone() function:\nSys.timezone() #\u0026gt; [1] \"America/New_York\"   To see the complete list of all time zone names, use OlsonNames():\n# See the first four time zones in the list of the time zone head(OlsonNames(), 4) #\u0026gt; [1] \"Africa/Abidjan\" \"Africa/Accra\" \"Africa/Addis_Ababa\" #\u0026gt; [4] \"Africa/Algiers\"   lubridate has two functions for working with time zones:\n  with_tz(): Changes the time zone in which an instant is displayed. The clock time displayed for the instant changes, but the moment of time described remains the same.\n  force_tz(): Changes only the time zone element of an instant. The clock time displayed remains the same, but the resulting instant describes a new moment of time.\n  x1 \u0026lt;- now() # An example using with_tz() x1a \u0026lt;- with_tz(x1, tzone = \"Australia/Lord_Howe\") x1 - x1a #\u0026gt; Time difference of 0 secs # Now use force_tz() x1b \u0026lt;- force_tz(x1, tzone = \"Australia/Lord_Howe\") x1 - x1b #\u0026gt; Time difference of 14.5 hours    4. Manipulating dates: Time spans A timespan is a length of time that may or may not be connected to a particular instant. For example, two months is a timespan. lubridate has three timespan classes: Durations, Periods and Intervals.\nDurations Durations measure the exact amount of time that occurs between two instants.\nFunctions for working with durations include is.duration(), as.duration() and duration(). For specific lengths, dseconds(), dminutes(), dhours(), ddays(), dweeks() and dyears() convenient lengths.\nPeriods Periods measure the change in clock time that occurs between two instants.\nFunctions for working with periods include is.period(), as.period() and period(). seconds(), minutes(), hours(), days(), weeks(), months() and years() quickly create periods of convenient lengths.\nIntervals Intervals are timespans that begin at a specific instant and end at a specific instant. Intervals retain complete information about a timespan. They provide the only reliable way to convert between periods and durations.\nFunctions for working with intervals include is.interval(), as.interval(), interval(), int_shift(), int_flip(), int_aligns(), int_overlaps().\nLet us look at an example\n# John was born on 19841014. How old is John h_age \u0026lt;- today() - ymd(19841014) h_age #\u0026gt; Time difference of 13427 days # Time difference in years as.duration(h_age) #\u0026gt; [1] \"1160092800s (~36.76 years)\"    5. Plotting the bird data We will plot the bird data using ggplot2.\nFirst, we plot a bar graph of days of the week:\nbirds %\u0026gt;% mutate(Wday = wday(eventDate, label = TRUE)) %\u0026gt;% ggplot(aes(x = Wday)) + geom_bar()   Second, we\u0026rsquo;ll plot the relative abundance of different bird orders by day of the week:\nbirds %\u0026gt;% mutate(Wday = wday(eventDate, label = TRUE)) %\u0026gt;% ggplot(aes(x = Wday, fill = order)) + geom_bar()    6. Breakout rooms!  Exercise 1 Jane was born on January 31st, 1992. How old is Jane today?\n  Hints (click here)  \nUse the functions today(), mdy(), and as.duration().\n   Solution (click here)  Jane_age \u0026lt;- today() - mdy(\"January 31st, 1992\") as.duration(Jane_age) #\u0026gt; [1] \"929836800s (~29.46 years)\"       Exercise 2 Calculate the time differences between the last four time zones with the current time.\n  Hints (click here)    You can get the last four time zones using the tail() function in combination with the OlsonNames() functions.\n  Then, you can compare the current time (now()) with times in different time zones using the force_tz() function.\n     Solution (click here)  # Current time  C_time \u0026lt;- now() # Last four time zones tail(OlsonNames(), 4) #\u0026gt; [1] \"UTC\" \"W-SU\" \"WET\" \"Zulu\" # Calculate time for UTC time zone  UTC_time \u0026lt;- force_tz(C_time, tzone = \"UTC\") # Calculate time difference C_time - UTC_time #\u0026gt; Time difference of 4 hours # Calculate time for W-SU time zone  WSU_time \u0026lt;- force_tz(C_time, tzone = \"W-SU\") WSU_time #\u0026gt; [1] \"2021-07-19 20:54:29 MSK\" # Calculate time difference C_time - WSU_time #\u0026gt; Time difference of 7 hours C_time #\u0026gt; [1] \"2021-07-19 20:54:29 EDT\" # Calculate time for WET time zone  WET_time \u0026lt;- force_tz(C_time, tzone = \"WET\") # Calculate time difference C_time - WET_time #\u0026gt; Time difference of 5 hours # Calculate time for Zulu time zone  Zulu_time \u0026lt;- force_tz(C_time, tzone = \"Zulu\") # Calculate time difference C_time - Zulu_time #\u0026gt; Time difference of 4 hours       Bonus exercise Remove the order \u0026ldquo;Passeriformes\u0026rdquo; from the bird data, and plot relative abundance of order based on days of the week.\n  Hints (click here)  \nUse the functions filter() and mutate().\n   Solution (click here)  # Remove Passeriformes: birds_a \u0026lt;- filter(birds, order != \"Passeriformes\") # Create the plot: birds_a %\u0026gt;% mutate(Wday = wday(eventDate, label = TRUE)) %\u0026gt;% ggplot(aes(x = Wday, fill = order)) + geom_bar()      ","date":1617148800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1626742536,"objectID":"87c18c604b69c5ed531b30df969f498e","permalink":"https://biodash.github.io/codeclub/16_lubridate/","publishdate":"2021-03-31T00:00:00Z","relpermalink":"/codeclub/16_lubridate/","section":"codeclub","summary":"Today, we will learn how to work effectively with dates and times in R using the lubridate package.","tags":null,"title":"Session 16: lubridate","type":"codeclub"},{"authors":["Jessica Cooperstone"],"categories":null,"content":" Prep homework Basic computer setup   If you didn\u0026rsquo;t already do this, please follow the Code Club Computer Setup instructions, which also has pointers for if you\u0026rsquo;re new to R or RStudio.\n  If you\u0026rsquo;re able to do so, please open RStudio a bit before Code Club starts \u0026ndash; and in case you run into issues, please join the Zoom call early and we\u0026rsquo;ll help you troubleshoot.\n  New to ggplot? Check out the three Code Club pages for Session 4, Session 5 and Session 10 which are all about ggplot2.\nIf you\u0026rsquo;ve never used ggplot2 before (or even if you have), you may find this cheat sheet useful.\nGetting Started RMarkdown for today\u0026rsquo;s session # directory for Code Club Session 15: dir.create(\"S15\") # directory for our RMarkdown # (\"recursive\" to create two levels at once.) dir.create(\"S15/Rmd/\") # save the url location for today's script todays_Rmd \u0026lt;- 'https://raw.githubusercontent.com/biodash/biodash.github.io/master/content/codeclub/15_plotly/Plotly-withOUT-answers.Rmd' # indicate the name of the new Rmd Session15_Rmd \u0026lt;- \"S15/Rmd/Session15_plotly.Rmd\" # go get that file!  download.file(url = todays_Rmd, destfile = Session15_Rmd)    1 - What is plotly? Today we are going to talk about making interactive plots using Plotly. Plotly exists in a variety of programming languages, but today we will be just talking about using it in R. All of the plotly documentation can be found here.\nIf you have never used plotly before, install it with the code below.\ninstall.packages(\"plotly\")   Here are some useful links to find info about using ggplotly.\n Basic ggplot2 charts Plotly R library fundamentals Intro to ggplotly() Using layout() ggplotly() tooltips  Before we start, there are two basic ways to use plot in R using plotly:\n Using ggplotly() - this is what we will go over today because it has the same syntax as ggplot() which we have already learned Using plot_ly() - there is slightly more functionality in this function, but the syntax is all new, so I\u0026rsquo;d suggest if you can do what you want with ggplotly(), do that. The syntax is not particularly hard so don\u0026rsquo;t be scared to use it if interactive plots are something you\u0026rsquo;re very interested in.  When you are googling about using plotly, you will find a combination of ggplotly() and plot_ly() approaches, and some parts of the code are interchangable. The easiesy way to see which parts are, is to try.\nAlso note, Google gets a bit confused when googling \u0026ldquo;ggplotly\u0026rdquo; and often returns information about just ggplot, so read extra carefully when problem solving.\nThis is an example of work from my group where we have found plotly to be particularly useful.\n   (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./apples.json\", function(chart) { Plotly.plot('chart-846279513', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })();  Data from Bilbrey et al., bioRxiv 2021\n 2 - Load libraries, get data Lets load the libraries we are using for today.\nlibrary(tidyverse) library(plotly) # for making interactive plots library(htmlwidgets) # for saving html files library(palmerpenguins) # for our penguins data   Let\u0026rsquo;s look at penguins_raw this time, a df that has a bit more data than the penguins df.\nhead(penguins_raw) #\u0026gt; # A tibble: 6 x 17 #\u0026gt; studyName `Sample Number` Species Region Island Stage `Individual ID` #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt;  #\u0026gt; 1 PAL0708 1 Adelie… Anvers Torge… Adul… N1A1  #\u0026gt; 2 PAL0708 2 Adelie… Anvers Torge… Adul… N1A2  #\u0026gt; 3 PAL0708 3 Adelie… Anvers Torge… Adul… N2A1  #\u0026gt; 4 PAL0708 4 Adelie… Anvers Torge… Adul… N2A2  #\u0026gt; 5 PAL0708 5 Adelie… Anvers Torge… Adul… N3A1  #\u0026gt; 6 PAL0708 6 Adelie… Anvers Torge… Adul… N3A2  #\u0026gt; # … with 10 more variables: `Clutch Completion` \u0026lt;chr\u0026gt;, `Date Egg` \u0026lt;date\u0026gt;, #\u0026gt; # `Culmen Length (mm)` \u0026lt;dbl\u0026gt;, `Culmen Depth (mm)` \u0026lt;dbl\u0026gt;, `Flipper Length #\u0026gt; # (mm)` \u0026lt;dbl\u0026gt;, `Body Mass (g)` \u0026lt;dbl\u0026gt;, Sex \u0026lt;chr\u0026gt;, `Delta 15 N (o/oo)` \u0026lt;dbl\u0026gt;, #\u0026gt; # `Delta 13 C (o/oo)` \u0026lt;dbl\u0026gt;, Comments \u0026lt;chr\u0026gt; head(penguins) #\u0026gt; # A tibble: 6 x 8 #\u0026gt; species island bill_length_mm bill_depth_mm flipper_length_… body_mass_g sex  #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; #\u0026gt; 1 Adelie Torge… 39.1 18.7 181 3750 male  #\u0026gt; 2 Adelie Torge… 39.5 17.4 186 3800 fema… #\u0026gt; 3 Adelie Torge… 40.3 18 195 3250 fema… #\u0026gt; 4 Adelie Torge… NA NA NA NA NA  #\u0026gt; 5 Adelie Torge… 36.7 19.3 193 3450 fema… #\u0026gt; 6 Adelie Torge… 39.3 20.6 190 3650 male  #\u0026gt; # … with 1 more variable: year \u0026lt;int\u0026gt;    3 - Create base ggplot object Using the penguins_raw dataset and make a scatter plot with Culmen Length on the y, and Culmen Depth on the x.\nbill_depth_length \u0026lt;- penguins_raw %\u0026gt;% ggplot(aes(x = `Culmen Length (mm)`, y = `Culmen Depth (mm)`)) + geom_point() bill_depth_length #\u0026gt; Warning: Removed 2 rows containing missing values (geom_point).    4 - Make it interactive with ggplotly() You can learn more about the ggplotly() function, including its arguments here.\nggplotly(bill_depth_length)       (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./penguins1.json\", function(chart) { Plotly.plot('chart-649812735', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })(); \nWow that was easy!\nLet\u0026rsquo;s add a title and change the theme to make our plot a little prettier before we progress.\nbill_depth_length \u0026lt;- penguins_raw %\u0026gt;% ggplot(aes(x = `Culmen Length (mm)`, y = `Culmen Depth (mm)`)) + geom_point() + theme_minimal() + labs(title = \"Understanding Penguin Bill Dimensions\") ggplotly(bill_depth_length)       (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./penguins2.json\", function(chart) { Plotly.plot('chart-946172853', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })(); \n 5 - Using tooltip Using tooltip helps you to indicate what appears when you hover over different parts of your plot. You can learn more about controlling tooltip here.\nWhat if we want to hover over each point and be able to tell which Island the penguin was found on?\nTo do this, we indicate what we want to hover with using text = in our aesthetic mappings. Then, we indicate tooltip = \u0026quot;text\u0026quot; to tell ggplotly() what we want to hover.\nbill_depth_length \u0026lt;- penguins_raw %\u0026gt;% ggplot(aes(x = `Culmen Length (mm)`, y = `Culmen Depth (mm)`, text = Island)) + geom_point() + theme_minimal() + labs(title = \"Understanding Penguin Bill Dimensions\") ggplotly(bill_depth_length, tooltip = \"text\")       (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./penguins3.json\", function(chart) { Plotly.plot('chart-946318527', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })(); \nYou can play around a lot with tooltip to get it to be exactly how you want, and you can include multiple things in your hover text.\nYou can also indicate to hover with data that is not inherently in your plot by mapping it to a group aesthetic.\nbill_depth_length \u0026lt;- penguins_raw %\u0026gt;% ggplot(aes(x = `Culmen Length (mm)`, y = `Culmen Depth (mm)`, text = Island, group = `Individual ID`)) + geom_point() + theme_minimal() + labs(title = \"Understanding Penguin Bill Dimensions\") ggplotly(bill_depth_length, tooltip = c(\"text\", \"Individual ID\")) # hover test will be in this order   #\u0026gt; Warning: `group_by_()` was deprecated in dplyr 0.7.0. #\u0026gt; Please use `group_by()` instead. #\u0026gt; See vignette('programming') for more help      (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./penguins4.json\", function(chart) { Plotly.plot('chart-986271453', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })(); \nYou may also want to paste in some text to your hover info to provide additional clarity on what you are showing.\nYou can use paste to add some information you\u0026rsquo;d like to see in each of the hover texts, here, we are indicating Island: Island. You can also add multiple variables within text, and it will populate in the hover text in the way you indicate. There is an example of how to do this in Bonus 1.\nbill_depth_length \u0026lt;- penguins_raw %\u0026gt;% ggplot(aes(x = `Culmen Length (mm)`, y = `Culmen Depth (mm)`, text = paste(\"Island:\", Island))) + geom_point() + theme_minimal() + labs(title = \"Understanding Penguin Bill Dimensions\") ggplotly(bill_depth_length, tooltip = \"text\")       (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./penguins5.json\", function(chart) { Plotly.plot('chart-185327649', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })(); \n 6 - Hover label aesthetics You might not like the default hover text aesthetics, and can change them! You can do this using style and layout and adding these functions using the pipe %\u0026gt;%.\n# setting fonts for the plot font \u0026lt;- list( family = \"Roboto Condensed\", size = 15, color = \"white\") # setting hover label specs label \u0026lt;- list( bgcolor = \"#FF0000\", bordercolor = \"transparent\", font = font) # we can do this bc we already set font # plotting like normal bill_depth_length \u0026lt;- penguins_raw %\u0026gt;% ggplot(aes(x = `Culmen Length (mm)`, y = `Culmen Depth (mm)`, text = paste(\"Island:\", Island))) + geom_point() + theme_minimal() + labs(title = \"A Deep Dive (ha) Into \\nUnderstanding Penguin Bill Dimensions\") # use\\n to bring your text to another line # amending our ggplotly call to include new fonts and hover label specs ggplotly(bill_depth_length, tooltip = \"text\") %\u0026gt;% style(hoverlabel = label) %\u0026gt;% layout(font = font)       (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./penguins6.json\", function(chart) { Plotly.plot('chart-725694381', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })(); \n 7 - Dynamic ticks Keep your axis labels so when you zoom, you can see where you are on your plot. Remember, you can zoom and pan around your plot!\nggplotly(bill_depth_length, tooltip = \"text\", dynamicTicks = TRUE)       (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./penguins7.json\", function(chart) { Plotly.plot('chart-358476219', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })(); \n 8 - Animating Add frame in your aesthetics mapping to tell plotly what column to animate over. You can then play your animation, or toggle from one view to another.\n# add frame bill_depth_length \u0026lt;- penguins_raw %\u0026gt;% ggplot(aes(x = `Culmen Length (mm)`, y = `Culmen Depth (mm)`, frame = Island, text = `Individual ID`)) + geom_point() + theme_minimal() + labs(title = \"Understanding Penguin Bill Dimensions\") ggplotly(bill_depth_length, tooltip = \"text\")       (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./penguins8.json\", function(chart) { Plotly.plot('chart-815967432', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })(); \nNote: I know this plot isn\u0026rsquo;t animating \u0026ndash; for an animated version, see this page. Also, if you do this in R yourself, you will find the code works.\n 9 - Everything you know about ggplot still applies! Don\u0026rsquo;t forget you can use things like faceting, that we have gone over previously in Session 10.\nbill_depth_length \u0026lt;- penguins %\u0026gt;% ggplot(aes(x = bill_length_mm, y = bill_depth_mm, color = species, text = paste(\"Island:\", island))) + geom_point() + theme_minimal() + theme(legend.position = \"none\") + labs(title = \"Understanding Penguin Bill Dimensions\", x = \"Culmen Bill Length (mm)\", y = \"Culmen Bill Depth (mm)\") + facet_wrap(~species) ggplotly(bill_depth_length, tooltip = \"text\")       (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./penguins9.json\", function(chart) { Plotly.plot('chart-821945736', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })(); \n 10 - Saving your plots Now that you\u0026rsquo;ve made a beautiful interactive plot, you probably want to save it.\nAssign the plot you want to save to an object, and use the function saveWidget() to save it. You can find the documentation here.\n# assign ggplotly plot to an object ggplotly_to_save \u0026lt;- ggplotly(bill_depth_length, tooltip = \"text\") # save saveWidget(widget = ggplotly_to_save, file = \"ggplotlying.html\")    Breakout rooms We are going to use the birds dataset from previous weeks, and gapminder data for the bonus.\nLet\u0026rsquo;s grab the birds data.\n# create directory for data to go dir.create('data/birds/', recursive = TRUE) # preparing to download # denote bird file url birds_url \u0026lt;- 'https://raw.githubusercontent.com/biodash/biodash.github.io/master/assets/data/birds/backyard-birds_Ohio.tsv' # denote file name birds_file \u0026lt;- 'data/birds/backyard-birds_Ohio.tsv' # get file download.file(url = birds_url, destfile = birds_file)   Read in data.\n# read in birds data birds \u0026lt;- read_tsv(file = 'data/birds/backyard-birds_Ohio.tsv') #\u0026gt;  #\u0026gt; ── Column specification ──────────────────────────────────────────────────────── #\u0026gt; cols( #\u0026gt; class = col_character(), #\u0026gt; order = col_character(), #\u0026gt; family = col_character(), #\u0026gt; genus = col_character(), #\u0026gt; species = col_character(), #\u0026gt; locality = col_character(), #\u0026gt; stateProvince = col_character(), #\u0026gt; decimalLatitude = col_double(), #\u0026gt; decimalLongitude = col_double(), #\u0026gt; eventDate = col_datetime(format = \"\"), #\u0026gt; species_en = col_character(), #\u0026gt; range = col_character() #\u0026gt; )   Look at your new df.\nhead(birds) #\u0026gt; # A tibble: 6 x 12 #\u0026gt; class order family genus species locality stateProvince decimalLatitude #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 Aves Pass… Corvi… Cyan… Cyanoc… 44805 A… Ohio 40.9 #\u0026gt; 2 Aves Pass… Corvi… Cyan… Cyanoc… 45244 C… Ohio 39.1 #\u0026gt; 3 Aves Pass… Corvi… Cyan… Cyanoc… 44132 E… Ohio 41.6 #\u0026gt; 4 Aves Pass… Corvi… Cyan… Cyanoc… 45242 C… Ohio 39.2 #\u0026gt; 5 Aves Pass… Corvi… Cyan… Cyanoc… 45246 C… Ohio 39.3 #\u0026gt; 6 Aves Pass… Corvi… Cyan… Cyanoc… 44484 W… Ohio 41.2 #\u0026gt; # … with 4 more variables: decimalLongitude \u0026lt;dbl\u0026gt;, eventDate \u0026lt;dttm\u0026gt;, #\u0026gt; # species_en \u0026lt;chr\u0026gt;, range \u0026lt;chr\u0026gt;   For a knitted HTML with answers, you can also see this page.\nExercise 1  Filter your new birds df to only include bald eagles. Check to see how many bald eagle sightings there were in Ohio.\n  Hints (click here)  Try using a [`filter()`](https://dplyr.tidyverse.org/reference/filter.html), and consider filtering based on `species_en`    Solutions (click here)  bald_eagle \u0026lt;- birds %\u0026gt;% filter(species_en == \"Bald Eagle\") # what do we have? head(bald_eagle) #\u0026gt; # A tibble: 6 x 12 #\u0026gt; class order family genus species locality stateProvince decimalLatitude #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 Aves Acci… Accip… Hali… Haliae… Mentor Ohio 41.7 #\u0026gt; 2 Aves Acci… Accip… Hali… Haliae… 45742 L… Ohio 39.3 #\u0026gt; 3 Aves Acci… Accip… Hali… Haliae… Morelan… Ohio 41.4 #\u0026gt; 4 Aves Acci… Accip… Hali… Haliae… Eastlake Ohio 41.7 #\u0026gt; 5 Aves Acci… Accip… Hali… Haliae… 44060 M… Ohio 41.7 #\u0026gt; 6 Aves Acci… Accip… Hali… Haliae… 44839 H… Ohio 41.4 #\u0026gt; # … with 4 more variables: decimalLongitude \u0026lt;dbl\u0026gt;, eventDate \u0026lt;dttm\u0026gt;, #\u0026gt; # species_en \u0026lt;chr\u0026gt;, range \u0026lt;chr\u0026gt; # check our df dimensions dim(bald_eagle) #\u0026gt; [1] 381 12       Exercise 2  Create a map that plots all the bald eagles found around Ohio. Color the points blue. Make sure the aspect ratio of Ohio looks reasonable to you.\n  Hints (click here)  Go back to Sessions [11](https://biodash.github.io/codeclub/11_ggplot-maps/) and [12](https://biodash.github.io/codeclub/12_loops/) to re-remember how maps work. Don't forget to call [`library(maps)`](https://rdrr.io/r/base/library.html).    Solutions (click here)  library(maps) #\u0026gt;  #\u0026gt; Attaching package: 'maps' #\u0026gt; The following object is masked from 'package:purrr': #\u0026gt;  #\u0026gt; map # get map of the states states \u0026lt;- map_data(\"state\") # filter states to only include ohio ohio \u0026lt;- states %\u0026gt;% filter(region == \"ohio\") # plot ggplot(data = ohio, aes(x = long, y = lat, group = group)) + geom_polygon(color = \"black\", fill = \"white\") + geom_point(data = bald_eagle, aes(x = decimalLongitude, y = decimalLatitude, group = NULL), color = \"blue\", alpha = 0.2) + coord_fixed(1.2) + labs(title = 'Bald Eagles Around Ohio')       Exercise 3  Make your plot interactive so you can hover and and see the locality of each bald eagle observation.\n  Hints (click here)  You may want to call `text` within `geom_point()`.    Solutions (click here)  bald_eagles_ohio \u0026lt;- ggplot(data = ohio, aes(x = long, y = lat, group = group)) + geom_polygon(color = \"black\", fill = \"white\") + geom_point(data = bald_eagle, aes(x = decimalLongitude, y = decimalLatitude, group = NULL, text = locality), color = \"blue\", alpha = 0.2) + coord_fixed(1.2) + labs(title = 'Bald Eagles Around Ohio') ggplotly(bald_eagles_ohio, tooltip = \"text\")       (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./ohio1.json\", function(chart) { Plotly.plot('chart-671395824', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })(); \n    Exercise 4  Change the hover text so that the background color is red, clean up your axis labels, and make all the fonts for the plot Arial.\n  Hints (click here)  You can set fonts either within your `ggplot()` call, or setting `font` within [`layout()`](https://docs.ropensci.org/plotly/reference/layout.html). You can customize the hover label with [`style()`](https://docs.ropensci.org/plotly/reference/style.html).    Solutions (click here)  # setting fonts for the plot eagle_font \u0026lt;- list( family = \"Arial\", size = 15, color = \"white\") # setting hover label specs eagle_label \u0026lt;- list( bgcolor = \"red\", bordercolor = \"transparent\", font = eagle_font) # we can do this bc we already set font bald_eagles_ohio \u0026lt;- ggplot(data = ohio, aes(x = long, y = lat, group = group)) + geom_polygon(color = \"black\", fill = \"white\") + geom_point(data = bald_eagle, aes(x = decimalLongitude, y = decimalLatitude, group = NULL, text = locality), color = \"blue\", alpha = 0.2) + coord_fixed(1.2) + labs(title = 'Bald Eagles Around Ohio', x = \"Latitude\", y = \"Longitude\") # amending our ggplotly call to include new fonts and hover label specs ggplotly(bald_eagles_ohio, tooltip = \"text\") %\u0026gt;% style(hoverlabel = eagle_label) %\u0026gt;% layout(font = eagle_font)       (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./ohio2.json\", function(chart) { Plotly.plot('chart-819547623', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })(); \n    Bonus Bonus 1  Let\u0026rsquo;s go back to the Gapminder data we looked at in the instructional part of Session 10 on faceting, animating, and multi-plotting.\nMake a bubble-style plot that shows the life expectancy vs. GDP per capita over 1952 to 2007 for all countries. Color by continent, and indicate population by size. Use your knowledge of making plots to alter it such that you think it is descriptive and aesthetic.\n  Hints (click here)  Set text to what you want to hover (try adding multiple variables in there!), play around with theme and scaling, change fonts and aesthetics until you are pleased. You can download the gapminder data like this:\n# install.packages(\"gapminder\") # if you weren't at Session 10 library(gapminder) head(gapminder) #\u0026gt; # A tibble: 6 x 6 #\u0026gt; country continent year lifeExp pop gdpPercap #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 Afghanistan Asia 1952 28.8 8425333 779. #\u0026gt; 2 Afghanistan Asia 1957 30.3 9240934 821. #\u0026gt; 3 Afghanistan Asia 1962 32.0 10267083 853. #\u0026gt; 4 Afghanistan Asia 1967 34.0 11537966 836. #\u0026gt; 5 Afghanistan Asia 1972 36.1 13079460 740. #\u0026gt; 6 Afghanistan Asia 1977 38.4 14880372 786.      Solutions (click here)  gapminder_font \u0026lt;- list( family = \"Roboto Condensed\") gapminder_bubble \u0026lt;- gapminder %\u0026gt;% ggplot(aes(x = gdpPercap, y = lifeExp, fill = continent, size = pop, text = paste( \"Country:\", country, \"\\nLife expectancy:\", round(lifeExp,1), \"\\nGDP per capita:\", round(gdpPercap,0)))) + geom_point(aes(frame = year), color = \"black\", shape = 21, stroke = 0.2) + scale_x_log10() + theme_minimal() + theme(plot.title = element_text(size = 18)) + labs(title = \"Changing Life Expectancy and GDP Per Capita Worldwide \\nFrom 1952 to 2007\", x = \"GDP per capita (in International Dollars)\", y = \"Life Expectancy (years)\", fill = \"\", size = \"\") ggplotly(gapminder_bubble, tooltip = c(\"text\")) %\u0026gt;% layout(font = gapminder_font)       (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./gapminder.json\", function(chart) { Plotly.plot('chart-619253748', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })(); \nNote: I know this plot isn\u0026rsquo;t animating \u0026ndash; for an animated version, see this page. Also, if you do this in R yourself, you will find the code works.\n    ","date":1616544000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1616780375,"objectID":"ce6d61c1afe64ebae9067e14090c8573","permalink":"https://biodash.github.io/codeclub/15_plotly/","publishdate":"2021-03-24T00:00:00Z","relpermalink":"/codeclub/15_plotly/","section":"codeclub","summary":"During this fifteenth session of Code Club, we will learn to make interactive plots using ggplotly.","tags":null,"title":"Session 15: Plotly","type":"codeclub"},{"authors":["Michael Broe"],"categories":null,"content":"\n New To Code Club?   First, check out the Code Club Computer Setup instructions, which also has some pointers that might be helpful if you\u0026rsquo;re new to R or RStudio.\n  Please open RStudio before Code Club to test things out \u0026ndash; if you run into issues, join the Zoom call early and we\u0026rsquo;ll troubleshoot.\n   Session Goals  Learn another way to avoid repetition in your code by creating your own functions. Learn the basic template of a function in R. Learn to incorporate your own functions into for loops and functionals like lapply() and map(). Learn all the advantages of using functions instead of copied code blocks.   We\u0026rsquo;ll be using tibble() and map() from the tidyverse packages, so we need to load that first.\nlibrary(tidyverse) #\u0026gt; ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── #\u0026gt; ✔ ggplot2 3.3.2 ✔ purrr  0.3.4 #\u0026gt; ✔ tibble  3.0.4 ✔ dplyr  0.8.5 #\u0026gt; ✔ tidyr  1.0.3 ✔ stringr 1.4.0 #\u0026gt; ✔ readr  1.3.1 ✔ forcats 0.5.0 #\u0026gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── #\u0026gt; ✖ dplyr::filter() masks stats::filter() #\u0026gt; ✖ dplyr::lag() masks stats::lag()   Why write functions? Copying your code is not good The first motivation for writing a function is when you find yourself cut-and-pasting code blocks with slight alterations each time.\nSay we have the following toy tidyverse data frame, where each column is a vector of 10 random numbers from a normal distribution, with mean = 0 and sd = 1 (the defaults for rnorm):\ndf \u0026lt;- tibble( a = rnorm(10), b = rnorm(10), c = rnorm(10), d = rnorm(10) ) df #\u0026gt; # A tibble: 10 x 4 #\u0026gt; a b c d #\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 0.388 0.736 2.26 -0.302  #\u0026gt; 2 1.19 -0.131 -2.17 -0.0537 #\u0026gt; 3 0.971 0.919 0.0329 -0.227  #\u0026gt; 4 -0.147 -1.43 1.15 -2.09  #\u0026gt; 5 1.87 0.566 -0.935 0.601  #\u0026gt; 6 -1.07 0.941 1.56 -0.413  #\u0026gt; 7 1.22 0.637 1.62 0.976  #\u0026gt; 8 -0.142 1.02 1.98 1.22  #\u0026gt; 9 -2.02 0.499 -1.93 -0.0917 #\u0026gt; 10 0.177 0.347 1.79 1.55   In previous Code Clubs we\u0026rsquo;ve seen how you can apply a built-in function like median to each column using a for loop or lapply. But say we wanted to do something a bit fancier that is not part of core R. For example, we can normalize the values in a column so they range from 0 to 1 using the following code block:\n(df$a - min(df$a)) / (max(df$a) - min(df$a)) #\u0026gt; [1] 0.6193844 0.8252053 0.7693140 0.4817556 1.0000000 0.2453463 0.8345409 #\u0026gt; [8] 0.4828838 0.0000000 0.5650749   This code is a literal translation of the mathematical formula for normalization:\n$$z_{i} = \\frac{x_{i} - min(x)}{max(x)-min(x)}$$ OK, so how can we do this for each column? Here is a first attempt:\ndf$a \u0026lt;- (df$a - min(df$a)) / (max(df$a) - min(df$a)) df$b \u0026lt;- (df$b - min(df$a)) / (max(df$b) - min(df$b)) df$c \u0026lt;- (df$c - min(df$c)) / (max(df$c) - min(df$c)) df$d \u0026lt;- (df$d - min(df$d)) / (max(df$d) - min(df$d)) df #\u0026gt; # A tibble: 10 x 4 #\u0026gt; a b c d #\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 0.619 0.301 1 0.491 #\u0026gt; 2 0.825 -0.0535 0 0.559 #\u0026gt; 3 0.769 0.376 0.497 0.512 #\u0026gt; 4 0.482 -0.582 0.749 0  #\u0026gt; 5 1 0.231 0.278 0.739 #\u0026gt; 6 0.245 0.384 0.843 0.461 #\u0026gt; 7 0.835 0.260 0.855 0.842 #\u0026gt; 8 0.483 0.418 0.938 0.910 #\u0026gt; 9 0 0.204 0.0532 0.549 #\u0026gt; 10 0.565 0.142 0.895 1   This works, but it caused me mental anguish to type it out. Even with cut and paste! All those manual textual substitutions!! And manual data entry is prone to mistakes, especially repetitive tasks like this. And say you had 1,000 columns\u0026hellip;\nAnd it didn\u0026rsquo;t work!! Honestly, I swear that mistake was totally real: I didn\u0026rsquo;t notice it until I looked at the output. Can you spot the mistake?\nIt turns out R has a range function that returns the minimum and maximum of a vector, which somewhat simplifies the coding:\nrange(df$a)   The result is a vector something like c(-1.2129504, 2.1011248) (it varies run to run, since the columns values are random) which we can index, and so we only do the min/max computation once for each column, instead of three times, so we get the following block of code for each column:\nrng \u0026lt;- range(df$a) (df$a - rng[1]) / (rng[2] - rng[1])   Does this help?\nrng \u0026lt;- range(df$a) df$a \u0026lt;- (df$a - rng[1]) / (rng[2] - rng[1]) rng \u0026lt;- range(df$b) df$b \u0026lt;- (df$b - rng[1]) / (rng[2] - rng[1]) rng \u0026lt;- range(df$c) df$c \u0026lt;- (df$c - rng[1]) / (rng[2] - rng[1]) rng \u0026lt;- range(df$d) df$d \u0026lt;- (df$d - rng[1]) / (rng[2] - rng[1])   Still pretty horrible, and arguably worse since we add a line for each column.\nHow can we distill this into a function to avoid all that repetition?\nEncapsulation of code in a function The secret to function writing is abstracting the constant from the variable. (Using the range function does throw into sharper relief what is constant and what is varying at least.) The constant part is the body of the function: the template or boiler-plate you use over and over again. The variable parts are the arguments of the function. We also need to give the function a name, so we can call it and reuse it. The template for a function is then:\nname \u0026lt;- function(arg1, arg2...) { \u0026lt;body\u0026gt; # do something with arg1, arg2 } The arguments go inside (...). The body is the block of code you want to reuse, and it\u0026rsquo;s contained in curly brackets {...}.\nHere\u0026rsquo;s what it looks like in this case:\nnormalize \u0026lt;- function(x) \u0026#123; rng \u0026lt;- range(x) (x - rng[1]) / (rng[2] - rng[1]) \u0026#125;   Pretty cool, right? Here normalize is the descriptive name we give the function.\nWe assign the function to the name using \u0026lt;- just like any other value. This means that now normalize is a function object, just like you create vector objects, or list or data frame objects, when you assigned them to names. Notice too that in RStudio they appear in the Global Environment in a special section, and clicking on them shows the code. This means that if you have a large file of code with many functions defined, you don\u0026rsquo;t have to go back searching for the function definition in the code itself.\nx is the argument of the function. In the current case this is a data frame column vector, but we can potentially use this function on any vector, so let\u0026rsquo;s not be too specific. The more generally you can write your function, the more useful it will be.\ntest_vec \u0026lt;- c(3, 7, pi, 8.657, 80) normalize(test_vec) #\u0026gt; [1] 0.000000000 0.051948052 0.001838866 0.073467532 1.000000000   When we call the function, the value we use in the function call is assigned to x and is passed in to the body of the function. So if we call the function on the first column, it gets passed in to the body, and returns the result:\ndf \u0026lt;- tibble(a = rnorm(10)) normalize(df$a) #\u0026gt; [1] 0.8726863 1.0000000 0.8764474 0.5618469 0.6111361 0.4147958 0.2889830 #\u0026gt; [8] 0.2683500 0.7181709 0.0000000   A couple of things to note:\n  Including that extra line rng \u0026lt;- range(x) is no longer a problem, since we just type it once. If you are typing things out over and over you might prefer brevity. When you write a function, you should prefer clarity. It\u0026rsquo;s good practice to break the the function down into logical steps, and name them properly. It\u0026rsquo;s much easier for others to \u0026lsquo;read\u0026rsquo; your function, and much easier for you when you come back to it in a couple of years. This is the principle of making your code \u0026lsquo;self-annotated\u0026rsquo;.\n  Functions should be simple, clear, and do one thing well. You create programs by combining simple functions in a modular manner.\n  There\u0026rsquo;s something very important but rather subtle about this use of the argument. As noted in CodeClub 12, once a for loop completes, the variable you\u0026rsquo;re using keeps the value it had at the last iteration of the loop, which persists in the global environment. Below we\u0026rsquo;ll compare that behavior to what happens with the function\u0026rsquo;s x argument.\n  Our original horrible code can now be rewritten as:\ndf$a \u0026lt;- normalize(df$a) df$b \u0026lt;- normalize(df$b) df$c \u0026lt;- normalize(df$c) df$d \u0026lt;- normalize(df$d)   Which is an improvement, but the real power comes from the fact that we can use our new function in for loops and apply statements. Here is the data from the previous couple of Clubs:\ndists_Mar4 \u0026lt;- c(17, 93, 56, 19, 175, 40, 69, 267, 4, 91) dists_Mar5 \u0026lt;- c(87, 143, 103, 223, 106, 18, 87, 72, 59, 5) dist_df \u0026lt;- data.frame(dists_Mar4, dists_Mar5)   Let\u0026rsquo;s first sanity check that our new function behaves sensibly on these vectors:\nnormalize(dists_Mar4) #\u0026gt; [1] 0.04942966 0.33840304 0.19771863 0.05703422 0.65019011 0.13688213 #\u0026gt; [7] 0.24714829 1.00000000 0.00000000 0.33079848   normalize(dists_Mar5) #\u0026gt; [1] 0.37614679 0.63302752 0.44954128 1.00000000 0.46330275 0.05963303 #\u0026gt; [7] 0.37614679 0.30733945 0.24770642 0.00000000   And while we\u0026rsquo;re here, let\u0026rsquo;s circle back to the assignment of the x argument outside and inside the function. Below we first assign a value to x outside the function; pass in a value to x inside the function; then reevaluate x outside the function call, to see what happens:\nx = pi x #\u0026gt; [1] 3.141593 normalize(dists_Mar5) # inside the function, x \u0026lt;- dists_Mar5 #\u0026gt; [1] 0.37614679 0.63302752 0.44954128 1.00000000 0.46330275 0.05963303 #\u0026gt; [7] 0.37614679 0.30733945 0.24770642 0.00000000 x #\u0026gt; [1] 3.141593   Whatever value x has outside the function does not affect, and is not affected by, the value of x inside the function. In computer science terms we say the variable(s) used inside the function are local to the function. They are freshly minted inside it, and safely destroyed before you leave it. So there is no chance of weird or unexpected conflicts with whatever variable values are set outside. In contrast, the variable in the for loop is global. It \u0026lsquo;leaks out\u0026rsquo; from where you actually used it, with perhaps unforeseen consequences. This is extremely important when you start embedding your own functions in larger programs.\nDefault values for arguments In R, we can assign a default value for an argument using = assignment. This means the argument will be called automatically, but can be overridden if explicitly called. First we create a function in the usual way:\nvariable_power \u0026lt;- function(x, p)\u0026#123; x**p # raises x to the power p \u0026#125; variable_power(2, 3) #\u0026gt; [1] 8   And now we create a version with a default value for the power:\nvariable_power_2 \u0026lt;- function(x, p = 2)\u0026#123; x**p \u0026#125; variable_power_2(2) #\u0026gt; [1] 4 variable_power_2(2, 3) #\u0026gt; [1] 8   Functions in for loops Here is how we can use our new function in a for loop over a data frame. In our previous examples of for loops median was a summary statistic and we return a single value for each column, so we created an empty vector of the desired length to hold the values for each column. Here we want to modify the original data frame with the same dimensions and column names. The following code copies the original data frame (so we don\u0026rsquo;t destroy it) and then modifies the copy \u0026lsquo;in place\u0026rsquo;:\ndist_df_norm \u0026lt;- dist_df for (column_number in 1:ncol(dist_df))\u0026#123; dist_df_norm[[column_number]] \u0026lt;- normalize(dist_df[[column_number]]) \u0026#125; dist_df_norm #\u0026gt; dists_Mar4 dists_Mar5 #\u0026gt; 1 0.04942966 0.37614679 #\u0026gt; 2 0.33840304 0.63302752 #\u0026gt; 3 0.19771863 0.44954128 #\u0026gt; 4 0.05703422 1.00000000 #\u0026gt; 5 0.65019011 0.46330275 #\u0026gt; 6 0.13688213 0.05963303 #\u0026gt; 7 0.24714829 0.37614679 #\u0026gt; 8 1.00000000 0.30733945 #\u0026gt; 9 0.00000000 0.24770642 #\u0026gt; 10 0.33079848 0.00000000   Copying an entire data frame could take a lot of time. So we can also create an empty data frame (of the same dimensions) and populate it:\nempty_vec \u0026lt;- vector(length = nrow(dist_df)) dist_df_norm_2 \u0026lt;- tibble(norm_Mar4 = empty_vec, norm_Mar5 = empty_vec) for (column_number in 1:ncol(dist_df))\u0026#123; dist_df_norm_2[[column_number]] \u0026lt;- normalize(dist_df[[column_number]]) \u0026#125; dist_df_norm_2 #\u0026gt; # A tibble: 10 x 2 #\u0026gt; norm_Mar4 norm_Mar5 #\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 0.0494 0.376  #\u0026gt; 2 0.338 0.633  #\u0026gt; 3 0.198 0.450  #\u0026gt; 4 0.0570 1  #\u0026gt; 5 0.650 0.463  #\u0026gt; 6 0.137 0.0596 #\u0026gt; 7 0.247 0.376  #\u0026gt; 8 1 0.307  #\u0026gt; 9 0 0.248  #\u0026gt; 10 0.331 0   By writing our own function, we\u0026rsquo;ve effectively extended what R can do. And this is all that packages are: libraries of new functions that extend the capabilities of base R. In fact, if there are functions you design for your particular subject area and find yourself using all the time, you can make your own package and load it, and all your favorite functions will be right there (but that\u0026rsquo;s for another day\u0026hellip;)\nFunctional programming with your own functions We saw above that you assign the function object to a name, just as you would a vector, list or data frame. In R, functions are \u0026lsquo;first class citizens\u0026rsquo;, which means you can pass them as arguments to another function. This is a very powerful idea, and part of the program of functional programming (we introduced this idea in Session 13):\n In functional programming, functions are treated as first-class citizens, meaning that they can be bound to names\u0026hellip;, passed as arguments, and returned from other functions, just as any other data type can.\n Functions that take other functions as arguments are sometimes referred to as functionals.\nIn the previous session we showed how to use built-in functions like median as arguments to functionals. The functions you write yourself can also be used in exactly the same way.\nlapply() We introduced this functional in Session 13: it always returns a list:\nlapply_norm \u0026lt;- lapply(dist_df, normalize) lapply_norm #\u0026gt; $dists_Mar4 #\u0026gt; [1] 0.04942966 0.33840304 0.19771863 0.05703422 0.65019011 0.13688213 #\u0026gt; [7] 0.24714829 1.00000000 0.00000000 0.33079848 #\u0026gt;  #\u0026gt; $dists_Mar5 #\u0026gt; [1] 0.37614679 0.63302752 0.44954128 1.00000000 0.46330275 0.05963303 #\u0026gt; [7] 0.37614679 0.30733945 0.24770642 0.00000000 typeof(lapply_norm) #\u0026gt; [1] \"list\" str(lapply_norm) #\u0026gt; List of 2 #\u0026gt; $ dists_Mar4: num [1:10] 0.0494 0.3384 0.1977 0.057 0.6502 ... #\u0026gt; $ dists_Mar5: num [1:10] 0.376 0.633 0.45 1 0.463 ...   sapply() attempts to simplify the outputs. Here both lists are of type num, and the same length, so in this case R simplifies to a matrix data structure with a single type:\nsapply_norm \u0026lt;- sapply(dist_df, normalize) sapply_norm #\u0026gt; dists_Mar4 dists_Mar5 #\u0026gt; [1,] 0.04942966 0.37614679 #\u0026gt; [2,] 0.33840304 0.63302752 #\u0026gt; [3,] 0.19771863 0.44954128 #\u0026gt; [4,] 0.05703422 1.00000000 #\u0026gt; [5,] 0.65019011 0.46330275 #\u0026gt; [6,] 0.13688213 0.05963303 #\u0026gt; [7,] 0.24714829 0.37614679 #\u0026gt; [8,] 1.00000000 0.30733945 #\u0026gt; [9,] 0.00000000 0.24770642 #\u0026gt; [10,] 0.33079848 0.00000000 typeof(sapply_norm) #\u0026gt; [1] \"double\" dim(sapply_norm) #\u0026gt; [1] 10 2 str(sapply_norm) #\u0026gt; num [1:10, 1:2] 0.0494 0.3384 0.1977 0.057 0.6502 ... #\u0026gt; - attr(*, \"dimnames\")=List of 2 #\u0026gt; ..$ : NULL #\u0026gt; ..$ : chr [1:2] \"dists_Mar4\" \"dists_Mar5\"   lapply() yields a named list, sapply() yields a named matrix.\npurrr::map() The functional map() from the purrr package behaves the same as lapply(), it always returns a list (purrr is automatically loaded as part of the tidyverse):\nmap_norm \u0026lt;- map(dist_df, normalize) map_norm #\u0026gt; $dists_Mar4 #\u0026gt; [1] 0.04942966 0.33840304 0.19771863 0.05703422 0.65019011 0.13688213 #\u0026gt; [7] 0.24714829 1.00000000 0.00000000 0.33079848 #\u0026gt;  #\u0026gt; $dists_Mar5 #\u0026gt; [1] 0.37614679 0.63302752 0.44954128 1.00000000 0.46330275 0.05963303 #\u0026gt; [7] 0.37614679 0.30733945 0.24770642 0.00000000 typeof(map_norm) #\u0026gt; [1] \"list\" str(map_norm) #\u0026gt; List of 2 #\u0026gt; $ dists_Mar4: num [1:10] 0.0494 0.3384 0.1977 0.057 0.6502 ... #\u0026gt; $ dists_Mar5: num [1:10] 0.376 0.633 0.45 1 0.463 ...   Notice another advantage of both lapply() and map(): we don\u0026rsquo;t need to explicitly preallocate any kind of data structure to collect the results. The allocation is done behind the scenes as part of the implementation of lapply() and map(), which makes sure they run efficiently. In fact, R implements these functionals as a for loop behind the scenes, and in map() that for loop is implemented in C, so it optimizes performance.\nIf we want the output to be a data frame to match the input, we can simply coerce it:\nmap_norm_df \u0026lt;- map(dist_df, normalize) %\u0026gt;% as_tibble str(map_norm_df) #\u0026gt; tibble [10 × 2] (S3: tbl_df/tbl/data.frame) #\u0026gt; $ dists_Mar4: num [1:10] 0.0494 0.3384 0.1977 0.057 0.6502 ... #\u0026gt; $ dists_Mar5: num [1:10] 0.376 0.633 0.45 1 0.463 ...   Advantages of using functions Functions:\n avoid duplication, save time avoid coding errors in repetitive code localize variables, avoiding unexpected assignment surprises let you modify code in a single place, not multiple places lets you reuse code, since a single function can often be used on multiple inputs (vectors, lists and data frames), and can be imported from a package, instead of copy and paste.  Breakout rooms Exercise 1 R does not have a built-in function for calculating the coefficient of variation, aka the RSD (relative standard deviation). This is defined as the ratio of the standard deviation to the mean.\nCreate a function that computes this, and test it on a couple of vectors.\n  Hints (click here)  The relevant R built-ins are sd() and mean(). The function should have one argument, which is assumed to be a vector.    Solution (click here)  cv \u0026lt;- function(v)\u0026#123; sd(v)/mean(v) \u0026#125; cv(1:200) #\u0026gt; [1] 0.5759123      Exercise 2 Write a function equalish() which compares two numbers a and b, and checks if they are \u0026lsquo;equal enough\u0026rsquo; according to some threshold epsilon. Set a default threshold of 0.000001. The function should return TRUE if the absolute value of the difference is inside this threshold.\nCheck that it works on a couple of test numbers.\nNow pass in a couple of test vectors. Is this new function vectorized?\nNow call the function explicitly with a different threshold.\n  Hints (click here)  You'll need to use the absolute value function abs(), and the logical comparison operator for \"less than\".    Solution (click here)  equalish \u0026lt;- function(a, b, epsilon = 0.000001)\u0026#123; abs(a - b) \u0026lt; epsilon \u0026#125;   equalish(4.0, 4.01) #\u0026gt; [1] FALSE equalish(4.0, 4.000000001) #\u0026gt; [1] TRUE   v1 \u0026lt;- c(4.000000001, 2) v2 \u0026lt;- c(4.0, 7) equalish(v1, v2) #\u0026gt; [1] TRUE FALSE   equalish(v1, v2, 0.000000000000001) #\u0026gt; [1] FALSE FALSE      Exercise 3 The fastq file format for DNA sequencing uses a letter/punctuation code for the quality of the base called at each position (the fourth line below) which is in one-to-one relationship to the bases in the second line:\n@SIM:1:FCX:1:15:6329:1045 1:N:0:2 TCGCACTCAACGCCCTGCATATGACAAGACAGAATC + \u0026lt;\u0026gt;;##=\u0026gt;\u0026lt;9=AAAAAAAAAA9#:\u0026lt;#\u0026lt;;\u0026lt;\u0026lt;\u0026lt;????#=  To translate a letter code into a numerical phred quality score we have to do two things: (i) translate the character to an integer using the ASCII code look up table (ii) subtract 33 from that value (!).\nFor the first step, R has a function that converts a character into an integer according to that table, for example:\nutf8ToInt(\"!\") #\u0026gt; [1] 33   Write a function phred_score() that computes the phred score for any character. Check that it returns 0 for \u0026ldquo;!\u0026rdquo;.\nApply your function to our example string\n\u0026lt;\u0026gt;;##=\u0026gt;\u0026lt;9=AAAAAAAAAA9#:\u0026lt;#\u0026lt;;\u0026lt;\u0026lt;\u0026lt;????#=\nto convert it to phred quality scores.\nMini Bonus: Why is \u0026ldquo;33\u0026rdquo; the magic number?\n  Hints (click here)  \nRemember when you pass the value to the function it has to be an R character string.\nMini Bonus: look at the position of \u0026ldquo;!\u0026rdquo; in the ASCII table linked above and its raw ASCII integer value.    Solution (click here)  phred_score \u0026lt;- function(character)\u0026#123; utf8ToInt(character) - 33 \u0026#125; phred_score(\"\u0026lt;\u0026gt;;##=\u0026gt;\u0026lt;9=AAAAAAAAAA9#:\u0026lt;#\u0026lt;;\u0026lt;\u0026lt;\u0026lt;????#=\") #\u0026gt; [1] 27 29 26 2 2 28 29 27 24 28 32 32 32 32 32 32 32 32 32 32 24 2 25 27 2 #\u0026gt; [26] 27 26 27 27 27 30 30 30 30 2 28   \u0026ldquo;!\u0026rdquo; is the first printing character in the ASCII table. The previous characters were used historically to control the behavior of teleprinters: \u0026ldquo;the original ASCII specification included 33 non-printing control codes which originated with Teletype machines; most of these are now obsolete\u0026rdquo;. If the ASCII table started with \u0026ldquo;!\u0026rdquo; we wouldn\u0026rsquo;t need the correction (!).    ","date":1615852800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1616095517,"objectID":"657b84b08bbd34eb732254eb7a147443","permalink":"https://biodash.github.io/codeclub/14_functions/","publishdate":"2021-03-16T00:00:00Z","relpermalink":"/codeclub/14_functions/","section":"codeclub","summary":"In this session of Code Club, we'll look at how to avoid repetition in another way by writing your own functions.","tags":null,"title":"Session 14: Writing your own Functions","type":"codeclub"},{"authors":["Mike Sovic"],"categories":null,"content":" Session Goals  List the functions in the apply() family of functions from base R. Describe how the apply() functions relate to loops in R. Identify the input and output formats associated with different apply() functions. Identify appropriate apply() functions for different scenarios. Use apply() functions to explore some US state temperature data.   Intro: The apply() Functions R is sometimes referred to as a functional programming language, and the apply() family of functions from base R is an example of this functional programming. Let\u0026rsquo;s first take a look at some available functions - they include\u0026hellip;\n apply() lapply() sapply() tapply() mapply()  Last week in session 12, Jelmer introduced for loops as one method for iterating over some set of things in R. Let\u0026rsquo;s briefly revisit one of his examples. First, we\u0026rsquo;ll recreate his distance dataset\u0026hellip;\n#distance data (km) for two dates dists_Mar4 \u0026lt;- c(17, 93, 56, 19, 175, 40, 69, 267, 4, 91) dists_Mar5 \u0026lt;- c(87, 143, 103, 223, 106, 18, 87, 72, 59, 5) dist_df \u0026lt;- data.frame(dists_Mar4, dists_Mar5) #view the data frame dist_df #\u0026gt; dists_Mar4 dists_Mar5 #\u0026gt; 1 17 87 #\u0026gt; 2 93 143 #\u0026gt; 3 56 103 #\u0026gt; 4 19 223 #\u0026gt; 5 175 106 #\u0026gt; 6 40 18 #\u0026gt; 7 69 87 #\u0026gt; 8 267 72 #\u0026gt; 9 4 59 #\u0026gt; 10 91 5   As he showed, one way to get the median distance traveled for each day (column) is to iterate over each column with a for loop, applying the median() function to each one\u0026hellip;\n#create object to store the loop output column_medians \u0026lt;- vector(length = ncol(dist_df)) #for loop to calculate median for each column for (column_number in 1:ncol(dist_df)) \u0026#123; ## We extract one column using \"dataframe_name[[column_number]]\": column_median \u0026lt;- median(dist_df[[column_number]]) ## We add the single-column median to its associated position ## in the vector: column_medians[column_number] \u0026lt;- column_median \u0026#125; #view the result column_medians #\u0026gt; [1] 62.5 87.0   Let\u0026rsquo;s think of this loop as the \u0026ldquo;programming\u0026rdquo; part of the functional programming I mentioned earlier - we\u0026rsquo;ve written, or programmed, some code the computer will execute for us - we\u0026rsquo;ll get to the \u0026ldquo;functional\u0026rdquo; part of functional programming shortly.\nUnless you\u0026rsquo;re brand new to R, you\u0026rsquo;ve probably realized by now that there are a few data structures you find yourself working with pretty frequently. These include data frames, matrices, and lists. Not only do these get used a lot, but there are also certain operations that get performed pretty frequently on each of those types of objects. For example, doing something like iterating over either the rows or columns of a data frame and applying some function to each, like we did with the median function in the data frame above, is pretty common. That means lots of people would end up independently writing for loops that would look a lot like the one in our example. This is where the \u0026ldquo;functional\u0026rdquo; part of \u0026ldquo;functional programming\u0026rdquo; starts to come in. Instead of everyone independently writing that same basic loop over and over, it can be written one time in a general form and packaged into a function that can be called instead. And this is what the apply() functions do. Then, going one step further, functional programming allows us to pass individual functions as arguments to other functions, as we\u0026rsquo;re going to see shortly. Let\u0026rsquo;s take a look at some examples.\n Examples apply() We\u0026rsquo;ll start with the apply() function, which we can use to iterativey apply some function to the margins (rows or columns) of an object that has \u0026ldquo;row by column\u0026rdquo;\u0026quot; structure. There are three arguments that have to be passed to apply() - the object containing the data, the margin the function will be applied to (rows are designated with \u0026lsquo;1\u0026rsquo;, columns with \u0026lsquo;2\u0026rsquo;), and the function of interest.\nIn the example above, we used a loop to apply the median() function to each column of the data frame. Here, we\u0026rsquo;ll do the same thing with apply(), by passing the median() function as an argument to apply()\u0026hellip;\napply_out \u0026lt;- apply(dist_df, 2, median) #view the result apply_out #\u0026gt; dists_Mar4 dists_Mar5  #\u0026gt; 62.5 87.0   Notice how much less code it required here to do the same thing we did with the for loop above!\nNotice too that the output here is a vector (specifically, a named numeric vector). The apply() function determined this was most appropriate in this case, since the output of each iteration consisted of a single value. Here\u0026rsquo;s another scenario\u0026hellip;\napply_out_quantiles \u0026lt;- apply(dist_df, 2, quantile, probs = c(0.25, 0.5, 0.75)) #view the result apply_out_quantiles #\u0026gt; dists_Mar4 dists_Mar5 #\u0026gt; 25% 24.25 62.25 #\u0026gt; 50% 62.50 87.00 #\u0026gt; 75% 92.50 105.25   This time, the function output consisted of 3 values for each iteration, or column of the data frame. In this case, the output from apply is a matrix.\nA quick additional note about how the function above is structured. In it, we applied the quantile() function to each column, passing the probs argument to it to define the specific quantiles we wanted it to return. If we were running quantile() by itself (not in the context of apply()), it might look like this\u0026hellip;\nquantile(dists_Mar4, probs = c(0.25, 0.50, 0.75)) #\u0026gt; 25% 50% 75%  #\u0026gt; 24.25 62.50 92.50   Notice the slight difference in how the probs argument is passed to the quantile() function here versus inside the apply() function above. Here, probs is inside a set of parentheses associated with the function. But inside the apply() function, any arguments associated with the function get passed as a separate argument (separated from the function by a comma). If you check out the apply() documentation, this is indicated with the \u0026ldquo;\u0026hellip;\u0026rdquo; argument, which is described as \u0026ldquo;optional arguments to FUN\u0026rdquo;. You\u0026rsquo;ll see this kind of thing show up in other functions too.\nSo, what about the other types of apply() functions? Well, the different types are designed for different types of input. For example\u0026hellip;\nlapply() Remember that apply() requires you to define whether you\u0026rsquo;ll apply the function in a row-wise or column-wise manner. But lists aren\u0026rsquo;t set up as rows and columns. So, if we want to iterate over the elements of a list, apply() won\u0026rsquo;t work. An alternative is lapply().\nIn the next example, we\u0026rsquo;ll add some new distance data in for two additional dates. The number of observations are different this time though, so the data can\u0026rsquo;t be combined in a data frame (you might remember that a data frame is a special kind of list where each of the list elements are the same length). Since we have different lengths here, we\u0026rsquo;ll store the data as a list\u0026hellip;\n#create a list that includes the new distance data dists_Mar11 \u0026lt;- c(45, 34, 100, 40, 29, 88, 84, 102) dists_Mar12 \u0026lt;- c(90, 50, 19, 123, 77, 13, 70) dist_ls \u0026lt;- list(dists_Mar4, dists_Mar5, dists_Mar11, dists_Mar12) #view the list dist_ls #\u0026gt; [[1]] #\u0026gt; [1] 17 93 56 19 175 40 69 267 4 91 #\u0026gt;  #\u0026gt; [[2]] #\u0026gt; [1] 87 143 103 223 106 18 87 72 59 5 #\u0026gt;  #\u0026gt; [[3]] #\u0026gt; [1] 45 34 100 40 29 88 84 102 #\u0026gt;  #\u0026gt; [[4]] #\u0026gt; [1] 90 50 19 123 77 13 70   Now we\u0026rsquo;ll apply the median() function to each element of the list. Again, we could write a for loop to iterate over each list element, but lapply() will do the same thing with much less code to write\u0026hellip;\nlapply_out \u0026lt;- lapply(dist_ls, median) #view the output lapply_out #\u0026gt; [[1]] #\u0026gt; [1] 62.5 #\u0026gt;  #\u0026gt; [[2]] #\u0026gt; [1] 87 #\u0026gt;  #\u0026gt; [[3]] #\u0026gt; [1] 64.5 #\u0026gt;  #\u0026gt; [[4]] #\u0026gt; [1] 70   This time, the output is a list - lapply() always gives output in list format. But in this specific case, the output could just as easily (and maybe more simply) be stored as a vector of four values - one for each list element. sapply() is an alternative to lapply() that, like lapply() still works on list input, but that attempts to simplify the output where possible\u0026hellip;\nsapply() sapply_out \u0026lt;- sapply(dist_ls, median) #view the output sapply_out #\u0026gt; [1] 62.5 87.0 64.5 70.0   Those three: apply(), lapply(), and sapply() are the apply functions you\u0026rsquo;ll likely encounter most frequently, but there are others that apply in more specific cases - we\u0026rsquo;ll take a look at at least one more later in the Bonus section.\nBreakout Rooms We\u0026rsquo;ll work with a new temperature dataset for the Breakout Room Exercises. I\u0026rsquo;ve filtered and cleaned these data from the original dataset that\u0026rsquo;s available from climate.gov They consist of maximum average temperature values for three states - Colorado, Ohio, and Virginia, with years in rows and months in columns. You can download the data with this code\u0026hellip;\nlibrary(tidyverse) temp_url \u0026lt;- 'https://raw.githubusercontent.com/biodash/biodash.github.io/master/assets/data/temperature/co_oh_va_max_temp.txt' temp_file \u0026lt;- 'state_max_temps.tsv' download.file(url = temp_url, destfile = temp_file)   Exercise 1  First let\u0026rsquo;s load the dataset and assign it to an object named \u0026lsquo;maxtemps\u0026rsquo;. Then preview the dataset and determine its dimensions (number of rows and columns). As the \u0026lsquo;.tsv\u0026rsquo; extension on the file suggests, this is a tab delimited file.\n  Hints (click here)  \nUse read_tsv() to load the dataset. The functions head() and glimpse() are a couple good options for previewing the data. If you don\u0026rsquo;t get the dimensions from the function you preview the data with, the dim() function will provide this info.    Solution (click here)  maxtemps \u0026lt;- read_tsv(\"state_max_temps.tsv\") #\u0026gt; Parsed with column specification: #\u0026gt; cols( #\u0026gt; STATE = col_character(), #\u0026gt; YEAR = col_double(), #\u0026gt; JAN = col_double(), #\u0026gt; FEB = col_double(), #\u0026gt; MAR = col_double(), #\u0026gt; APR = col_double(), #\u0026gt; MAY = col_double(), #\u0026gt; JUN = col_double(), #\u0026gt; JUL = col_double(), #\u0026gt; AUG = col_double(), #\u0026gt; SEP = col_double(), #\u0026gt; OCT = col_double(), #\u0026gt; NOV = col_double(), #\u0026gt; DEC = col_double() #\u0026gt; ) glimpse(maxtemps) #\u0026gt; Rows: 378 #\u0026gt; Columns: 14 #\u0026gt; $ STATE \u0026lt;chr\u0026gt; \"CO\", \"CO\", \"CO\", \"CO\", \"CO\", \"CO\", \"CO\", \"CO\", \"CO\", \"CO\", \"CO… #\u0026gt; $ YEAR \u0026lt;dbl\u0026gt; 1895, 1896, 1897, 1898, 1899, 1900, 1901, 1902, 1903, 1904, 190… #\u0026gt; $ JAN \u0026lt;dbl\u0026gt; 33.6, 41.2, 34.2, 32.6, 34.0, 40.8, 39.4, 38.4, 37.5, 36.6, 33.… #\u0026gt; $ FEB \u0026lt;dbl\u0026gt; 33.3, 41.3, 36.8, 43.4, 29.4, 39.1, 37.9, 43.2, 28.3, 47.3, 33.… #\u0026gt; $ MAR \u0026lt;dbl\u0026gt; 46.1, 44.8, 42.8, 43.5, 43.7, 51.9, 46.0, 44.3, 44.1, 51.4, 48.… #\u0026gt; $ APR \u0026lt;dbl\u0026gt; 60.8, 58.3, 55.8, 59.0, 58.7, 52.4, 55.5, 59.0, 55.6, 58.3, 53.… #\u0026gt; $ MAY \u0026lt;dbl\u0026gt; 66.5, 68.0, 70.5, 61.8, 66.5, 68.1, 67.6, 68.7, 63.8, 65.5, 61.… #\u0026gt; $ JUN \u0026lt;dbl\u0026gt; 74.0, 80.2, 77.4, 76.6, 77.2, 80.1, 76.8, 78.8, 69.8, 71.7, 77.… #\u0026gt; $ JUL \u0026lt;dbl\u0026gt; 77.7, 80.9, 82.0, 81.9, 80.2, 82.9, 86.7, 80.3, 80.9, 79.0, 79.… #\u0026gt; $ AUG \u0026lt;dbl\u0026gt; 80.0, 81.2, 79.6, 82.0, 79.8, 81.9, 80.7, 80.8, 80.3, 77.8, 81.… #\u0026gt; $ SEP \u0026lt;dbl\u0026gt; 75.7, 70.1, 75.2, 72.2, 74.9, 70.2, 72.7, 72.3, 70.0, 72.2, 73.… #\u0026gt; $ OCT \u0026lt;dbl\u0026gt; 60.3, 59.2, 59.8, 57.7, 58.0, 62.9, 63.1, 61.8, 62.4, 60.2, 57.… #\u0026gt; $ NOV \u0026lt;dbl\u0026gt; 42.3, 42.8, 48.5, 43.0, 50.5, 49.3, 52.9, 46.7, 49.7, 53.0, 48.… #\u0026gt; $ DEC \u0026lt;dbl\u0026gt; 33.9, 43.1, 33.4, 30.9, 35.0, 41.5, 38.6, 36.4, 41.8, 39.8, 35.… dim(maxtemps) #\u0026gt; [1] 378 14       Exercise 2  The dataset is currently in tibble form. This is the default object type created by the read_tsv() command from readr (common in tidy workflows). The apply functions are not associated with the tidyverse, and it turns out they sometimes don\u0026rsquo;t work well with tibbles. So, before we go any further, let\u0026rsquo;s convert the tibble to a data frame.\n  Hints (click here)  Use the as.data.frame() function to convert the tibble to a data frame.\n   Solution (click here)  maxtemps \u0026lt;- as.data.frame(maxtemps) class(maxtemps) #\u0026gt; [1] \"data.frame\"       Exercise 3  Calculate the average temperature for each month across the whole dataset (using the data for all three states together).\n  Hints (click here)  \nChoose an appropriate function from the apply() family of functions and use the mean() function to calculate the mean value for each column of temperatures in the dataset (cols 3 through 14). Remember that when you\u0026rsquo;re designating the margin to apply the function to, \u0026lsquo;1\u0026rsquo; means rows and \u0026lsquo;2\u0026rsquo; means columns.    Solution (click here)  mean_monthly \u0026lt;- apply(maxtemps[,3:14], 2, mean) #OR mean_monthly \u0026lt;- sapply(maxtemps[,3:14], mean) #Remember that a data frame is just a special case of a list (one that's structured in rows and columns), so either `apply()` or `sapply()` will work here #view results mean_monthly #\u0026gt; JAN FEB MAR APR MAY JUN JUL AUG  #\u0026gt; 38.89206 41.93598 50.73148 61.35608 71.26111 79.95503 84.17460 82.23571  #\u0026gt; SEP OCT NOV DEC  #\u0026gt; 75.92857 64.56296 51.49921 41.11508       Exercise 4  Now let\u0026rsquo;s get the average annual (max) temperatures for Ohio for all the years available in the dataset (1895-2020) and view the temperatures for the first 5 years of the dataset (1895-1899). Since it\u0026rsquo;s not really obvious what each of these values correspond to, try converting this vector to a named vector with the years serving as the names.\n  Hints (click here)  \nUse the same apply() and mean() functions as above, but this time, filter the dataset for just the \u0026ldquo;OH\u0026rdquo; entries, and also apply the function by rows. Remember that a two-dimensional object like a data frame or matrix is indexed with the form [rows, columns]. Alternatively, you can use tidy notation (i.e. filter, select). Then index the resulting vector with the square bracket notation (Session 9) to get the first five items. The names() function will allow you to add names to the vector elements.\n   Solution (click here)  #base R indexing... mean_annual_oh \u0026lt;- apply(maxtemps[maxtemps$STATE == \"OH\", 3:14], 1, mean) #OR  #a more tidy approach (actually a hybrid approach here - the apply function is still base R)... mean_annual_oh \u0026lt;- maxtemps %\u0026gt;% filter(STATE == \"OH\") %\u0026gt;% select(JAN:DEC) %\u0026gt;% apply(1, mean) #view first 5 items mean_annual_oh[1:5] #\u0026gt; [1] 60.23333 60.74167 61.20833 61.42500 61.59167 #add names to the vector names(mean_annual_oh) \u0026lt;- 1895:2020 #view first 5 items mean_annual_oh[1:5] #\u0026gt; 1895 1896 1897 1898 1899  #\u0026gt; 60.23333 60.74167 61.20833 61.42500 61.59167       Bonus 1  What if we wanted to compare the mean max July temperatures for each of the three states? Use an appropriate apply() function to calculate the mean values for July separately for CO, OH, and VA.\n  Hints (click here)  \ntapply() allows you to apply a function to subsets of a vector that are defined by a set of grouping variables (factors). Check the help page for tapply() and use the \u0026ldquo;STATE\u0026rdquo; column as the grouping factor.    Solution (click here)  tapply(maxtemps[,\"JUL\"], maxtemps$STATE, mean) #\u0026gt; CO OH VA  #\u0026gt; 82.25238 84.53810 85.73333       Bonus 2  Now, instead of focusing on just July, let\u0026rsquo;s try to get the average max temperatures for each month for each of the three states separately.\n  Hint 1 (click here)  \nThe tapply() function we used in Exercise 4 only works when the input is a single vector. Look toward the end of the tapply() documentation for a suggested related function that might apply here.    Hint 2 (click here)  \nGive the aggregate() function a try. Notice that the grouping variable (the \u0026ldquo;by\u0026rdquo; argument in the function) has to be provided in the form of a list.    Solution (click here)  aggregate(maxtemps[,3:14], by = list(maxtemps$STATE), mean) #\u0026gt; Group.1 JAN FEB MAR APR MAY JUN JUL #\u0026gt; 1 CO 36.85238 40.45952 47.44444 56.49762 66.00952 76.75238 82.25238 #\u0026gt; 2 OH 35.15476 37.99444 48.45238 61.05714 72.28571 80.70000 84.53810 #\u0026gt; 3 VA 44.66905 47.35397 56.29762 66.51349 75.48810 82.41270 85.73333 #\u0026gt; AUG SEP OCT NOV DEC #\u0026gt; 1 79.99286 72.51508 60.87381 47.09365 37.73333 #\u0026gt; 2 82.60952 76.72619 64.47778 50.32698 38.54365 #\u0026gt; 3 84.10476 78.54444 68.33730 57.07698 47.06825       Purrr: An Alternative (Tidy) Approach To apply() Functions In the second exercise, we converted back from a tibble to a data frame, as the apply() functions we\u0026rsquo;ve worked with here are part of base R, and some aren\u0026rsquo;t compatible with tibbles. It\u0026rsquo;s worth mentioning that there are tidy alternatives to the apply functions - they\u0026rsquo;re part of the purrr package, which might be the topic of a future code club session. We decided to go with apply() in this session since there were a couple requests for it, and it still does get used enough that you\u0026rsquo;re likely to at least run across it, even if you don\u0026rsquo;t use it yourself. For now though, if you want more details on purrr you can find them here.\n ","date":1615248000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615489080,"objectID":"98b7c3b4309c191255a49396f4ee17f4","permalink":"https://biodash.github.io/codeclub/13_apply/","publishdate":"2021-03-09T00:00:00Z","relpermalink":"/codeclub/13_apply/","section":"codeclub","summary":"In this session of Code Club, we'll consider the `apply()` family of functions, which can often be used as efficient alternatives to writing some of the loops we worked with in the previous session.","tags":null,"title":"Session 13: Applying The Apply Functions","type":"codeclub"},{"authors":["Jelmer Poelstra"],"categories":null,"content":"\n Setup New to Code Club?   If you didn\u0026rsquo;t already do this, please follow the Code Club Computer Setup instructions, which also have pointers for if you\u0026rsquo;re new to R or RStudio.\n  If you\u0026rsquo;re able to do so, please open RStudio a bit before Code Club starts \u0026ndash; and in case you run into issues, please join the Zoom call early and we\u0026rsquo;ll help you troubleshoot.\n  Session goals Today, you will learn:\n That you should avoid copying your code. What different strategies for iteration exist in R. What vectorization is and how to make use of it. How to write a for loop. Best practices when using for loops. When you should (not) use for loops. Bonus: if statements.   Introduction Don\u0026rsquo;t Repeat Yourself Sometimes, you have a block of code and you need to repeat the operations in that code almost exactly. For instance, you may want to rerun a statistical model with different parameter values, rerun an analysis for a different batch of samples, or extract the same information for many different genes.\nYour first instinct may be to copy-and-paste the block of code, and make the necessary slight adjustments in the pasted block. However, iterating and writing your own functions are strategies that are clearer, less error-prone, and more flexible (and these two can also be combined). When the number of repeats are high, iteration is needed. When the code that needs to be repeated is more than a line or two, writing your own functions becomes useful.\nIteration Loops are the most universal iteration tool and the one we will focus on today. However, R has \u0026ldquo;functional programming\u0026rdquo; iteration methods that are less verbose and that can also be quicker to execute. These are the apply family of functions, and a more recent tidyverse approach implemented in the purrr package: we will learn more about those in the two upcoming Code Club sessions.\nLoops are still a very good place to start using iteration because they make the iteration explicit and are therefore more intuitive than functional alternatives. In addition, they can easily accommodate longer blocks of code without the need to also write your own function.\nToday, we will talk about the most common type of loop: the for loops. (Other types of loops in R are while loops and repeat loops. Related to loops are if statements, see the bonus exercise for some basics.)\nBut first\u0026hellip; Before we tackle loops we should take a step back and explore vectorization a bit more, which was briefly introduced by Michael in Code Club session 9. Besides functional programming methods, vectorization is the other reason that loops are not as widely used in R as in other programming languages.\n I: Vectorization Let\u0026rsquo;s say we have a vector (i.e., a collection of values) that consists of distances in miles:\ndists_miles \u0026lt;- c(24, 81, 48, 29, 177, 175, 20, 11, 62, 156)  Of course, we can\u0026rsquo;t science with miles, so we\u0026rsquo;ll have to convert these distances to kilometers by multiplying each value in the vector by 1.61. You may or may not know that this can be done really easily in R:\ndists_km \u0026lt;- dists_miles * 1.61 dists_km #\u0026gt; [1] 38.64 130.41 77.28 46.69 284.97 281.75 32.20 17.71 99.82 251.16  What is happening here is called a vectorized operation: 1.61 is automatically recycled as many times as needed to be multiplied with each individual value in the dist_miles vector. This is a pretty unique and very useful feature of R!\nIn many other languages, we would need a loop or a similar construct to iterate over each value in the vector and multiply by 1.61. In fact, under the hood, R also uses a loop to do this! So does it even make a difference? Yes \u0026ndash; the advantages of using vectorization in R are:\n  You don\u0026rsquo;t have to write the loop, saving you a fair bit of typing and making the code clearer.\n  The under-the-hood-loop is being executed much faster than a loop that you would write. This is because it is written in C/C++ code which only has to be called once (instead of at least as many times as there are iterations in our loop).\n  Other vectorization patterns Above, we saw an example of multiplying a vector by a single number. We can also use vectorized operations when both objects contain multiple items. For instance, say we have a vector with corresponding values for two dates:\ndists_Mar4 \u0026lt;- c(17, 93, 56, 19, 175, 40, 69, 267, 4, 91) dists_Mar5 \u0026lt;- c(87, 143, 103, 223, 106, 18, 87, 72, 59, 5)  To get the sum of these values at each position (index) of the two vectors (17 + 87, 93 + 143, etc.), we can simply do the following:\ndists_Mar4 + dists_Mar5 #\u0026gt; [1] 104 236 159 242 281 58 156 339 63 96   The two vectors don\u0026rsquo;t need to be of equal length, either:\nin the example below, we negate every other value in a vector:\nc(17, 93, 56, 19, 175, 40, 69, 267, 4, 91) * c(1, -1) #\u0026gt; [1] 17 -93 56 -19 175 -40 69 -267 4 -91    This also works for columns of a data frame, which we can extract using the dataframe_name$column_name notation (see Code Club session 9\u0026rsquo;s section on data frames, and the Base R data frame indexing summary below). Let\u0026rsquo;s say we wanted the mean distance this time:\ndist_df \u0026lt;- data.frame(dists_Mar4, dists_Mar5) dist_df$dists_mean = (dist_df$dists_Mar4 + dist_df$dists_Mar5) / 2 head(dist_df) #\u0026gt; dists_Mar4 dists_Mar5 dists_mean #\u0026gt; 1 17 87 52.0 #\u0026gt; 2 93 143 118.0 #\u0026gt; 3 56 103 79.5 #\u0026gt; 4 19 223 121.0 #\u0026gt; 5 175 106 140.5 #\u0026gt; 6 40 18 29.0  Vectorization with matrices Furthermore, we can also perform vectorized operations on entire matrices. With the following matrix:\n## We use the \"sample\" function to get 25 random values between 1 and a 100, ## and put those in a 5*5 matrix: mat \u0026lt;- matrix(sample(1:100, 25), nrow = 5, ncol = 5) mat #\u0026gt; [,1] [,2] [,3] [,4] [,5] #\u0026gt; [1,] 2 55 8 28 24 #\u0026gt; [2,] 81 30 99 33 12 #\u0026gt; [3,] 22 67 41 54 6 #\u0026gt; [4,] 48 84 42 35 100 #\u0026gt; [5,] 57 47 93 10 31  \u0026hellip;we could multiple all values by 10 or get the square of each value simply as follows:\nmat_more \u0026lt;- mat * 10 mat_more #\u0026gt; [,1] [,2] [,3] [,4] [,5] #\u0026gt; [1,] 20 550 80 280 240 #\u0026gt; [2,] 810 300 990 330 120 #\u0026gt; [3,] 220 670 410 540 60 #\u0026gt; [4,] 480 840 420 350 1000 #\u0026gt; [5,] 570 470 930 100 310 mat_squared \u0026lt;- mat * mat mat_squared #\u0026gt; [,1] [,2] [,3] [,4] [,5] #\u0026gt; [1,] 4 3025 64 784 576 #\u0026gt; [2,] 6561 900 9801 1089 144 #\u0026gt; [3,] 484 4489 1681 2916 36 #\u0026gt; [4,] 2304 7056 1764 1225 10000 #\u0026gt; [5,] 3249 2209 8649 100 961  Vectorization with indices We can also use vectorized solutions when we want to operate only on elements that satisfy a certain condition.\nLet\u0026rsquo;s say we consider any distance in one of our vectors that is below 50 to be insufficient, and we want to turn those values into negatives (a little harsh maybe, but we go with it).\nTo do so, we make use of R\u0026rsquo;s ability to index a vector with a logical vector:\n## \"not_far_enough\" will be a vector of logicals: not_far_enough \u0026lt;- dists_Mar4 \u0026lt; 50 not_far_enough #\u0026gt; [1] TRUE FALSE FALSE TRUE FALSE TRUE FALSE FALSE TRUE FALSE ## When we index the original vector with a logical vector, ## we get only those values for which \"not_far_enough\" is TRUE: dists_Mar4[not_far_enough] #\u0026gt; [1] 17 19 40 4  With the following syntax, we can replace just those low distances in our original vector:\ndists_Mar4[not_far_enough] \u0026lt;- dists_Mar4[not_far_enough] * -1 dists_Mar4 #\u0026gt; [1] -17 93 56 -19 175 -40 69 267 -4 91   In a simple case like this, we could also use the vectorized ifelse() function:\nifelse(dists_Mar5 \u0026lt; 50, dists_Mar5 * -1, dists_Mar5) #\u0026gt; [1] 87 143 103 223 106 -18 87 72 59 -5     II: For loops While it is important to use vectorization whenever possible, it can only be applied to a specific set of problems. A more universal solution when you need to repeat operations is the for loop. for loops iterate over a collection of values, allowing you to perform one or more actions for each value in the collection.\nThe basic syntax is as follows:\nfor (variable_name in collection_name) { #...do things for each item (variable_name) in the collection, one at a time... } On the first line, you initialize the for loop, telling it to assign each item in the collection to a variable (here, variable_name) one at a time.\nThe variable name is arbitrary, and the collection is whatever you want to loop over. However, for, the parentheses (), in, and the curly braces {} are all fixed elements of for loops. A simple example will help to understand the synax:\n## A loop to print negated values: for (one_number in c(1, 2, 3, 4)) \u0026#123; # We iterate over 1, 2, 3, 4 print(one_number * -1) # Multiply each number by -1 \u0026#125; #\u0026gt; [1] -1 #\u0026gt; [1] -2 #\u0026gt; [1] -3 #\u0026gt; [1] -4  Note that we don\u0026rsquo;t have to use the variable that we are looping over: we could also use a for loop as a roundabout way to simply repeat something as many times as there are values in our collection:\nfor (dummy in c(1, 2, 3, 4)) \u0026#123; print(\"Yes!\") # Print \"Yes!\" in each of our four iterations  \u0026#125; #\u0026gt; [1] \"Yes!\" #\u0026gt; [1] \"Yes!\" #\u0026gt; [1] \"Yes!\" #\u0026gt; [1] \"Yes!\"  As mentioned, the variable name that we assign is arbitrary: we could use anything, as long as we reference it with the same name inside the loop:\n## Example 1 with a different variable name: \"positive_number\" for (positive_number in c(1, 2, 3, 4)) \u0026#123; print(positive_number * -1) \u0026#125; #\u0026gt; [1] -1 #\u0026gt; [1] -2 #\u0026gt; [1] -3 #\u0026gt; [1] -4 ## Example 2 with a different variable name: \"i\" for (i in c(1, 2, 3, 4)) \u0026#123; print(i * -1) \u0026#125; #\u0026gt; [1] -1 #\u0026gt; [1] -2 #\u0026gt; [1] -3 #\u0026gt; [1] -4  Note that the variable as it was last assigned in the loop does persist in your environment:\ni #\u0026gt; [1] 4  The curly braces are not strictly necessary for one-liners like this:\nfor (i in 1:4) print(i * -1) #\u0026gt; [1] -1 #\u0026gt; [1] -2 #\u0026gt; [1] -3 #\u0026gt; [1] -4  for loop output Note that we need the print() function to print anything to screen \u0026ndash; nothing will be printed if we omit this:\nfor (i in 1:4) \u0026#123; i * -1 \u0026#125;  Similarly, if we want the output to be saved in an object of some kind, we need to explicitly make an assignment in each iteration of the loop. This is where we need to start paying attention to the design of our loop. Unless computational speed is of no concern at all, you should avoid growing an object in each iteration of the loop.\nFor example, you might be inclined to do the following if you wanted to compute the median of each column in a data frame:\n## We initialize a vector in which we collect the column medians: column_medians \u0026lt;- vector() for (column_number in 1:ncol(dist_df)) \u0026#123; ## We extract one column using \"dataframe_name[[column_number]]\": column_median \u0026lt;- median(dist_df[[column_number]]) ## We add the single-column median to our vector of medians: column_medians \u0026lt;- c(column_medians, column_median) \u0026#125; column_medians #\u0026gt; [1] 62.50 87.00 78.75  Similarly, you may be tempted to add a column (with cbind()) or a row (with rbind()) to a data frame in each iteration of the loop. However, the problem with these approaches is that R has to create an entirely new object in each iteration of the loop, because the object\u0026rsquo;s memory requirements keep increasing.\nInstead, you\u0026rsquo;ll want to give the final vector (here, column_medians) the appropriate size before you start the loop:\ncolumn_medians \u0026lt;- vector(length = ncol(dist_df)) for (column_number in 1:ncol(dist_df)) \u0026#123; column_median \u0026lt;- median(dist_df[[column_number]]) column_medians[column_number] \u0026lt;- column_median \u0026#125;  Note that for very small problems, such as the example above, there will not be a noticeable difference in computation time between pre-assigning a properly sized object versus growing an object inside the loop. However, it is still good to get into the habit of pre-assigning an object of the right size.\nSummary guidelines (when speed is an issue)  Don\u0026rsquo;t use a loop when you can instead use vectorized operations. Don\u0026rsquo;t grow objects inside the loop. Instead, pre-assign an object large enough to contain all output of the loop and fill it in inside the loop. When you write a loop, avoid doing things inside the loop that don\u0026rsquo;t need to be repeated.  Learning about how to create your own functions and/or to use functional programming techniques like purrr and the apply family of functions (upcoming Code Club sessions!) will likely reduce your reliance on loops. For instance, as we\u0026rsquo;ll see next week, computing the median of each column in a data frame can be done much more succinctly with apply().\nEven for more experienced users, loops remain a more viable option when longer blocks of code need to be repeated: we will practice with that in the exercises.\n Breakout rooms! For the exercises, you can download an R Markdown file with some code to get set up (I recommend coding in that document to get a nice overview of the maps that you will make):\ndir.create('S12') todays_rmd \u0026lt;- 'https://raw.githubusercontent.com/biodash/biodash.github.io/master/content/codeclub/12_loops/exercises.Rmd' download.file(url = todays_rmd, destfile = 'S12/exercises.Rmd')  The following code is already in your R Markdown file, which will download and read the bird dataset and the necessary packages:\n## Download the file with bird data: birds_url \u0026lt;- 'https://raw.githubusercontent.com/biodash/biodash.github.io/master/assets/data/birds/backyard-birds_sample_error.tsv' birds_file \u0026lt;- 'backyard-birds_sample_error.tsv' download.file(url = birds_url, destfile = birds_file)   ## Load the tidyverse: library(tidyverse) ## Read the file with bird data: birds \u0026lt;- read_tsv(birds_file) ## Load the maps package and get the state map: # install.packages('maps') # first install if necessary library(maps) states \u0026lt;- map_data(\"state\")  Last week, we learned about making maps. If you attended one of the first few Code Club sessions, you\u0026rsquo;ll recall our Great Backyard Birdcount data set. Here, we\u0026rsquo;ll use a country-wide random subset of this data (the full file is over 4 GB) to see where Carolina Chickadees were seen:\n## With this line, we select only the rows where the column \"species_en\" ## (English species name) equals \"Carolina Chickadee\", ## i.e. we are getting just the records for the Carolina Chickadee: caro_chickadee \u0026lt;- birds[birds$species_en == 'Carolina Chickadee', ] ## Or in tidyverse-speak: # caro_chickadee \u0026lt;- birds %\u0026gt;% filter(species_en == 'Carolina Chickadee') # Next, we create a map much like we did last week: ggplot(data = states, # Use \"states\" for underlying map mapping = aes(x = long, y = lat, group = group)) + geom_polygon(color = \"black\", fill = \"white\") + # Black state outlines, white fill geom_point(data = caro_chickadee, # Plot points from bird data set aes(x = long, y = lat, group = NULL), color = \"green4\", alpha = 0.5) + # Green points, somewhat transparent coord_fixed(1.3) + # Fix projection labs(title = 'Carolina Chickadee')   Uh-oh! Something appears to have gone wrong. In the first exercise, you\u0026rsquo;ll use vectorization to fix the coordinates in the bird data set.\nIn the second exercise, you\u0026rsquo;ll use a loop to quickly produce similar plots for several other species.\n Exercise 1: Vectorization Try to fix the coordinates using vectorized operations, and recreate the map to see if it worked.\n Start with the latitude, which is wrong for all points.    Hints (click here)     You\u0026rsquo;ll need to modify the caro_chickadee data frame, while you can keep the plotting code exactly the same.\n  Simply prepending the latitude column with a minus sign (-) will negate the values.\n  Equivalent base R ways to refer to the column with latitudes are caro_chickadee$lat and caro_chickadee[['lat']].\n     Solution (click here)  First we fix the latitude, which was simply negated:\ncaro_chickadee$lat \u0026lt;- -caro_chickadee$lat ## Or equivalently: # caro_chickadee[['lat']] \u0026lt;- -caro_chickadee[['lat']] ## Or a tidyverse way of doing this: # caro_chickadee \u0026lt;- caro_chickadee %\u0026gt;% mutate(lat = -lat)  Create the first map with the same code as the example:\nggplot(data = states, mapping = aes(x = long, y = lat, group = group)) + geom_polygon(color = \"black\", fill = \"white\") + geom_point(data = caro_chickadee, aes(x = long, y = lat, group = NULL), color = \"green4\", alpha = 0.5) + coord_fixed(1.3) + labs(title = 'Carolina Chickadee')     Once you have fixed the latitude, you should notice that for one state, there is a problem with the longitude (the offset is 10 decimal degrees).    Hints (click here)     The displaced state is North Carolina.\n  The state of each sighting is in the stateProvince column, and North Carolina\u0026rsquo;s name is simply \u0026ldquo;North Carolina\u0026rdquo; in that column.\n  It may help to first create a logical vector indicating whether for each row in the caro_chickadee data frame, stateProvincefor equals \u0026ldquo;North Carolina\u0026rdquo;.\n  Your final map will look nicer if you get rid of the plotting canvas by adding\n+ theme_void() to the code for the plot.\n     Solution (click here)  It turns out that North Carolina\u0026rsquo;s chickadees are above the Atlantic. Let\u0026rsquo;s perform a rescue operation by fixing the longitudes, which are offset by 10 degrees, just for North Carolina:\n## Get a vector of logicals, indicating which rows are from North Carolina: NC_rows \u0026lt;- caro_chickadee$stateProvince == \"North Carolina\" ## Only for North Carolina rows, change the longitude: caro_chickadee$long[NC_rows] \u0026lt;- caro_chickadee$long[NC_rows] - 10 ## Or with ifelse in one line: # caro_chickadee$long \u0026lt;- ifelse(caro_chickadee$stateProvince == \"North Carolina\", # caro_chickadee$long - 10, # caro_chickadee$long) ## Or with mutate and ifelse: # caro_chickadee %\u0026gt;% # mutate(long = ifelse(stateProvince == \"North Carolina\", long - 10, long))  And we create the final map:\nggplot(data = states, mapping = aes(x = long, y = lat, group = group)) + geom_polygon(color = \"black\", fill = \"white\") + geom_point(data = caro_chickadee, aes(x = long, y = lat, group = NULL), color = \"green4\", alpha = 0.5) + coord_fixed(1.3) + labs(title = 'Carolina Chickadee') + theme_void()   Nice!\n    Exercise 2: for loops Find the 10 most commonly observed bird species in the data set, and save their English names (found in the species_en column) in a vector.\nFeel free to check out the solution if you\u0026rsquo;re not sure how, because the focus here is on the next step: trying to create a loop.\n  Solution (click here)   top10 \u0026lt;- birds %\u0026gt;% count(species_en, sort = TRUE) %\u0026gt;% # Produces a sorted count table for \"species_en\" pull(species_en) %\u0026gt;% # Extracts the \"species_en\" column head(n = 10) # Take the top 10   Next, loop over the top-10 species to produce a plot for each one of them.\nStart with the code for the Carolina Chickadee, including the subsetting operation, and modify that.\n  Hints (click here)     In the subsetting operation where you select data for the focal species, replace \u0026ldquo;Carolina Chickadee\u0026rdquo; with whatever you name the variable (indicating an individual species) that you loop over.\nBecause this is a variable name, and not a string like \u0026ldquo;Carolina Chickadee\u0026rdquo;, don\u0026rsquo;t forget to omit the quotes.\n  You\u0026rsquo;ll also need to change the title with the looping variable.\n     Solution (click here)  for (one_species in top10) \u0026#123; ## Select just the data for one species: one_bird_data \u0026lt;- birds[birds$species_en == one_species, ] ## Or in tidyverse-speak: # one_bird_data \u0026lt;- birds %\u0026gt;% filter(species_en == one_species) p \u0026lt;- ggplot(data = states, mapping = aes(x = long, y = lat, group = group)) + geom_polygon(color = \"black\", fill = \"white\") + geom_point(data = one_bird_data, aes(x = long, y = lat, group = NULL), color = \"green4\", alpha = 0.5) + coord_fixed(1.3) + labs(title = one_species) + # Make sure to change this to the looping variable theme_void() print(p) \u0026#125;      Bonus exercise: if statements if statements are similar in syntax to for loops, and are also considered a \u0026ldquo;control flow\u0026rdquo; structure. But their purpose is different from loops: instead of iterating, if statements do something once and they only do it when a condition is fulfilled.\nFor instance, we may want to check in a script whether a certain directory (folder) exists on our computer, and if it doesn\u0026rsquo;t, then we create the directory:\nif (dir.exists('path/to/my/dir')) \u0026#123; warning(\"Oh my, the output directory doesn't exist yet!\") dir.create('path/to/my/dir') \u0026#125;  Inside the parentheses () after if should be a statement that evaluates to either TRUE or FALSE (dir.exists() will be TRUE if the directory exists, and FALSE if it does not). If it is TRUE, whatever is inside the curly braces {} will be executed, and if it is FALSE, what is inside the curly braces will be ignored.\nif statements are commonly combined with for loops: we may want to only execute the functions in our loop for items in our collection that fulfill a certain condition:\nfor (one_number in 1:10) \u0026#123; if(one_number \u0026gt; 7) \u0026#123; print(one_number) \u0026#125; \u0026#125; #\u0026gt; [1] 8 #\u0026gt; [1] 9 #\u0026gt; [1] 10  In the example above, one number \u0026gt; 7 will only be TRUE for numbers larger than 7. This example is quite contrived, as it would have been easier (and faster!) to remove these items from the vector before the loop, but it hopefully gets the point across of how an if statement works.\n Many of the maps we produced in the previous exercise looked quite similar, with most species very widespread and a few restricted to the east of the US. Maybe if we select species that haven\u0026rsquo;t been seen in Ohio, we can find some other distributional patterns.\nFirst, select the the top 50 most observed bird species, just like you did in exercise 2.\nThen, use an if statement to create plots only for those top-50 birds that have not been seen in Ohio.\n  Solution (click here)   Select the top-50 birds:  all_species \u0026lt;- birds %\u0026gt;% count(species_en, sort = TRUE) %\u0026gt;% pull(species_en) %\u0026gt;% head(n = 50)   Loop over the species:  for (one_species in all_species) \u0026#123; ## Select the focal species: one_bird \u0026lt;- birds[birds$species_en == one_species, ] ## Create a data frame with only records from Ohio: one_bird_ohio \u0026lt;- one_bird[one_bird$stateProvince == 'Ohio', ] ## Test whether the data frame with only records from Ohio has any rows. ## If it does not, we create the map for the species in question:  if(nrow(one_bird_ohio) == 0) \u0026#123; p \u0026lt;- ggplot(data = states, mapping = aes(x = long, y = lat, group = group)) + geom_polygon(color = \"black\", fill = \"white\") + geom_point(data = one_bird, aes(x = long, y = lat, group = NULL), color = \"green4\", alpha = 0.5) + coord_fixed(1.3) + labs(title = one_species) + theme_void() print(p) \u0026#125; \u0026#125;       Going further  Base R data frame indexing Extract a column as a vector:\n## By name: birds$lat birds[['lat']] # Equivalent, $ notation is shorthand ## By index (column number): birds[[8]]  Extract one or more columns as a data frame using [row, column] notation,\nwith a leading comma ([, column]) meaning all rows:\n## By name: birds[, 'lat'] # dataframe['row_name', 'column_name'] birds[, c('lat', 'long')] ## By index (column numbers): birds[, 8] # dataframe[row_number, column_number] birds[, c(8, 9)]   Subset rows by a condition, with a trailing comma ([row, ]) meaning all columns:\nbirds[birds$lat \u0026gt; 25, ] birds[birds$species_en == 'Carolina Chickadee', ]     seq_along() To loop over column indices, we have used 1:ncol() above, and to loop over vector indices, you could similarly use 1:length().\nHowever, an alternative is seq_along(), which will create an index for you.\nbirds \u0026lt;- c('titmouse', 'chickadee', 'cardinal') seq_along(birds) #\u0026gt; [1] 1 2 3  The advantage of seq_along() is thtat it will behave better when your vector accidentally has length 0 (because 1:length() will have 1 and 0 when the length is 0, and you\u0026rsquo;ll get odd-seeming errors).\n  Further reading  Vectorization in R: Why? (Noam Ross, 2014) The Iteration chapter in Hadley Wickham\u0026rsquo;s R for Data Science (2017)  ","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629301677,"objectID":"0d1188d36cd786cdd66d047b20214d54","permalink":"https://biodash.github.io/codeclub/12_loops/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/codeclub/12_loops/","section":"codeclub","summary":"This will be the first of several sessions broadly about iterating in order to avoid copying-and-pasting of code, and today we will talk about vectorization and for loops.","tags":["codeclub","markdown","rmarkdown"],"title":"Session 12: Vectorization and loops in R","type":"codeclub"},{"authors":["Stephen Opiyo"],"categories":null,"content":"\n Prep and setup New to Code Club?   If you didn\u0026rsquo;t already do this, please follow the Code Club Computer Setup instructions, which also has pointers for if you\u0026rsquo;re new to R or RStudio.\n  If you\u0026rsquo;re able to do so, please open RStudio a bit before Code Club starts \u0026ndash; and in case you run into issues, please join the Zoom call early and we\u0026rsquo;ll help you troubleshoot.\n  New to ggplot2? Check out the two Code Club pages for Session 4 and Session 5.\nIf you\u0026rsquo;ve never used ggplot2 before (or even if you have), you may find this cheat sheet useful.\nDownload the CSV file on Ohio ohio_csv \u0026lt;- 'https://raw.githubusercontent.com/biodash/biodash.github.io/master/content/codeclub/11_ggplot-maps/Ohio.csv' download.file(url = ohio_csv, destfile = 'Ohio.csv')    Creating maps in R Today, we will cover the visualization of spatial data in R using the layered grammar of graphics implementation of ggplot2 in conjunction with the contextual information of static maps from world maps in the maps package.\nBefore we look at mapping using ggplot2, let us define some terms.\nAreal data  Areal data is data which corresponds to geographical extents with polygonal boundaries.\nThe layered grammar of graphics  By definition, the layered grammar demands that every plot consist of five components: \n  a default dataset with aesthetic mappings,\n  one or more layers, each with either a geometric object (\u0026ldquo;geom\u0026rdquo;), a statistical transformation (\u0026ldquo;stat\u0026rdquo;), etc.\n  a scale for each aesthetic mapping (which can be automatically generated),\n  a coordinate system, and \n  a facet specification. \n  Since ggplot2 is an implementation of the layered grammar of graphics, every plot made with ggplot2 has each of the above elements. Consequently, map plots also have these elements, but certain elements are ﬁxed to map components: the x aesthetic is ﬁxed to longitude, the y aesthetic is ﬁxed to latitude.\nDrawing a map  Drawing a map in R requires two things. First, we have to draw the map using data that directs R to draw the polygon shapes that constitute the map. Then we add information to our map to plot color and marks. It\u0026rsquo;s the same basic logic that we have used in ggplot figures. The key thing is to have datasets that link that geographic data with the information that we want to put on the plot.\nThe maps package in R  The \u0026ldquo;maps\u0026rdquo; package in R contains a set of maps of the United States and the world drawn using longitude and latitude data. With world map, the USA map with the individual states you can accomplish a lot of the mapping tasks using the maps package. The maps package contains a lot of outlines of continents, countries, states, and counties\nMaking dataframes from map outlines by ggplot2  Recall that ggplot2 operates on dataframes. Therefore, we need some way to translate the maps data into a data frame format the ggplot can use. The package ggplot2 provides the map_data() function. The function turns a series of points along an outline into a data frame of those points. The package ggplot2 uses the following syntax: map_data(\u0026quot;name\u0026quot;) where \u0026ldquo;name\u0026rdquo; is a quoted string of the name of a map in the maps package.\nOur first maps Let us start by drawing maps of the World, USA, states, Ohio, Ohio and Indiana, and part of Europe using the maps package.\nlibrary(tidyverse) library(maps) library(scales) library(stringr) # Let us get a world map using the \"map_data\" function  world \u0026lt;- map_data(\"world\") ## Let us get a US map: usa \u0026lt;- map_data(\"usa\") # Let us get the states: states \u0026lt;- map_data(\"state\") # Select Ohio using the filter function: ohio \u0026lt;- states %\u0026gt;% filter(region == \"ohio\")    Let us plot a world map:  ggplot(data = world, mapping = aes(x = long, y = lat, group = group)) + geom_polygon(fill = \"white\", color = \"black\")    Let us plot a map of the US:  ggplot(data = usa, mapping = aes(x = long, y= lat, group = group)) + geom_polygon(fill = \"white\", color = \"black\")    Let us plot a map of the US with states:  ggplot(data = states, mapping = aes(x = long, y = lat, group = group)) + geom_polygon(fill = \"blue\", color = \"black\")    Let us plot a map of Ohio:  ggplot(data = ohio, mapping=aes(x = long, y = lat, group = group)) + geom_polygon(fill = \"white\", color = \"green\")    We can also plot a map for an arbitrary selection of states:  # We can select data for two states, for example Ohio and Indiana: ohio_indiana \u0026lt;- states %\u0026gt;% filter(region == \"ohio\" | region == \"indiana\") # Plot the map of Ohio and Indiana: ggplot(data = ohio_indiana, mapping = aes(x = long, y = lat, group = group)) + geom_polygon(fill = \"green\" , color = \"red\")    We can also plot only a specific region by filtering by latitude and longitude:  world \u0026lt;- map_data(\"world\") a_region \u0026lt;- filter(world, long \u0026gt;- 10 \u0026amp; long \u0026lt; 15.1 \u0026amp; lat \u0026gt; 32 \u0026amp; lat \u0026lt; 55) ggplot(data = a_region, mapping = aes(x = long, y = lat, group = group)) + geom_polygon(fill = \"white\", color = \"black\") + coord_fixed(1.3)   The structure of the dataframe ohio. head(ohio) #\u0026gt; long lat group order region subregion #\u0026gt; 1 -80.51776 40.64563 42 10440 ohio \u0026lt;NA\u0026gt; #\u0026gt; 2 -80.55787 40.63990 42 10441 ohio \u0026lt;NA\u0026gt; #\u0026gt; 3 -80.62089 40.63417 42 10442 ohio \u0026lt;NA\u0026gt; #\u0026gt; 4 -80.66100 40.61698 42 10443 ohio \u0026lt;NA\u0026gt; #\u0026gt; 5 -80.66673 40.60552 42 10444 ohio \u0026lt;NA\u0026gt; #\u0026gt; 6 -80.67245 40.58833 42 10445 ohio \u0026lt;NA\u0026gt;   Look at the variables in ohio, note what they refer to: \n  long = longitude. Lines of longitude, or meridians, run between the North and South Poles and measure east-west positions. While prime meridian is assigned the value of 0 degrees, and runs through Greenwich (England), meridians to the west of the prime meridian are measured in degrees west (up to 180 degrees) and those to the east of the prime meridian are measured to in degrees east (up to 180 degrees).\n  lat = latitude. Lines of latitude measure north-south position between the poles with the equator defined as 0 degrees, the North Pole defined as 90 degrees north, and the South Pole defined as 90 degrees south. \n  group = an identifier that is unique for each subregion (here the counties). Group is very important! ggplot2\u0026rsquo;s functions can take a group argument which controls (amongst other things) whether adjacent points should be connected by lines. If they are in the same group, then they get connected, but if they are in different groups then they don\u0026rsquo;t. \n  order = an identifier that indicates the order in which the boundary lines should be drawn \n  region = string indicator for regions (here the states) \n  subregion = string indicator for sub-regions (here the county names) \n  Add information to the maps The second part of mapping in R, is to add information on the map created in the first part.  In drawing the map, the \u0026ldquo;maps\u0026rdquo; package creates the backbone for visualizations. Then we add additional information to show colors and shapes. \nWe will:  - fill a map by region,  - draw a Bubble map using city population,  - make a point for every city,  - vary size of point by city size and vary the color of the dots, and  - add external data to the map. \n Let us fill by region and make sure the the lines of state borders are white:  ggplot(data = states) + geom_polygon(aes(x = long, y = lat, fill = region, group = group), color = \"white\") + coord_fixed(1.3) + guides(fill = FALSE) # Do this to omit the legend    Let us draw a \u0026ldquo;Bubble map\u0026rdquo;:  # The maps package has city data head(maps::world.cities) #\u0026gt; name country.etc pop lat long capital #\u0026gt; 1 'Abasan al-Jadidah Palestine 5629 31.31 34.34 0 #\u0026gt; 2 'Abasan al-Kabirah Palestine 18999 31.32 34.35 0 #\u0026gt; 3 'Abdul Hakim Pakistan 47788 30.55 72.11 0 #\u0026gt; 4 'Abdullah-as-Salam Kuwait 21817 29.36 47.98 0 #\u0026gt; 5 'Abud Palestine 2456 32.03 35.07 0 #\u0026gt; 6 'Abwein Palestine 3434 32.03 35.20 0 my_cities \u0026lt;- maps::world.cities usa_cities \u0026lt;- filter(my_cities,country.etc == \"USA\") head(usa_cities) #\u0026gt; name country.etc pop lat long capital #\u0026gt; 1 Abilene USA 113888 32.45 -99.74 0 #\u0026gt; 2 Akron USA 206634 41.08 -81.52 0 #\u0026gt; 3 Alameda USA 70069 37.77 -122.26 0 #\u0026gt; 4 Albany USA 45535 44.62 -123.09 0 #\u0026gt; 5 Albany USA 75510 31.58 -84.18 0 #\u0026gt; 6 Albany USA 93576 42.67 -73.80 0    Make a point for every city:  ggplot(data = usa, mapping = aes(x = long, y = lat, group = group)) + geom_polygon(color = \"black\", fill = \"white\") + geom_point(data = usa_cities, color = \"red\", aes(x = long, y = lat, group = NULL))    Let\u0026rsquo;s pick just the big cities:  usa_big_cities \u0026lt;- filter(my_cities, country.etc == \"USA\" \u0026amp; pop \u0026gt; 500000) ggplot(data = usa, mapping = aes(x = long, y = lat, group = group)) + geom_polygon(color = \"black\", fill = \"white\") + geom_point(data = usa_big_cities, color = \"red\", aes(x = long, y = lat, group = NULL))    Vary size of point by city size:  ggplot(data = usa, mapping = aes(x = long, y = lat, group = group)) + geom_polygon(color = \"black\", fill = \"white\") + geom_point(data = usa_big_cities, color = \"red\", aes(x = long, y = lat, group = NULL, size = pop))    Now vary the color of the dots:  usa_big_cities$qual \u0026lt;- sample(LETTERS[1:5], nrow(usa_big_cities), replace = TRUE) ggplot(data = usa, mapping = aes(x = long, y = lat, group = group)) + geom_polygon(color = \"black\", fill = \"white\") + geom_point(data = usa_big_cities, aes(x = long, y = lat, group = NULL, color = qual, size = pop))    Tweak the map:  # No scientific notation in legend r ggplot: # scales package adds the \"scale_size_continuous\" function, and we can set label=comma library(scales) # Change the column name to make the legend nicer\" usa_big_cities$Population \u0026lt;- usa_big_cities$pop usa_big_cities$Qualitative \u0026lt;- usa_big_cities$qual # Do some additional refining: ggplot(data = usa, mapping = aes(x = long, y= lat, group = group)) + geom_polygon(color = \"black\", fill = \"white\") + geom_point(data = usa_big_cities, aes(x = long, y = lat, group = NULL, color = Qualitative, size = Population)) + scale_size_continuous(label = comma)    Work with Ohio counties with external data:  # Get basic map data for all USA counties: usa_counties = map_data(\"county\") # Subset to counties in Ohio: oh = subset(usa_counties, region == \"ohio\") head(oh) #\u0026gt; long lat group order region subregion #\u0026gt; 59960 -83.66902 39.02989 2012 59960 ohio adams #\u0026gt; 59961 -83.56590 39.02989 2012 59961 ohio adams #\u0026gt; 59962 -83.37109 39.06426 2012 59962 ohio adams #\u0026gt; 59963 -83.30806 39.06426 2012 59963 ohio adams #\u0026gt; 59964 -83.30233 39.05280 2012 59964 ohio adams #\u0026gt; 59965 -83.25649 39.01842 2012 59965 ohio adams   # Plot ohio counties ggplot() + geom_polygon(data = oh, aes(x = long, y = lat, group = group, fill = subregion), color = \"black\", alpha = 0.3) + coord_fixed(1.3) + guides(fill = FALSE) + ggtitle(\"Ohio counties\")    Read population data for Ohio counties:  # The data of the estimated population of each county in 2021 and percent change from 2010 Ohio \u0026lt;- read_csv(\"Ohio.csv\") head(Ohio) #\u0026gt; # A tibble: 6 x 6 #\u0026gt; county Pop Perc Town x_1 y_1 #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 Adams 27706 -2.91 West Union -83.5 38.8 #\u0026gt; 2 Allen 101603 -4.47 Lima -84.1 40.7 #\u0026gt; 3 Ashland 53040 -0.53 Ashland -82.3 40.9 #\u0026gt; 4 Ashtabula 96549 -4.79 Jefferson -80.8 41.7 #\u0026gt; 5 Athens 64943 -0.35 Athens -82.1 39.3 #\u0026gt; 6 Auglaize 45496 -0.88 Wapakoneta -84.2 40.6    Prepare the data for plotting:  # Create a new column called \"county\" so that counties start with capital letters # using str_to_title function  oh$county \u0026lt;- str_to_title(oh$subregion) # Merge population data with counties data by county variable using inner_join # function, and named the new object \"ohio_pop\" ohio_pop \u0026lt;- inner_join(oh, Ohio, by = \"county\") # Select counties with population greater than 100000 ohio_big_pop \u0026lt;- filter(ohio_pop, Pop \u0026gt; 100000)    Create the plot where we vary point size by population size:  ### vary size of point by population size ggplot(data = ohio_pop, mapping = aes(x = long, y = lat, group = group))+ geom_polygon(color = \"black\", fill = \"white\")+ geom_point(data = ohio_big_pop, color = \"red\", aes(x = x_1, y = y_1, group = NULL, size = Pop)) + coord_fixed(1.3) + guides(size = FALSE) # Omit the legend    Improve the graph by creating groups of population using quantile.  ApplyQuintiles \u0026lt;- function(x) \u0026#123; cut(x, breaks = c(quantile(ohio_pop$Pop, probs = seq(0, 1, by = 0.2))), labels = c(\"0-20\", \"20-40\", \"40-60\", \"60-80\", \"80-100\"), include.lowest = TRUE) \u0026#125; ohio_pop$grouped_pop \u0026lt;- sapply(ohio_pop$Pop, ApplyQuintiles) head(ohio_pop) #\u0026gt; long lat group order region subregion county Pop Perc Town #\u0026gt; 1 -83.66902 39.02989 2012 59960 ohio adams Adams 27706 -2.91 West Union #\u0026gt; 2 -83.56590 39.02989 2012 59961 ohio adams Adams 27706 -2.91 West Union #\u0026gt; 3 -83.37109 39.06426 2012 59962 ohio adams Adams 27706 -2.91 West Union #\u0026gt; 4 -83.30806 39.06426 2012 59963 ohio adams Adams 27706 -2.91 West Union #\u0026gt; 5 -83.30233 39.05280 2012 59964 ohio adams Adams 27706 -2.91 West Union #\u0026gt; 6 -83.25649 39.01842 2012 59965 ohio adams Adams 27706 -2.91 West Union #\u0026gt; x_1 y_1 grouped_pop #\u0026gt; 1 -83.54616 38.79483 0-20 #\u0026gt; 2 -83.54616 38.79483 0-20 #\u0026gt; 3 -83.54616 38.79483 0-20 #\u0026gt; 4 -83.54616 38.79483 0-20 #\u0026gt; 5 -83.54616 38.79483 0-20 #\u0026gt; 6 -83.54616 38.79483 0-20    Plot the map:  ggplot() + geom_polygon(data = ohio_pop, aes(x = long, y = lat, group = group, fill = grouped_pop), color = \"black\") + coord_fixed(1.3) + scale_fill_brewer(palette = \"Reds\", direction = 1) + labs(fill = \"Population Quantiles\")   Breakout rooms Exercise 1 Use the dataset of 2021 Ohio county\u0026rsquo;s population to plot counties with % positive population growth.\n  Solution (click here)  # Get basic map data for all USA counties: usa_counties = map_data(\"county\") # Subset to counties in Ohio: oh = subset(usa_counties, region == \"ohio\") # Read population data: Ohio \u0026lt;- read_csv(\"Ohio.csv\") # Create a new column called \"county\" so that counties start with capital # letters using str_to_title function  oh$county = str_to_title(oh$subregion) # Merge counties with population: ohio_pop\u0026lt;-inner_join(oh, Ohio, by = \"county\") # Select counties with % positive population growth: ohio_pos_pop \u0026lt;- filter(ohio_pop, Perc \u0026gt; 0) ggplot(data = ohio_pop, mapping = aes(x = long, y = lat, group = group)) + geom_polygon(color = \"black\", fill = \"white\") + geom_point(data = ohio_pos_pop, color = \"red\", aes(x = x_1, y = y_1, group = NULL, size = Perc)) + coord_fixed(1.3) + guides(size = FALSE) # Omit the legend     Exercise 2 Use the same data to plot counties with % negative population growth with quantile of 0-20, 20-40, 40-60, 60-80, and 80-100.\n  Solution (click here)  ohio_neg_pop \u0026lt;- filter(ohio_pop, Perc\u0026lt;0) ggplot(data = ohio_pop, mapping = aes(x= long, y= lat, group = group))+ geom_polygon(color=\"black\",fill=\"white\")+ geom_point(data = ohio_neg_pop, color = \"red\", aes(x = x_1,y = y_1, group = NULL, size = Perc)) + coord_fixed(1.3) + guides(size = FALSE) # Omit the legend   ApplyQuintiles_n \u0026lt;- function(x) \u0026#123; cut(x, breaks = c(quantile(ohio_neg_pop$Perc, probs = seq(0, 1, by = 0.2))), labels = c(\"0-20\", \"20-40\", \"40-60\", \"60-80\", \"80-100\"), include.lowest = TRUE) \u0026#125; ohio_neg_pop$grouped_pop \u0026lt;- sapply(ohio_neg_pop$Perc, ApplyQuintiles_n) # Plot the map ggplot() + geom_polygon(data = ohio_neg_pop, aes(x = long, y = lat, group = group, fill = grouped_pop), color = \"black\") + coord_fixed(1.3) + scale_fill_brewer(palette = \"Set1\", direction = -1) + labs(fill = \"Negative population growth counties\")     Bonus exercise Plot the cities of France with population greater than 100,000. Vary size of point by city size, and vary the color of the dots.\n  Solution (click here)  world \u0026lt;- map_data(\"world\") france \u0026lt;- filter(world,region == \"France\") ggplot(data = france, mapping = aes(x = long, y = lat, group = group)) + geom_polygon(color = \"black\", fill = \"white\") + labs(fill = \"France\")  # The \"maps\" package has city data head(maps::world.cities) #\u0026gt; name country.etc pop lat long capital #\u0026gt; 1 'Abasan al-Jadidah Palestine 5629 31.31 34.34 0 #\u0026gt; 2 'Abasan al-Kabirah Palestine 18999 31.32 34.35 0 #\u0026gt; 3 'Abdul Hakim Pakistan 47788 30.55 72.11 0 #\u0026gt; 4 'Abdullah-as-Salam Kuwait 21817 29.36 47.98 0 #\u0026gt; 5 'Abud Palestine 2456 32.03 35.07 0 #\u0026gt; 6 'Abwein Palestine 3434 32.03 35.20 0 my_cities \u0026lt;-maps::world.cities france_cities \u0026lt;- filter(my_cities, country.etc == \"France\") head(france_cities) #\u0026gt; name country.etc pop lat long capital #\u0026gt; 1 Abbeville France 26656 50.12 1.83 0 #\u0026gt; 2 Acheres France 23219 48.97 2.06 0 #\u0026gt; 3 Agde France 23477 43.33 3.46 0 #\u0026gt; 4 Agen France 34742 44.20 0.62 0 #\u0026gt; 5 Aire-sur-la-Lys France 10470 50.64 2.39 0 #\u0026gt; 6 Aix-en-Provence France 148622 43.53 5.44 0 # Make a point for every city: ggplot(data = france, mapping = aes(x = long, y = lat, group = group)) + geom_polygon(color = \"black\", fill = \"white\") + geom_point(data = france_cities, color = \"red\", aes(x = long, y = lat, group = NULL))   # Let's pick just the big cities: france_big_cities \u0026lt;- filter(my_cities,country.etc == \"France\" \u0026amp; pop \u0026gt; 100000) ggplot(data = france, mapping = aes(x = long, y = lat, group = group)) + geom_polygon(color = \"black\", fill = \"white\") + geom_point(data = france_big_cities, color = \"red\", aes(x = long, y = lat, group = NULL))   # vary size of point by city size ggplot(data = france, mapping = aes(x = long, y = lat, group = group)) + geom_polygon(color = \"black\", fill = \"white\") + geom_point(data = france_big_cities, color = \"red\", aes(x = long, y = lat, group = NULL, size = pop))   # Now vary the color of the dots france_big_cities$qual \u0026lt;- sample(LETTERS[1:5], nrow(france_big_cities), replace = TRUE) ggplot(data = france, mapping = aes(x = long, y = lat, group = group)) + geom_polygon(color = \"black\",fill = \"white\") + geom_point(data = france_big_cities, aes(x = long, y = lat, group = NULL, color = qual, size = pop))   # Do some tweaking: # no scientific notation in legend r ggplot # scales package adds the \"scale_size_continuous\" function to our arsenal, and we can set label=comma library(scales) # Change the column name to make the legend nicer: france_big_cities$Population \u0026lt;- france_big_cities$pop france_big_cities$Qualitative \u0026lt;- france_big_cities$qual # Do some additional refining: ggplot(data = france, mapping = aes(x = long, y = lat, group = group)) + geom_polygon(color = \"black\", fill = \"white\") + geom_point(data = france_big_cities, aes(x = long, y = lat, group = NULL, color = Qualitative, size = Population)) + scale_size_continuous(label = comma)     ","date":1614124800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614293945,"objectID":"29ed38d8f1680daed1e667b71019651a","permalink":"https://biodash.github.io/codeclub/11_ggplot-maps/","publishdate":"2021-02-24T00:00:00Z","relpermalink":"/codeclub/11_ggplot-maps/","section":"codeclub","summary":"Today, we will be making cool maps in R!","tags":null,"title":"Session 11: Spatial data visualization with ggplot2","type":"codeclub"},{"authors":["Jessica Cooperstone"],"categories":null,"content":"\n Prep homework Basic computer setup   If you didn\u0026rsquo;t already do this, please follow the Code Club Computer Setup instructions, which also has pointers for if you\u0026rsquo;re new to R or RStudio.\n  If you\u0026rsquo;re able to do so, please open RStudio a bit before Code Club starts \u0026ndash; and in case you run into issues, please join the Zoom call early and we\u0026rsquo;ll help you troubleshoot.\n  New to ggplot? Check out the two Code Club pages for Session 4 and Session 5.\nIf you\u0026rsquo;ve never used ggplot2 before (or even if you have), you may find this cheat sheet useful.\n Getting Started RMarkdown for today\u0026rsquo;s session # directory for Code Club Session 2: dir.create(\"S10\") # directory for our RMarkdown # (\"recursive\" to create two levels at once.) dir.create(\"S10/Rmd/\") # save the url location for today's script todays_R_script \u0026lt;- 'https://raw.githubusercontent.com/biodash/biodash.github.io/master/content/codeclub/10_faceting-animating/Session10_faceting_animating_multifigs.Rmd' # indicate the name of the new script file Session10_Rmd \u0026lt;- \"S10/Rmd/Session10_faceting_animating_multifigs.Rmd\" # go get that file!  download.file(url = todays_R_script, destfile = Session10_Rmd)   1 - How can I do more with my figures? Artwork by Allison Horst\nSometimes we have so much data that it is difficult to make sense of it if we look at it all at once. One way to get around this is to create facets in your data \u0026ndash; small subplots that help you to see different relationships among different variables in your dataset.\nToday we will be using ggplot2 to make a series of plots that help us better understand the underlying structure in our dataset.\n What will we go over today\nThese functions or packages will help you to get better visualize with your data.\n facet_wrap() and facet_grid()- makes small multiple plots based on some variable. set scales to indicate the linked or not-linked nature of your axes in a faceted plot gghighlight() - allows you to direct focus on a particular portion of your data. patchwork - to compose super easy multi-plot figures gganimate - to make your plots gif!  I will also go over a few tricks along the way.\n   2 - Accessing our data Let\u0026rsquo;s get set up and grab some data so that we can learn more about this world (and ggplot2)\n You can do this locally, or at OSC. You can find instructions if you are having trouble here.  First load your libraries. We are using a lot of new packages today.\nIf you\u0026rsquo;ve never downloaded these packages before, use the chunk below.\ninstall.packages(c(\"gghighlight\", \"gganimate\", \"magick\", \"patchwork\", \"ggrepel\", \"gapminder\"))  Once you have the packages above downloaded, load your libraries.\nlibrary(tidyverse) library(gghighlight) # for bringing attention to certain parts of your plot library(gganimate) # for animating library(magick) # for rendering gifs and saving them library(patchwork) # for making multi-panel plots library(ggrepel) # for getting labels to not be on top of your points # data for today library(gapminder)  Then let\u0026rsquo;s access the dataset gapminder, which is both the name of the package, and the name of the dataset. It contains a subset of data from Gapminder.org, an educational non-profit aimed to fight global misconceptions about statistics of our world.\nFrom Gapminder.org\nLet\u0026rsquo;s look at the data in gapminder.\n# look at the first 6 rows, all columns head(gapminder) #\u0026gt; # A tibble: 6 x 6 #\u0026gt; country continent year lifeExp pop gdpPercap #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 Afghanistan Asia 1952 28.8 8425333 779. #\u0026gt; 2 Afghanistan Asia 1957 30.3 9240934 821. #\u0026gt; 3 Afghanistan Asia 1962 32.0 10267083 853. #\u0026gt; 4 Afghanistan Asia 1967 34.0 11537966 836. #\u0026gt; 5 Afghanistan Asia 1972 36.1 13079460 740. #\u0026gt; 6 Afghanistan Asia 1977 38.4 14880372 786. # check the structure # this tell us what is contained within our df glimpse(gapminder) #\u0026gt; Rows: 1,704 #\u0026gt; Columns: 6 #\u0026gt; $ country \u0026lt;fct\u0026gt; Afghanistan, Afghanistan, Afghanistan, Afghanistan, Afghani… #\u0026gt; $ continent \u0026lt;fct\u0026gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia,… #\u0026gt; $ year \u0026lt;int\u0026gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997,… #\u0026gt; $ lifeExp \u0026lt;dbl\u0026gt; 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.854, 40.… #\u0026gt; $ pop \u0026lt;int\u0026gt; 8425333, 9240934, 10267083, 11537966, 13079460, 14880372, 1… #\u0026gt; $ gdpPercap \u0026lt;dbl\u0026gt; 779.4453, 820.8530, 853.1007, 836.1971, 739.9811, 786.1134,…  This dataset contains the following measurements about the life expectancy, population, and GDP per capita for 142 countries from 1952 to 2007. It includes the following variables:\n country continent year lifeExp pop gdpPercap  Note, this data is already in tidy-style format meaning:\n Each variable must have its own column. Each observation must have its own row. Each value must have its own cell.  Learn more about tidy data here.\nTo make things a bit less complex, let\u0026rsquo;s look at data just from the Americas (i.e., North and South America). To do that, we can use filter() like we learned using dplyr back in Code Club Session 2\n# make a df with data only from the Americas gapminder_americas \u0026lt;- gapminder %\u0026gt;% filter(continent == \"Americas\") # what countries do we have? unique(gapminder_americas$country) #\u0026gt; [1] Argentina Bolivia Brazil  #\u0026gt; [4] Canada Chile Colombia  #\u0026gt; [7] Costa Rica Cuba Dominican Republic  #\u0026gt; [10] Ecuador El Salvador Guatemala  #\u0026gt; [13] Haiti Honduras Jamaica  #\u0026gt; [16] Mexico Nicaragua Panama  #\u0026gt; [19] Paraguay Peru Puerto Rico  #\u0026gt; [22] Trinidad and Tobago United States Uruguay  #\u0026gt; [25] Venezuela  #\u0026gt; 142 Levels: Afghanistan Albania Algeria Angola Argentina Australia ... Zimbabwe   3 - Life expectancy vs. time We will plot the relationship between lifeExp and year with the goal of understanding how life expectancy has changed in the second half of the 20th century. We will use geom_line() to make a line plot.\ngapminder_americas %\u0026gt;% ggplot(aes(x = year, y = lifeExp, group = country, color = country)) + geom_line()   This plot has so many countries, and we can only visually distinguish colors so well, that it makes this a bit of a mess. We can do better!\n 4 - Highlighting What if we want to highlight one country of interest, with the backdrop of all the data in the Americas? We can do this using gghighlight(), which will distinguish our country of interest, from the rest of the countries which will be indicated in gray.\nLet\u0026rsquo;s highlight the United States, and since we are at it, let\u0026rsquo;s also add x and y axis labels, a title, subtitle, and caption with labs().\ngapminder_americas %\u0026gt;% ggplot(aes(x = year, y = lifeExp, group = country, color = country)) + geom_line() + gghighlight(country == \"United States\") + labs(x = \"Year\", y = \"Life Expectancy (years)\", title = \"Life Expectancy in Countries in the Americas\", subtitle = \"From 1952 to 2007\", caption = \"Data from gapminder.org\") #\u0026gt; Warning: Tried to calculate with group_by(), but the calculation failed. #\u0026gt; Falling back to ungrouped filter operation... #\u0026gt; label_key: country    5 - Faceting What if we want to see all the data at once, but just be able to better attribute each line to the correct country? We can use the principle of small multiples, popularized by Edward Tufte, to make a series of charts all on the same scale to allow comparison between them easily.\nWe can facet using facet_wrap to create small plots for each country. If you want a certain number of rows or columns you can indicate them by including ncol and nrow in the facet_wrap() statement.\nI have also made the strip text, or the label on top of each of the facets bigger using theme.\ngapminder_americas %\u0026gt;% ggplot(aes(x = year, y = lifeExp, color = country)) + geom_line() + theme(strip.text.x = element_text(size = 14)) + facet_wrap(vars(country)) + # facet_wrap(~country) also works labs(x = \"Year\", y = \"Life Expectancy (years)\", title = \"Life Expectancy in Countries in the Americas\", subtitle = \"From 1952 to 2007\", caption = \"Data from gapminder.org\")   Now our legend is not necessary, so let\u0026rsquo;s remove it. Let\u0026rsquo;s also remove the gray background since its not really doing much for us. We will also change to theme_minimal() to get rid of the grey background which I don\u0026rsquo;t think we need.\ngapminder_americas %\u0026gt;% ggplot(aes(x = year, y = lifeExp)) + geom_line(aes(color = country)) + theme_minimal() + theme(legend.position = \"none\", strip.text.x = element_text(size = 14)) + facet_wrap(~country) + labs(x = \"Year\", y = \"Life Expectancy (years)\", title = \"Life Expectancy in Countries in the Americas\", subtitle = \"From 1952 to 2007\", caption = \"Data from gapminder.org\")   Wow better! But now its a bit hard to contextualize the line for each country to the whole dataset. We can fix this too.\n 6 - Highlighting plus faceting Let\u0026rsquo;s bring the rest of data back in, and highlight in each facet the country of interest. We can do this by just adding gghighlight() to our ggplot call.\nNote: if you want to assign something in R to an object, and then view it, you can put the whole thing in parentheses, without having to call that object back at the end.\n(americas_lifeexp \u0026lt;- gapminder_americas %\u0026gt;% ggplot(aes(x = year, y = lifeExp)) + geom_line(aes(color = country)) + gghighlight() + theme_minimal() + theme(legend.position = \"none\", strip.text.x = element_text(size = 14)) + facet_wrap(~country) + labs(x = \"Year\", y = \"Life Expectancy (years)\", title = \"Life Expectancy in Countries in the Americas\", subtitle = \"From 1952 to 2007\", caption = \"Data from gapminder.org\")) #\u0026gt; label_key: country #\u0026gt; Too many data series, skip labeling   Wow, we now have so much more information about our data!\n 7 - Adjusting scales while faceting The default in faceting is that the x and y-axes for each plot are all the same. This aids in the interpretation of each small plot in relation to the others, but sometimes you may want freedom to adjust your axes.\nFor example, if we wanted to plot population over time, if we used the same scale, it would be really hard to see trends within a country.\n(americas_pop \u0026lt;- gapminder_americas %\u0026gt;% ggplot(aes(x = year, y = pop)) + geom_line(aes(color = country)) + theme_minimal() + theme(legend.position = \"none\", strip.text.x = element_text(size = 14)) + facet_wrap(~country) + labs(x = \"Year\", y = \"Population\", title = \"Population in Countries in the Americas\", subtitle = \"From 1952 to 2007\", caption = \"Data from gapminder.org\"))   Let\u0026rsquo;s change the scales so that the y-axis is \u0026ldquo;free\u0026rdquo; - i.e., each plot will have an independent y-axis. Note, when you do this, you aren\u0026rsquo;t really using the principle of small multiples anymore, since the data isn\u0026rsquo;t all on comparable scales.\ngapminder_americas %\u0026gt;% ggplot(aes(x = year, y = pop)) + geom_line(aes(color = country)) + theme_minimal() + theme(legend.position = \"none\", strip.text.x = element_text(size = 14)) + facet_wrap(~country, scales = \"free_y\") + labs(x = \"Year\", y = \"Population\", title = \"Population of Countries in the Americas\", subtitle = \"From 1952 to 2007\", caption = \"Data from gapminder.org\")   The default for scales is \u0026quot;fixed\u0026quot;, but you can also set to be \u0026quot;free_x\u0026quot;, \u0026quot;free_y\u0026quot;, or \u0026quot;free\u0026quot;, which means both x and y are free.\n 8 - Multi-panel plots What if I take plots I\u0026rsquo;ve already made and assemble them together? You can do that simply with the package patchwork().\nArtwork by Allison Horst\nYou can use the syntax:\n plot1 + plot2 to get two plots next to each other plot1 / plot2 to get two plots stacked vertically plot1 | (plot2 + plot3) to get plot1 in the first row, and plots 2 and 3 in a second row  You can use plot_annotation() to indicate your plots with letters or numbers.\nI am going to make some quick plots so we can see how it works. Let\u0026rsquo;s look at some plots of the United States.\n# make df with just United States data gapminder_usa \u0026lt;- gapminder %\u0026gt;% filter(country == \"United States\") # make some plots (usa_lifeexp \u0026lt;- gapminder_usa %\u0026gt;% ggplot(aes(x = year, y = lifeExp)) + geom_point())   (usa_gdppercap \u0026lt;- gapminder_usa %\u0026gt;% ggplot(aes(x = year, y = gdpPercap)) + geom_line())   (usa_pop \u0026lt;- gapminder_usa %\u0026gt;% ggplot(aes(x = year, y = pop)) + geom_col())   Let\u0026rsquo;s make multi-panel plots. If you need to wrap around a line, make sure you don\u0026rsquo;t start your line with the +, it won\u0026rsquo;t work (this is true for all ggplot2 syntax).\n(usa_lifeexp + usa_gdppercap) / usa_pop + plot_annotation(title = \"Some plots about the United States\", tag_levels = \"A\")   You can see how this would be really useful for publications!\n 9 - Animating Artwork by Allison Horst\nSince we have time-scale data here, we could also build an animation that would help us look at our data. What if we wanted to look at how life expectancy (lifeExp) and population (pop) change over time? We could animate over the variable year, and do this by using the function animate(), and set transition_states() to the variable we are giffing over.\nNote, I have included closest_state in the subtitle so the viewer can see what is the year at any stage of the animation.\nTo be able to tell which dot belongs to which country, I added a geom_text_repel() statement, which labels each point but is smart enough to not let the labels overlap.\nI have also set pop to be on a log10 scale.\nIf you want to increase the resolution of your gif, and set the code chunk to cache = TRUE if the chunk runs slowly, so that it doesn\u0026rsquo;t re-run when knitting if nothing has been edited, you can do this in the curly brackets at the top of your chunk, like this:\n{r, cache = TRUE, dpi = 600}\n# install.packages(\"transformr\")  # if you are having problems with gganimate you may need to install transformr (p \u0026lt;- ggplot(gapminder_americas, aes(x = lifeExp, y = pop, fill = country, label = country)) + geom_point(shape = 21, color = \"black\") + geom_text_repel() + scale_y_log10() + theme_classic() + theme(legend.position = 'none') + labs(title = \"Population and Life Expectancy in the Americas\", subtitle = 'Year: \u0026#123;closest_state\u0026#125;', x = \"Life Expectancy\", y = \"Log10 Population\") + transition_states(year))  There are many different ways to transition your data in gganimate - and you can learn more about them here.\nSaving my gif\nNow I want to save my gif. We can do that simply with the function anim_save() which works a lot like ggsave().\n# set parameters for your animation animate(p, duration = 10, fps = 10, width = 700, height = 700, renderer = magick_renderer()) # save it anim_save(filename = \"gapminder_gif.gif\", animation = last_animation(), path = \"/Users/jessicacooperstoneimac\")   10 - Breakout rooms Loading data and get set up Load the palmerpenguins dataset, look at its structure, and view the beginning of the df.\nlibrary(palmerpenguins) str(penguins) #\u0026gt; tibble [344 × 8] (S3: tbl_df/tbl/data.frame) #\u0026gt; $ species : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ... #\u0026gt; $ island : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 3 3 ... #\u0026gt; $ bill_length_mm : num [1:344] 39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ... #\u0026gt; $ bill_depth_mm : num [1:344] 18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ... #\u0026gt; $ flipper_length_mm: int [1:344] 181 186 195 NA 193 190 181 195 193 190 ... #\u0026gt; $ body_mass_g : int [1:344] 3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ... #\u0026gt; $ sex : Factor w/ 2 levels \"female\",\"male\": 2 1 1 NA 1 2 1 2 NA NA ... #\u0026gt; $ year : int [1:344] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ... head(penguins) #\u0026gt; # A tibble: 6 x 8 #\u0026gt; species island bill_length_mm bill_depth_mm flipper_length_… body_mass_g sex  #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; #\u0026gt; 1 Adelie Torge… 39.1 18.7 181 3750 male  #\u0026gt; 2 Adelie Torge… 39.5 17.4 186 3800 fema… #\u0026gt; 3 Adelie Torge… 40.3 18 195 3250 fema… #\u0026gt; 4 Adelie Torge… NA NA NA NA NA  #\u0026gt; 5 Adelie Torge… 36.7 19.3 193 3450 fema… #\u0026gt; 6 Adelie Torge… 39.3 20.6 190 3650 male  #\u0026gt; # … with 1 more variable: year \u0026lt;int\u0026gt;   Main exercises Exercise 1  Like we did in Code Club 7, convert the two columns about penguin bill dimensions bill_length_mm and bill_depth_mm to two columns called bill_dimension and value. Drop your NAs also. Save this as a new df called penguins_long.\n  Hints (click here)  Use a combination of drop_na() and pivot_longer(), and it\u0026rsquo;s helpful if you also set names_prefix in your pivot_longer() statement but not totally necessary.    Solutions (click here)  penguins_long \u0026lt;- penguins %\u0026gt;% drop_na() %\u0026gt;% pivot_longer(cols = bill_length_mm:bill_depth_mm, names_to = \"bill_dimension\", values_to = \"value_mm\", names_prefix = \"bill_\") head(penguins_long) #\u0026gt; # A tibble: 6 x 8 #\u0026gt; species island flipper_length_… body_mass_g sex year bill_dimension #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt;  #\u0026gt; 1 Adelie Torge… 181 3750 male 2007 length_mm  #\u0026gt; 2 Adelie Torge… 181 3750 male 2007 depth_mm  #\u0026gt; 3 Adelie Torge… 186 3800 fema… 2007 length_mm  #\u0026gt; 4 Adelie Torge… 186 3800 fema… 2007 depth_mm  #\u0026gt; 5 Adelie Torge… 195 3250 fema… 2007 length_mm  #\u0026gt; 6 Adelie Torge… 195 3250 fema… 2007 depth_mm  #\u0026gt; # … with 1 more variable: value_mm \u0026lt;dbl\u0026gt;      Exercise 2  Plot body mass (body_mass_g) as related to bill length and depth.\n  Hints (click here)  Faceting will be useful here.    Solutions (click here)  penguins_long %\u0026gt;% ggplot(aes(x = body_mass_g, y = value_mm)) + geom_point() + facet_wrap(vars(bill_dimension))       Exercise 3  Take your plot from Exercise 2 and make it prettier. You can do things like change your axis labels, add title, change themes as you see fit. Color your points by sex.\n  Hints (click here)  Pick a theme you like. theme_classic() is a good place to start, and if you want to download the package hrbrthemes, I really like the theme_ipsum_rc().\n   Solutions (click here)  I\u0026rsquo;ve included some code that let\u0026rsquo;s you re-name the strip text, or the text that is above each of your facets. You do this with the labeller() function.\nlibrary(hrbrthemes) # for pretty \u0026amp; easy themes # formatting facet strip text labels dim_mm \u0026lt;- c(\"Culman Bill Depth\", \"Culman Bill Length\") names(dim_mm) \u0026lt;- c(\"depth_mm\", \"length_mm\") # this is just one example penguins_long %\u0026gt;% ggplot(aes(x = body_mass_g, y = value_mm, color = sex)) + geom_point() + theme_ipsum_rc() + theme(axis.title.x = element_text(hjust = 0.5), axis.title.y = element_text(hjust = 0.5), strip.text = element_text(hjust = 0.5)) + labs(x = \"Body Mass (g)\", y = \"mm\", title = \"Bill length and depth vs. body mass in penguins\", color = \"Sex\", caption = \"Data from https://allisonhorst.github.io/palmerpenguins/\") + facet_wrap(vars(bill_dimension), labeller = labeller(bill_dimension = dim_mm))       Exercise 4  Add a second dimension of faceting by species.\n  Hints (click here)  You do this within your facet_wrap() call. You might want to try the formula syntax, which works like this: var1~var2.    Solutions (click here)  penguins_long %\u0026gt;% ggplot(aes(x = body_mass_g, y = value_mm, color = sex)) + geom_point() + theme_ipsum_rc() + theme(axis.title.x = element_text(hjust = 0.5), axis.title.y = element_text(hjust = 0.5), strip.text = element_text(hjust = 0.5)) + labs(x = \"Body Mass (g)\", y = \"mm\", title = \"Bill length and depth vs. body mass in penguins\", color = \"Sex\", caption = \"Data from https://allisonhorst.github.io/palmerpenguins/\") + facet_wrap(bill_dimension~species, labeller = labeller(bill_dimension = dim_mm))       Exercise 5  Using your plot from Exercise 3, highlight the datapoints coming from Dream Island in purple.\n  Hints (click here)  You can use syntax inside gghighlight() just like you do in filter().    Solutions (click here)  # what are our islands? unique(penguins_long$island) #\u0026gt; [1] Torgersen Biscoe Dream  #\u0026gt; Levels: Biscoe Dream Torgersen penguins_long %\u0026gt;% ggplot(aes(x = body_mass_g, y = value_mm)) + geom_point(color = \"purple\") + gghighlight(island == \"Dream\") + facet_wrap(vars(bill_dimension))       Exercise 6  Using your sample plot for Exercise 3, highlight penguins that have a body_mass_g less than 3500 g, in blue.\n  Hints (click here)  You can use syntax inside gghighlight() just like you do in filter(), and you can also use these filter functions like \u0026lt;, \u0026gt;, \u0026lt;=, ! and AND inside your call.    Solutions (click here)  # what are our islands? unique(penguins_long$island) #\u0026gt; [1] Torgersen Biscoe Dream  #\u0026gt; Levels: Biscoe Dream Torgersen penguins_long %\u0026gt;% ggplot(aes(x = body_mass_g, y = value_mm)) + geom_point(color = \"blue\") + gghighlight(island == \"Dream\") + facet_wrap(vars(bill_dimension))       Bonus exercises Bonus 1  Plot flipper_length_mm vs. body_mass_g and animate the plot to show only one species at a time.\n  Hints (click here)  Try animating over species, using transition_states() and set {closest_state} in your title or subtitle so you can tell what you\u0026rsquo;re looking at.    Solutions (click here)  flipper_by_BW \u0026lt;- penguins %\u0026gt;% ggplot(aes(x = body_mass_g, y = flipper_length_mm, fill = species)) + geom_point(shape = 21, color = \"black\") + theme_classic() + theme(legend.position = 'none') + labs(title = \"Population and Life Expectancy in the Americas\", subtitle = 'Penguin Species: \u0026#123;closest_state\u0026#125;', x = \"Body Mass (g)\", y = \"Flipper Length (mm)\") + transition_states(species) animate(flipper_by_BW)      Bonus 2  You have now made an excellent gif, so save it!\n  Hints (click here)  Use anim_save() to save your animation, which works in a similar way to ggsave(), which is another very useful function.    Solutions (click here)  # set parameters for your animation animate(flipper_by_BW, duration = 10, fps = 10, width = 700, height = 700, renderer = magick_renderer()) # save it anim_save(filename = \"flippers_by_mass.gif\", animation = last_animation(), path = \"YOUR_PATH_HERE\")      Bonus 3  Let\u0026rsquo;s practice making multi-panel plots. Plot:\nBoxplot of body_mass_g by sex\nHistogram of number of observations per island\nDistribution of flipper_length_mm by species.\nTag your plots so each has a lowercase letter associated with it.\n  Hints (click here)  Use the syntax from the package patchwork. You can learn more here. Also use plot_annotation().    Solutions (click here)   title allows you to set a title tag_levels allows you to determine how you want your panels to be tagged.  Boxplot of body_mass_g by sex.\npenguins_mass_by_sex \u0026lt;- penguins_long %\u0026gt;% ggplot(aes(x = sex, y = body_mass_g)) + geom_boxplot() penguins_mass_by_sex   Histogram of number of observations per island.\npenguins_by_island \u0026lt;- penguins_long %\u0026gt;% ggplot(aes(y = island, fill = island)) + geom_histogram(stat = \"count\") #\u0026gt; Warning: Ignoring unknown parameters: binwidth, bins, pad penguins_by_island   Distribution of flipper_length_mm by species.\npenguins_flipper_species \u0026lt;- penguins_long %\u0026gt;% ggplot(aes(x = flipper_length_mm, group = species, fill = species)) + geom_density(alpha = 0.5) + scale_fill_viridis_d() penguins_flipper_species   penguins_flipper_species / (penguins_mass_by_sex + penguins_by_island) + plot_annotation(title = \"Looking at penguins...\", tag_levels = \"a\")       ","date":1613520000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613604588,"objectID":"3abf1bebd3e7269759e016e707cae871","permalink":"https://biodash.github.io/codeclub/10_faceting-animating/","publishdate":"2021-02-17T00:00:00Z","relpermalink":"/codeclub/10_faceting-animating/","section":"codeclub","summary":"During this tenth session of Code Club, we will be continuing to work on making visualizations using ggplot2, including faceting plots, making multi-panel figures, and animating charts.","tags":null,"title":"Session 10: Faceting, multi-plots, and animating","type":"codeclub"},{"authors":["Michael Broe"],"categories":null,"content":"  Image from https://r4ds.had.co.nz    New To Code Club?   First, check out the Code Club Computer Setup instructions, which also has some pointers that might be helpful if you\u0026rsquo;re new to R or RStudio.\n  Please open RStudio before Code Club to test things out \u0026ndash; if you run into issues, join the Zoom call early and we\u0026rsquo;ll troubleshoot.\n   Session Goals  Learn the uses of R\u0026rsquo;s three basic subsetting operators: [ ], [[ ]], and $. Learn how the behavior of these operators varies depending on the data structure you are subsetting (vector, matrix, list, or data frame). Prepare to learn how these resemble, and differ from, subsetting operators in Python.   Intro: What is \u0026lsquo;subsetting\u0026rsquo; anyway? Subsetting (also known as indexing) is simply a formal way of pulling out specific pieces of a data structure. We\u0026rsquo;ve already seen two dplyr verbs that perform this kind of operation for tibbles: filter (to pull out specific rows) and select (to pull out specific columns).\nBut these are tidyverse commands, and only work with tibbles. R has two more-basic data structures, vectors and lists, and for these we need different subsetting operators. We\u0026rsquo;ll also see that matrices are simply a special kind of vector, that data frames are a special kind of list, and basic subsetting operators also work for these.\nSince the behavior of these operators depends on the actual data structure you are working with, it\u0026rsquo;s useful when experimenting to use them in conjunction with the str() function, which compactly displays the internal structure of an arbitrary R object. A knowledge of the make-up of these data structures is also important when you come to write your own loops, iterations, and functions.\nThe most important distinction between vectors and lists is that vectors are homogeneous, while lists can be heterogeneous.\n Terminological note: \u0026lsquo;under-the-hood\u0026rsquo; both of these are vectors in the technical sense, and sometimes the distinction is referred to as atomic vectors versus recursive vectors. I\u0026rsquo;ll continue to use just \u0026lsquo;vector\u0026rsquo; and \u0026lsquo;list\u0026rsquo; here. This usage also lines-up with Python.\n   Vectors A vector is absolutely the most basic data structure in R. Every value in a vector must be of the same type. Strikingly, this sets R apart from Python. Using this kind of vector in Python requires loading a whole separate package: numpy. The most basic data structure in Python is the list.\nThere are four basic types of vector: integer, double, character, and logical. Vectors are created by hand with the c() (combine, concatenate) function. We can check the type with the typeof() operator. This is totally redundant if you just created the vector yourself, but when you are debugging code or creating a vector using an expression you might want to check exactly what type of vector is being used:\nvec_dbl \u0026lt;- c(1, 2, 3, 4, 5) typeof(vec_dbl) #\u0026gt; [1] \"double\"   vec_int \u0026lt;- c(1L, 2L, 3L, 4L, 5L) typeof(vec_int) #\u0026gt; [1] \"integer\"   vec_which \u0026lt;- seq(1, 10) typeof(vec_which) #\u0026gt; [1] \"integer\"   vec_which2 \u0026lt;- 1:10 typeof(vec_which2) #\u0026gt; [1] \"integer\"   vec_log \u0026lt;- c(TRUE, TRUE, FALSE, TRUE, FALSE) typeof(vec_log) #\u0026gt; [1] \"logical\"   vec_chr \u0026lt;- c(\"a\", \"b\", \"c\", \"d\", \"e\") typeof(vec_chr) #\u0026gt; [1] \"character\"   What happens when we perform a basic mathematical operation on a vector?\n2 * vec_int #\u0026gt; [1] 2 4 6 8 10   (This is completely different than what you obtain when multiplying a Python list).\nL = [1, 2, 3, 4, 5] 2 * L # [1, 2, 3, 4, 5, 1, 2, 3, 4, 5] So it\u0026rsquo;s not just that vectors are a basic R data structure, but R is a vectorised language. In many cases applying an operation to a vector automagically applies the operation to every element in the vector. This means that for many basic operations for loops and mapping functions, necessary in Python, are not needed in R (although if you write your own functions you will need these iteration tools). In Python we could use a \u0026lsquo;list comprehension\u0026rsquo; (a compact and fast version of a for loop):\n[2 * i for i in L] # [2, 4, 6, 8, 10] Or install the numpy package that makes vectors and vectorized functions available.\nVectors have an insanely simple structure:\nstr(vec_dbl) #\u0026gt; num [1:5] 1 2 3 4 5   str() also displays the type, and RStudio displays the result of str() in the Values pane.\nFor such a simple structure, there are a surprisingly large number of ways to subset a vector. We\u0026rsquo;ll use the following example:\nx \u0026lt;- c(2.1, 4.2, 3.3, 5.4)   (Notice that the number after the decimal point indicates the position (index) of the element of the vector.)\nPositive integers return elements at the specified positions. Any expression that evaluates to a vector of positive integers can be used as the index. The index operator is []:\nx[3] #\u0026gt; [1] 3.3 x[c(3, 1)] #\u0026gt; [1] 3.3 2.1 x[2:4] #\u0026gt; [1] 4.2 3.3 5.4 x[seq(1, 4, 2)] #\u0026gt; [1] 2.1 3.3   (In R the indices run from 1 to the length of the object: in Python indices run from 0 to length-1).\nNegative integers exclude elements at the specified positions:\nx[-3] #\u0026gt; [1] 2.1 4.2 5.4 x[-c(3, 1)] #\u0026gt; [1] 4.2 5.4 #\u0026gt; [1] 4.2 5.4   Logical vectors select elements where the corresponding logical value is TRUE. This is most useful if you can write a comparison expression 2 \u0026gt; 3, 4 == 4, that returns TRUE (or FALSE) for each element of the vector:\nx[c(TRUE, TRUE, FALSE, FALSE)] #\u0026gt; [1] 2.1 4.2 x[x \u0026gt; 3] #\u0026gt; [1] 4.2 3.3 5.4   Attributes. One of the unusual features of R as opposed to Python is that you can assign metadata of various kinds to the elements of vectors (and lists). For example, we can assign a name to each element, and then use a character vector as the index expression:\ny \u0026lt;- c(a = 2.1, b = 4.2, c = 3.3, d = 5.4) str(y) #\u0026gt; Named num [1:4] 2.1 4.2 3.3 5.4 #\u0026gt; - attr(*, \"names\")= chr [1:4] \"a\" \"b\" \"c\" \"d\" y[c(\"d\", \"c\", \"a\")] #\u0026gt; d c a  #\u0026gt; 5.4 3.3 2.1   (Again, Python has no direct equivalent of this, but we can get a similar effect using a dictionary data structure, which explicitly assigns a name to each value).\nNothing ([]) returns the entire vector:\nx[] #\u0026gt; [1] 2.1 4.2 3.3 5.4   This is not useful for (one dimensional) vectors, but is behind the notation for extracting rows and columns from matrices. Keep in mind a \u0026ldquo;nothing\u0026rdquo; index returns \u0026ldquo;everything\u0026rdquo;.\nMatrices A matrix is simply a vector with a dimensions attribute. Here we convert a vector to a two-dimensional matrix, with two rows and three columns, with dim(rows, cols).\nz \u0026lt;- c(2.1, 4.2, 3.3, 5.4, 8.5, 1.6) dim(z) \u0026lt;- c(2, 3) z #\u0026gt; [,1] [,2] [,3] #\u0026gt; [1,] 2.1 3.3 8.5 #\u0026gt; [2,] 4.2 5.4 1.6   Now we can index a specific value using comma notation, where the first index specifies the row, and the second index the column (in Python this is reversed):\nz[2,3] #\u0026gt; [1] 1.6   And in two dimensions the nothing after the , returns every column for that row:\nz[2,] #\u0026gt; [1] 4.2 5.4 1.6   And here is a way of selecting a submatrix (every row for all columns except the first):\nz[,-1] #\u0026gt; [,1] [,2] #\u0026gt; [1,] 3.3 8.5 #\u0026gt; [2,] 5.4 1.6   Lists There are two main differences between vectors and lists: (i) lists can contain elements of different types; and (ii) lists can contain other lists. This is why lists are sometimes referred to as recursive vectors. We will see examples of these below, but first let\u0026rsquo;s directly compare a list of numbers to a vector of numbers, and examine the structure. Lists are constructed with the list() function.\nl \u0026lt;- list(2.1, 4.2, 3.3, 5.4) l #\u0026gt; [[1]] #\u0026gt; [1] 2.1 #\u0026gt;  #\u0026gt; [[2]] #\u0026gt; [1] 4.2 #\u0026gt;  #\u0026gt; [[3]] #\u0026gt; [1] 3.3 #\u0026gt;  #\u0026gt; [[4]] #\u0026gt; [1] 5.4 str(l) #\u0026gt; List of 4 #\u0026gt; $ : num 2.1 #\u0026gt; $ : num 4.2 #\u0026gt; $ : num 3.3 #\u0026gt; $ : num 5.4   Here we see the appearance of a new subsetting operator [[ ]]. What does it yield?\nll_2 \u0026lt;- l[[2]] ll_2 #\u0026gt; [1] 4.2 typeof(ll_2) #\u0026gt; [1] \"double\"   Now compare this to the result of using the [ ] operator:\nl_2 \u0026lt;- l[2] l_2 #\u0026gt; [[1]] #\u0026gt; [1] 4.2 typeof(l_2) #\u0026gt; [1] \"list\" str(l_2) #\u0026gt; List of 1 #\u0026gt; $ : num 4.2   The behavior of the [ ] operator is very different for lists: it selects the element(s) you request, but it always returns a subsetted version of the original list. It \u0026lsquo;shrinks\u0026rsquo; the original list. There is nothing like this in Python; it\u0026rsquo;s quite unique to R. (The reason this is the case will become clear when we examine data frames.) The [[ ]] operator on the other hand just returns the object you select.\nAs mentioned above, it\u0026rsquo;s quite possible that an element of a list might itself be a list:\nm \u0026lt;- list(2.1, list(4.21, 4.22), 3.3, 5.4) m #\u0026gt; [[1]] #\u0026gt; [1] 2.1 #\u0026gt;  #\u0026gt; [[2]] #\u0026gt; [[2]][[1]] #\u0026gt; [1] 4.21 #\u0026gt;  #\u0026gt; [[2]][[2]] #\u0026gt; [1] 4.22 #\u0026gt;  #\u0026gt;  #\u0026gt; [[3]] #\u0026gt; [1] 3.3 #\u0026gt;  #\u0026gt; [[4]] #\u0026gt; [1] 5.4   This (print) output focuses on content, whereas the str() function focuses on structure, and is very useful for nested lists:\nstr(m) #\u0026gt; List of 4 #\u0026gt; $ : num 2.1 #\u0026gt; $ :List of 2 #\u0026gt; ..$ : num 4.21 #\u0026gt; ..$ : num 4.22 #\u0026gt; $ : num 3.3 #\u0026gt; $ : num 5.4   Once we combine nested lists and multiple types, things can get pretty hairy. There are various ways to visualize what\u0026rsquo;s going on. Here is one example:\nx1 \u0026lt;- list(c(1, 2), c(3, 4)) x2 \u0026lt;- list(list(1, 2), list(3, 4)) x3 \u0026lt;- list(1, list(2, list(3)))   However the printed form provides us a clue on how to extract an individual element from inside a nested list:\n# m \u0026lt;- list(2.1, list(4.21, 4.22), 3.3, 5.4) m[[2]] #\u0026gt; [[1]] #\u0026gt; [1] 4.21 #\u0026gt;  #\u0026gt; [[2]] #\u0026gt; [1] 4.22   m[[2]][[1]] #\u0026gt; [1] 4.21   In short, [[ ]] drills down into a list, while [ ] returns a diminished version of the original list.\nHere\u0026rsquo;s a visualization of various list subsetting operations:\na \u0026lt;- list(1:3, \"a string\", pi, list(-1, -5)) a #\u0026gt; [[1]] #\u0026gt; [1] 1 2 3 #\u0026gt;  #\u0026gt; [[2]] #\u0026gt; [1] \"a string\" #\u0026gt;  #\u0026gt; [[3]] #\u0026gt; [1] 3.141593 #\u0026gt;  #\u0026gt; [[4]] #\u0026gt; [[4]][[1]] #\u0026gt; [1] -1 #\u0026gt;  #\u0026gt; [[4]][[2]] #\u0026gt; [1] -5 str(a) #\u0026gt; List of 4 #\u0026gt; $ : int [1:3] 1 2 3 #\u0026gt; $ : chr \"a string\" #\u0026gt; $ : num 3.14 #\u0026gt; $ :List of 2 #\u0026gt; ..$ : num -1 #\u0026gt; ..$ : num -5   Subsetting a list, visually. Here is a recursive pepper shaker, p\nHere is the first packet, but still inside the shaker, p[1]\nHere is the first packet by itself, p[[1]]\nAnd here is the contents of that packet, p[[1]][[1]]\nWe\u0026rsquo;ll play with yet another type of visualization in the exercises.\nData frames Let\u0026rsquo;s look at a simple data frame:\ndf \u0026lt;- data.frame(x = 1:3, y = c(\"a\", \"b\", \"c\")) typeof(df) #\u0026gt; [1] \"list\" str(df) #\u0026gt; 'data.frame': 3 obs. of 2 variables: #\u0026gt; $ x: int 1 2 3 #\u0026gt; $ y: chr \"a\" \"b\" \"c\"   So a data frame is basically a list (of columns), with a names attribute for the column names; and with the extra condition that all the columns are of the same length. So we should be able to use our standard list subsetting operators on it:\ndf_col_1 \u0026lt;- df[1] str(df_col_1) #\u0026gt; 'data.frame': 3 obs. of 1 variable: #\u0026gt; $ x: int 1 2 3   Since a data frame is a list, subsetting using [ ] returns the specified column still inside a data frame. What about [[ ]]?\ncol_1 \u0026lt;- df[[1]] str(col_1) #\u0026gt; int [1:3] 1 2 3   Using [[ ]] returns the individual column.\nWe can also subset a data frame by name:\ndf_name \u0026lt;- df[\"x\"] str(df_name) #\u0026gt; 'data.frame': 3 obs. of 1 variable: #\u0026gt; $ x: int 1 2 3   df_nname \u0026lt;- df[[\"x\"]] str(df_nname) #\u0026gt; int [1:3] 1 2 3   Finally df$x is simply a shorthand for df[[\u0026quot;x\u0026quot;]] without the [[ ]] and the \u0026quot; \u0026quot;:\ndf_dollar_name \u0026lt;- df$x str(df_dollar_name) #\u0026gt; int [1:3] 1 2 3   Just as a matter of interest, in the grand scheme of things lists are just a special kind of vector (a \u0026lsquo;heterogeneous recursive vector\u0026rsquo;), so it should be possible to stick a list column into a data frame. We can, but we have to use the I \u0026lsquo;identity\u0026rsquo; operator around the list:\ndf_mixed \u0026lt;- data.frame( x = 1:3, y = I(list(4L, 7.2, \"string\"))) str(df_mixed) #\u0026gt; 'data.frame': 3 obs. of 2 variables: #\u0026gt; $ x: int 1 2 3 #\u0026gt; $ y:List of 3 #\u0026gt; ..$ : int 4 #\u0026gt; ..$ : num 7.2 #\u0026gt; ..$ : chr \"string\" #\u0026gt; ..- attr(*, \"class\")= chr \"AsIs\"   Further reading and acknowledgement For more details on subsetting see R for Data Science and Advanced R both by Hadley Wickham, from which much of the material in this module was borrowed.\n Exercise 1 A surprisingly useful operator for extracting elements of a numerical vector is the modulo operator x %% y. This returns the remainder when x is divided by y. It is a vectorized operation, so we can apply it to a list.\nx \u0026lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10) x %% 3 #\u0026gt; [1] 1 2 0 1 2 0 1 2 0 1   Use this operator to extract every third element of the above vector x.\n  Hints (click here)  \nCheck the example in the presentation about selecting elements when the logical comparison is TRUE. What is the logical test we need to identify every third element?    Solution (click here)  x \u0026lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10) x[x %% 3 == 0] #\u0026gt; [1] 3 6 9      Exercise 2 Here is a visualization of a list:\nCreate a list in R called train that captures this structure. Print the list, and also display its structure.\n  Hints (click here)  \nThis list has no nested lists, it\u0026rsquo;s just a list of vectors and individual values.    Solution (click here)  train \u0026lt;- list(1:3, \"a\", 4:6) train #\u0026gt; [[1]] #\u0026gt; [1] 1 2 3 #\u0026gt;  #\u0026gt; [[2]] #\u0026gt; [1] \"a\" #\u0026gt;  #\u0026gt; [[3]] #\u0026gt; [1] 4 5 6 str(train) #\u0026gt; List of 3 #\u0026gt; $ : int [1:3] 1 2 3 #\u0026gt; $ : chr \"a\" #\u0026gt; $ : int [1:3] 4 5 6      Exercise 3 For each of the following sub-trains/carriages, determine the subsetting expression by eye, and then check that it works by subsetting your train list from exercise 1.\n  Hints (click here)  There's more than one way to do these; you will have to use both `[ ]` and `[[ ]]` operators. The last two are tricky, experiment with them...    Solution (click here)  train[1] #\u0026gt; [[1]] #\u0026gt; [1] 1 2 3   train[[1]] #\u0026gt; [1] 1 2 3   train[1:2] #\u0026gt; [[1]] #\u0026gt; [1] 1 2 3 #\u0026gt;  #\u0026gt; [[2]] #\u0026gt; [1] \"a\"   train[-2] #\u0026gt; [[1]] #\u0026gt; [1] 1 2 3 #\u0026gt;  #\u0026gt; [[2]] #\u0026gt; [1] 4 5 6   train[c(1, 1)] #\u0026gt; [[1]] #\u0026gt; [1] 1 2 3 #\u0026gt;  #\u0026gt; [[2]] #\u0026gt; [1] 1 2 3   train[0] #\u0026gt; list()      Exercise 4 A common use of recursive structures in biology is to represent phylogenetic trees. Create a recursive list in R called tree which captures the following visual representation\n  Hints (click here)  Start at the top and work down. Start with a simpler subtree, then expand terminals. Alternatively, start at the bottom with the smallest subtree, then work up, adding sisters into parent nodes.\nIn either case, check your working with str() as you incrementally add structure.\nNotice this is a binary branching tree, so the root node of every subtree should contain two elements.\nOne of the tricks with these nested lists is to keep track of paired parentheses\u0026hellip;\nStay calm and recurse.    Solution (click here)  tree \u0026lt;- list(\"a\", list(list(\"b\", \"c\"), \"d\")) str(tree) #\u0026gt; List of 2 #\u0026gt; $ : chr \"a\" #\u0026gt; $ :List of 2 #\u0026gt; ..$ :List of 2 #\u0026gt; .. ..$ : chr \"b\" #\u0026gt; .. ..$ : chr \"c\" #\u0026gt; ..$ : chr \"d\"      \n","date":1612915200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613152528,"objectID":"1f0acc316d61e12ddf0b12b66dfb0a7c","permalink":"https://biodash.github.io/codeclub/09_subsetting/","publishdate":"2021-02-10T00:00:00Z","relpermalink":"/codeclub/09_subsetting/","section":"codeclub","summary":"In this session of Code Club, we'll move below and beyond the tidyverse to get an overview of subsetting various kinds of data structure in R.","tags":null,"title":"Session 9: Subsetting","type":"codeclub"},{"authors":["Mike Sovic"],"categories":null,"content":" Session Goals  Describe differences in long data vs wide data. Identify scenarios where it might be helpful to have data in one format over another (longer vs. wider). Use the functions pivot_longer() and pivot_wider() to reshape data. Use NHANES data to address whether blood pressure values vary in a predictable way with successive measurements.   Intro: The Shape Of A Dataset A single set of data can sometimes be stored in different ways, or in other words, it can have different shapes. Below is a small example. It\u0026rsquo;s a hypothetical dataset that stores the number of visitors at each of two parks over a long weekend, and we\u0026rsquo;ll look at two different versions of it\u0026hellip;\nWide Format #create the dataset visitors_wide \u0026lt;- data.frame(\"park\" = c(\"north_park\", \"south_park\"), \"Fri\" = c(65, 80), \"Sat\" = c(184, 160), \"Sun\" = c(135, 140), \"Mon\" = c(87, 71)) #view the dataset visitors_wide #\u0026gt; park Fri Sat Sun Mon #\u0026gt; 1 north_park 65 184 135 87 #\u0026gt; 2 south_park 80 160 140 71   Long Format #create the dataset visitors_long \u0026lt;- data.frame(\"park\" = rep(c(\"north_park\", \"south_park\"), 4), \"day\" = c(\"Fri\",\"Fri\",\"Sat\",\"Sat\",\"Sun\",\"Sun\",\"Mon\",\"Mon\"), \"visitors\" = c(65,80,184,160,135,140,87,71)) #view the dataset visitors_long #\u0026gt; park day visitors #\u0026gt; 1 north_park Fri 65 #\u0026gt; 2 south_park Fri 80 #\u0026gt; 3 north_park Sat 184 #\u0026gt; 4 south_park Sat 160 #\u0026gt; 5 north_park Sun 135 #\u0026gt; 6 south_park Sun 140 #\u0026gt; 7 north_park Mon 87 #\u0026gt; 8 south_park Mon 71   Notice that both datasets store the same information - it\u0026rsquo;s just formatted differently. These two datasets can be said to have different shapes. The first has a wider shape - it has more columns, stretching it out from left to right. The second has a longer shape, as it has fewer columns and more rows. Again, importantly, both datasets store the same information.\nWhat Shape Should Your Data Be In? The best answer to the question of what shape your data should be in is probably something like \u0026lsquo;Whatever shape makes it easiest to accomplish your goals with the data at any given time\u0026rsquo;. For example, sometimes when you\u0026rsquo;re entering data - say in to a spreadsheet in Excel or a similar program, you might find the data entry process easier if the dataset is in a wider format. In contrast, longer formats will generally be better when analyzing your data. This is consistent with the idea of tidy data we talked about in Session 2. For example, tidy data will be long because a characteristic of tidy data is that each variable has its own column. For these reasons, you might find it helpful or even necessary to reshape the data - possibly multiple times as you continue to work with the same dataset.\nHow To Reshape Data R offers several approaches for reshaping data. Functions for doing so often come in pairs that transform from wider to longer, and longer to wider, respectively. Pairs of functions include cast() and melt(), spread() and gather(), and pivot_longer() and pivot_wider(). While any of these can be used, we\u0026rsquo;ll focus on the \u0026lsquo;pivot\u0026rsquo; pair that come from the package tidyr, as they were written most recently with a goal of being the most user-friendly of the available functions so far.\nPivoting Resources If you want to dig in to pivoting a bit more, R offers a very useful vignette on pivoting, which is worth a look - portions of today\u0026rsquo;s breakout sessions will come from there. Chapter 12 of \u0026ldquo;R For Data Science\u0026rdquo; by Wickham and Grolemund, which covers tidy data, also includes a nice section on pivoting.\n Examples Let\u0026rsquo;s revisit the park visitors dataset for an example of how pivot_longer() and pivot_wider() work in their most basic form. Previously, I created each of the wide and long forms of this dataset by hand. It was manageable to do that, since it\u0026rsquo;s a very small dataset, but for most datasets, you\u0026rsquo;re not going to want to just recreate a data frame from scratch each time you need to reshape the data. Let\u0026rsquo;s start with the data in wide format\u0026hellip;\n#view the data frame visitors_wide #\u0026gt; park Fri Sat Sun Mon #\u0026gt; 1 north_park 65 184 135 87 #\u0026gt; 2 south_park 80 160 140 71   What if we wanted to plot the total mean number of visitors per day across both parks? To get the mean values, we might think about applying some of the functions we\u0026rsquo;ve been working with in previous sessions like group_by() and summarize(). For example, we might want to try grouping by day and then calculating the means from a column that stores the number of visitors. However, in it\u0026rsquo;s current wide form, this dataset doesn\u0026rsquo;t have the day and visitors columns we need. pivot_longer() can help us here. The command might look like this\u0026hellip;\nlibrary(tidyverse) visitors_longer \u0026lt;- visitors_wide %\u0026gt;% pivot_longer(-park, names_to = \"day\", values_to = \"visitors\")   First, we need to point it to the dataset we\u0026rsquo;re interested in reshaping - I\u0026rsquo;m doing that by piping the visitors_wide data frame to pivot_longer(). Next, we need to specify what columns to use to lengthen the dataset. This argument recognizes tidy-select notation, which can really simplify things. Here, I\u0026rsquo;m using -park, which tells it to use all the column names except park. Those column names will be transformed to values in a single new column, which needs a name. We\u0026rsquo;ll call it day, so names_to = \u0026quot;day\u0026quot;. Finally, the values in the current columns will be stacked in to a single column, and it too needs a name, so values_to = \u0026quot;visitors\u0026quot;. This lengthens the dataset, taking it from 5 columns down to 3.\n#view the data visitors_longer #\u0026gt; # A tibble: 8 x 3 #\u0026gt; park day visitors #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 north_park Fri 65 #\u0026gt; 2 north_park Sat 184 #\u0026gt; 3 north_park Sun 135 #\u0026gt; 4 north_park Mon 87 #\u0026gt; 5 south_park Fri 80 #\u0026gt; 6 south_park Sat 160 #\u0026gt; 7 south_park Sun 140 #\u0026gt; 8 south_park Mon 71   In this longer format, we\u0026rsquo;re able to apply the group_by() and summarize() functions\u0026hellip;\nvisitors_longer %\u0026gt;% group_by(day) %\u0026gt;% summarise(\"mean\" = mean(visitors)) #\u0026gt; `summarise()` ungrouping output (override with `.groups` argument) #\u0026gt; # A tibble: 4 x 2 #\u0026gt; day mean #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 Fri 72.5 #\u0026gt; 2 Mon 79  #\u0026gt; 3 Sat 172  #\u0026gt; 4 Sun 138.   And we can go in the opposite direction with pivot_wider()\u0026hellip;\nvisitors_longer %\u0026gt;% pivot_wider(names_from = day, values_from = visitors) #\u0026gt; # A tibble: 2 x 5 #\u0026gt; park Fri Sat Sun Mon #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 north_park 65 184 135 87 #\u0026gt; 2 south_park 80 160 140 71   The examples above represent the most basic uses of pivot_longer() and pivot_wider(). But each of these functions offer additional arguments that can help deal with more complicated situations. The next example is from the pivoting vignette I referenced above. It uses the billboard dataset that should already be available in your R session, and that stores weekly rankings of Billboard top 100 songs from the year 2000.\n#preview billboard data head(billboard) #\u0026gt; # A tibble: 6 x 79 #\u0026gt; artist track date.entered wk1 wk2 wk3 wk4 wk5 wk6 wk7 wk8 #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 2 Pac Baby… 2000-02-26 87 82 72 77 87 94 99 NA #\u0026gt; 2 2Ge+h… The … 2000-09-02 91 87 92 NA NA NA NA NA #\u0026gt; 3 3 Doo… Kryp… 2000-04-08 81 70 68 67 66 57 54 53 #\u0026gt; 4 3 Doo… Loser 2000-10-21 76 76 72 69 67 65 55 59 #\u0026gt; 5 504 B… Wobb… 2000-04-15 57 34 25 17 17 31 36 49 #\u0026gt; 6 98^0 Give… 2000-08-19 51 39 34 26 26 19 2 2 #\u0026gt; # … with 68 more variables: wk9 \u0026lt;dbl\u0026gt;, wk10 \u0026lt;dbl\u0026gt;, wk11 \u0026lt;dbl\u0026gt;, wk12 \u0026lt;dbl\u0026gt;, #\u0026gt; # wk13 \u0026lt;dbl\u0026gt;, wk14 \u0026lt;dbl\u0026gt;, wk15 \u0026lt;dbl\u0026gt;, wk16 \u0026lt;dbl\u0026gt;, wk17 \u0026lt;dbl\u0026gt;, wk18 \u0026lt;dbl\u0026gt;, #\u0026gt; # wk19 \u0026lt;dbl\u0026gt;, wk20 \u0026lt;dbl\u0026gt;, wk21 \u0026lt;dbl\u0026gt;, wk22 \u0026lt;dbl\u0026gt;, wk23 \u0026lt;dbl\u0026gt;, wk24 \u0026lt;dbl\u0026gt;, #\u0026gt; # wk25 \u0026lt;dbl\u0026gt;, wk26 \u0026lt;dbl\u0026gt;, wk27 \u0026lt;dbl\u0026gt;, wk28 \u0026lt;dbl\u0026gt;, wk29 \u0026lt;dbl\u0026gt;, wk30 \u0026lt;dbl\u0026gt;, #\u0026gt; # wk31 \u0026lt;dbl\u0026gt;, wk32 \u0026lt;dbl\u0026gt;, wk33 \u0026lt;dbl\u0026gt;, wk34 \u0026lt;dbl\u0026gt;, wk35 \u0026lt;dbl\u0026gt;, wk36 \u0026lt;dbl\u0026gt;, #\u0026gt; # wk37 \u0026lt;dbl\u0026gt;, wk38 \u0026lt;dbl\u0026gt;, wk39 \u0026lt;dbl\u0026gt;, wk40 \u0026lt;dbl\u0026gt;, wk41 \u0026lt;dbl\u0026gt;, wk42 \u0026lt;dbl\u0026gt;, #\u0026gt; # wk43 \u0026lt;dbl\u0026gt;, wk44 \u0026lt;dbl\u0026gt;, wk45 \u0026lt;dbl\u0026gt;, wk46 \u0026lt;dbl\u0026gt;, wk47 \u0026lt;dbl\u0026gt;, wk48 \u0026lt;dbl\u0026gt;, #\u0026gt; # wk49 \u0026lt;dbl\u0026gt;, wk50 \u0026lt;dbl\u0026gt;, wk51 \u0026lt;dbl\u0026gt;, wk52 \u0026lt;dbl\u0026gt;, wk53 \u0026lt;dbl\u0026gt;, wk54 \u0026lt;dbl\u0026gt;, #\u0026gt; # wk55 \u0026lt;dbl\u0026gt;, wk56 \u0026lt;dbl\u0026gt;, wk57 \u0026lt;dbl\u0026gt;, wk58 \u0026lt;dbl\u0026gt;, wk59 \u0026lt;dbl\u0026gt;, wk60 \u0026lt;dbl\u0026gt;, #\u0026gt; # wk61 \u0026lt;dbl\u0026gt;, wk62 \u0026lt;dbl\u0026gt;, wk63 \u0026lt;dbl\u0026gt;, wk64 \u0026lt;dbl\u0026gt;, wk65 \u0026lt;dbl\u0026gt;, wk66 \u0026lt;lgl\u0026gt;, #\u0026gt; # wk67 \u0026lt;lgl\u0026gt;, wk68 \u0026lt;lgl\u0026gt;, wk69 \u0026lt;lgl\u0026gt;, wk70 \u0026lt;lgl\u0026gt;, wk71 \u0026lt;lgl\u0026gt;, wk72 \u0026lt;lgl\u0026gt;, #\u0026gt; # wk73 \u0026lt;lgl\u0026gt;, wk74 \u0026lt;lgl\u0026gt;, wk75 \u0026lt;lgl\u0026gt;, wk76 \u0026lt;lgl\u0026gt;   Notice there are columns named \u0026lsquo;wk1\u0026rsquo; through \u0026lsquo;wk73\u0026rsquo; that store the weekly ranking for each song. Week itself is a variable with values that could be represented in a single column. We could do something similar to our above use of pivot_longer()\u0026hellip;\nbillboard %\u0026gt;% pivot_longer(cols = starts_with(\"wk\"), names_to = \"week\", values_to = \"rank\") %\u0026gt;% head() #\u0026gt; # A tibble: 6 x 5 #\u0026gt; artist track date.entered week rank #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 2 Pac Baby Don't Cry (Keep... 2000-02-26 wk1 87 #\u0026gt; 2 2 Pac Baby Don't Cry (Keep... 2000-02-26 wk2 82 #\u0026gt; 3 2 Pac Baby Don't Cry (Keep... 2000-02-26 wk3 72 #\u0026gt; 4 2 Pac Baby Don't Cry (Keep... 2000-02-26 wk4 77 #\u0026gt; 5 2 Pac Baby Don't Cry (Keep... 2000-02-26 wk5 87 #\u0026gt; 6 2 Pac Baby Don't Cry (Keep... 2000-02-26 wk6 94   This is a start - we\u0026rsquo;ve gone from 79 columns to just 6. But we can clean this up a bit more. Notice the values in the new week column all include the \u0026lsquo;wk\u0026rsquo; prefix. Since we\u0026rsquo;ve labeled the column \u0026lsquo;week\u0026rsquo;, it\u0026rsquo;s kind of redundant and unnecessary to have \u0026lsquo;wk\u0026rsquo; at the beginning of each value. We can add the \u0026lsquo;names_prefix\u0026rsquo; argument, which accepts a regular expression (regex). Characters at the beginning of column names that match the regex get removed.\nbillboard %\u0026gt;% pivot_longer(cols = starts_with(\"wk\"), names_to = \"week\", values_to = \"rank\", names_prefix = \"wk\") %\u0026gt;% head() #\u0026gt; # A tibble: 6 x 5 #\u0026gt; artist track date.entered week rank #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 2 Pac Baby Don't Cry (Keep... 2000-02-26 1 87 #\u0026gt; 2 2 Pac Baby Don't Cry (Keep... 2000-02-26 2 82 #\u0026gt; 3 2 Pac Baby Don't Cry (Keep... 2000-02-26 3 72 #\u0026gt; 4 2 Pac Baby Don't Cry (Keep... 2000-02-26 4 77 #\u0026gt; 5 2 Pac Baby Don't Cry (Keep... 2000-02-26 5 87 #\u0026gt; 6 2 Pac Baby Don't Cry (Keep... 2000-02-26 6 94   We haven\u0026rsquo;t dealt with regular expressions in Code Club yet - they\u0026rsquo;ll make a good topic for a future session, but if you\u0026rsquo;re interested in the meantime, I did a couple short videos introducing them as part of this set of videos on command line computing.\nBreakout Rooms In the breakout rooms, we\u0026rsquo;ll use a pivot function to analyze a portion of the NHANES dataset. We\u0026rsquo;ll use the data to try to address whether successive blood pressure measurements from the same individual differ in a predictable way.\nIf you haven\u0026rsquo;t already done it, you can install the NHANES dataset with\u0026hellip;\ninstall.packages(\"NHANES\", repos = \"http://cran.us.r-project.org\") #\u0026gt;  #\u0026gt; The downloaded binary packages are in #\u0026gt; /var/folders/s7/y_mgh3c54h9fjcyw9wqdkb8x4zs_jy/T//RtmpWxvWIv/downloaded_packages   Exercise 1  First let\u0026rsquo;s load and preview the NHANES dataset.\n  Hints (click here)  \nUse library() to load the dataset. The functions head() are glimpse() are a couple good options for previewing the data.    Solution (click here)  library(NHANES) glimpse(NHANES) #\u0026gt; Rows: 10,000 #\u0026gt; Columns: 76 #\u0026gt; $ ID \u0026lt;int\u0026gt; 51624, 51624, 51624, 51625, 51630, 51638, 51646, 516… #\u0026gt; $ SurveyYr \u0026lt;fct\u0026gt; 2009_10, 2009_10, 2009_10, 2009_10, 2009_10, 2009_10… #\u0026gt; $ Gender \u0026lt;fct\u0026gt; male, male, male, male, female, male, male, female, … #\u0026gt; $ Age \u0026lt;int\u0026gt; 34, 34, 34, 4, 49, 9, 8, 45, 45, 45, 66, 58, 54, 10,… #\u0026gt; $ AgeDecade \u0026lt;fct\u0026gt; 30-39, 30-39, 30-39, 0-9, 40-49, 0-9, 0-9, 4… #\u0026gt; $ AgeMonths \u0026lt;int\u0026gt; 409, 409, 409, 49, 596, 115, 101, 541, 541, 541, 795… #\u0026gt; $ Race1 \u0026lt;fct\u0026gt; White, White, White, Other, White, White, White, Whi… #\u0026gt; $ Race3 \u0026lt;fct\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … #\u0026gt; $ Education \u0026lt;fct\u0026gt; High School, High School, High School, NA, Some Coll… #\u0026gt; $ MaritalStatus \u0026lt;fct\u0026gt; Married, Married, Married, NA, LivePartner, NA, NA, … #\u0026gt; $ HHIncome \u0026lt;fct\u0026gt; 25000-34999, 25000-34999, 25000-34999, 20000-24999, … #\u0026gt; $ HHIncomeMid \u0026lt;int\u0026gt; 30000, 30000, 30000, 22500, 40000, 87500, 60000, 875… #\u0026gt; $ Poverty \u0026lt;dbl\u0026gt; 1.36, 1.36, 1.36, 1.07, 1.91, 1.84, 2.33, 5.00, 5.00… #\u0026gt; $ HomeRooms \u0026lt;int\u0026gt; 6, 6, 6, 9, 5, 6, 7, 6, 6, 6, 5, 10, 6, 10, 10, 4, 3… #\u0026gt; $ HomeOwn \u0026lt;fct\u0026gt; Own, Own, Own, Own, Rent, Rent, Own, Own, Own, Own, … #\u0026gt; $ Work \u0026lt;fct\u0026gt; NotWorking, NotWorking, NotWorking, NA, NotWorking, … #\u0026gt; $ Weight \u0026lt;dbl\u0026gt; 87.4, 87.4, 87.4, 17.0, 86.7, 29.8, 35.2, 75.7, 75.7… #\u0026gt; $ Length \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … #\u0026gt; $ HeadCirc \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … #\u0026gt; $ Height \u0026lt;dbl\u0026gt; 164.7, 164.7, 164.7, 105.4, 168.4, 133.1, 130.6, 166… #\u0026gt; $ BMI \u0026lt;dbl\u0026gt; 32.22, 32.22, 32.22, 15.30, 30.57, 16.82, 20.64, 27.… #\u0026gt; $ BMICatUnder20yrs \u0026lt;fct\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … #\u0026gt; $ BMI_WHO \u0026lt;fct\u0026gt; 30.0_plus, 30.0_plus, 30.0_plus, 12.0_18.5, 30.0_plu… #\u0026gt; $ Pulse \u0026lt;int\u0026gt; 70, 70, 70, NA, 86, 82, 72, 62, 62, 62, 60, 62, 76, … #\u0026gt; $ BPSysAve \u0026lt;int\u0026gt; 113, 113, 113, NA, 112, 86, 107, 118, 118, 118, 111,… #\u0026gt; $ BPDiaAve \u0026lt;int\u0026gt; 85, 85, 85, NA, 75, 47, 37, 64, 64, 64, 63, 74, 85, … #\u0026gt; $ BPSys1 \u0026lt;int\u0026gt; 114, 114, 114, NA, 118, 84, 114, 106, 106, 106, 124,… #\u0026gt; $ BPDia1 \u0026lt;int\u0026gt; 88, 88, 88, NA, 82, 50, 46, 62, 62, 62, 64, 76, 86, … #\u0026gt; $ BPSys2 \u0026lt;int\u0026gt; 114, 114, 114, NA, 108, 84, 108, 118, 118, 118, 108,… #\u0026gt; $ BPDia2 \u0026lt;int\u0026gt; 88, 88, 88, NA, 74, 50, 36, 68, 68, 68, 62, 72, 88, … #\u0026gt; $ BPSys3 \u0026lt;int\u0026gt; 112, 112, 112, NA, 116, 88, 106, 118, 118, 118, 114,… #\u0026gt; $ BPDia3 \u0026lt;int\u0026gt; 82, 82, 82, NA, 76, 44, 38, 60, 60, 60, 64, 76, 82, … #\u0026gt; $ Testosterone \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … #\u0026gt; $ DirectChol \u0026lt;dbl\u0026gt; 1.29, 1.29, 1.29, NA, 1.16, 1.34, 1.55, 2.12, 2.12, … #\u0026gt; $ TotChol \u0026lt;dbl\u0026gt; 3.49, 3.49, 3.49, NA, 6.70, 4.86, 4.09, 5.82, 5.82, … #\u0026gt; $ UrineVol1 \u0026lt;int\u0026gt; 352, 352, 352, NA, 77, 123, 238, 106, 106, 106, 113,… #\u0026gt; $ UrineFlow1 \u0026lt;dbl\u0026gt; NA, NA, NA, NA, 0.094, 1.538, 1.322, 1.116, 1.116, 1… #\u0026gt; $ UrineVol2 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … #\u0026gt; $ UrineFlow2 \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … #\u0026gt; $ Diabetes \u0026lt;fct\u0026gt; No, No, No, No, No, No, No, No, No, No, No, No, No, … #\u0026gt; $ DiabetesAge \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … #\u0026gt; $ HealthGen \u0026lt;fct\u0026gt; Good, Good, Good, NA, Good, NA, NA, Vgood, Vgood, Vg… #\u0026gt; $ DaysPhysHlthBad \u0026lt;int\u0026gt; 0, 0, 0, NA, 0, NA, NA, 0, 0, 0, 10, 0, 4, NA, NA, 0… #\u0026gt; $ DaysMentHlthBad \u0026lt;int\u0026gt; 15, 15, 15, NA, 10, NA, NA, 3, 3, 3, 0, 0, 0, NA, NA… #\u0026gt; $ LittleInterest \u0026lt;fct\u0026gt; Most, Most, Most, NA, Several, NA, NA, None, None, N… #\u0026gt; $ Depressed \u0026lt;fct\u0026gt; Several, Several, Several, NA, Several, NA, NA, None… #\u0026gt; $ nPregnancies \u0026lt;int\u0026gt; NA, NA, NA, NA, 2, NA, NA, 1, 1, 1, NA, NA, NA, NA, … #\u0026gt; $ nBabies \u0026lt;int\u0026gt; NA, NA, NA, NA, 2, NA, NA, NA, NA, NA, NA, NA, NA, N… #\u0026gt; $ Age1stBaby \u0026lt;int\u0026gt; NA, NA, NA, NA, 27, NA, NA, NA, NA, NA, NA, NA, NA, … #\u0026gt; $ SleepHrsNight \u0026lt;int\u0026gt; 4, 4, 4, NA, 8, NA, NA, 8, 8, 8, 7, 5, 4, NA, 5, 7, … #\u0026gt; $ SleepTrouble \u0026lt;fct\u0026gt; Yes, Yes, Yes, NA, Yes, NA, NA, No, No, No, No, No, … #\u0026gt; $ PhysActive \u0026lt;fct\u0026gt; No, No, No, NA, No, NA, NA, Yes, Yes, Yes, Yes, Yes,… #\u0026gt; $ PhysActiveDays \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, 5, 5, 5, 7, 5, 1, NA, 2,… #\u0026gt; $ TVHrsDay \u0026lt;fct\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … #\u0026gt; $ CompHrsDay \u0026lt;fct\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … #\u0026gt; $ TVHrsDayChild \u0026lt;int\u0026gt; NA, NA, NA, 4, NA, 5, 1, NA, NA, NA, NA, NA, NA, 4, … #\u0026gt; $ CompHrsDayChild \u0026lt;int\u0026gt; NA, NA, NA, 1, NA, 0, 6, NA, NA, NA, NA, NA, NA, 3, … #\u0026gt; $ Alcohol12PlusYr \u0026lt;fct\u0026gt; Yes, Yes, Yes, NA, Yes, NA, NA, Yes, Yes, Yes, Yes, … #\u0026gt; $ AlcoholDay \u0026lt;int\u0026gt; NA, NA, NA, NA, 2, NA, NA, 3, 3, 3, 1, 2, 6, NA, NA,… #\u0026gt; $ AlcoholYear \u0026lt;int\u0026gt; 0, 0, 0, NA, 20, NA, NA, 52, 52, 52, 100, 104, 364, … #\u0026gt; $ SmokeNow \u0026lt;fct\u0026gt; No, No, No, NA, Yes, NA, NA, NA, NA, NA, No, NA, NA,… #\u0026gt; $ Smoke100 \u0026lt;fct\u0026gt; Yes, Yes, Yes, NA, Yes, NA, NA, No, No, No, Yes, No,… #\u0026gt; $ Smoke100n \u0026lt;fct\u0026gt; Smoker, Smoker, Smoker, NA, Smoker, NA, NA, Non-Smok… #\u0026gt; $ SmokeAge \u0026lt;int\u0026gt; 18, 18, 18, NA, 38, NA, NA, NA, NA, NA, 13, NA, NA, … #\u0026gt; $ Marijuana \u0026lt;fct\u0026gt; Yes, Yes, Yes, NA, Yes, NA, NA, Yes, Yes, Yes, NA, Y… #\u0026gt; $ AgeFirstMarij \u0026lt;int\u0026gt; 17, 17, 17, NA, 18, NA, NA, 13, 13, 13, NA, 19, 15, … #\u0026gt; $ RegularMarij \u0026lt;fct\u0026gt; No, No, No, NA, No, NA, NA, No, No, No, NA, Yes, Yes… #\u0026gt; $ AgeRegMarij \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 20, 15, … #\u0026gt; $ HardDrugs \u0026lt;fct\u0026gt; Yes, Yes, Yes, NA, Yes, NA, NA, No, No, No, No, Yes,… #\u0026gt; $ SexEver \u0026lt;fct\u0026gt; Yes, Yes, Yes, NA, Yes, NA, NA, Yes, Yes, Yes, Yes, … #\u0026gt; $ SexAge \u0026lt;int\u0026gt; 16, 16, 16, NA, 12, NA, NA, 13, 13, 13, 17, 22, 12, … #\u0026gt; $ SexNumPartnLife \u0026lt;int\u0026gt; 8, 8, 8, NA, 10, NA, NA, 20, 20, 20, 15, 7, 100, NA,… #\u0026gt; $ SexNumPartYear \u0026lt;int\u0026gt; 1, 1, 1, NA, 1, NA, NA, 0, 0, 0, NA, 1, 1, NA, NA, 1… #\u0026gt; $ SameSex \u0026lt;fct\u0026gt; No, No, No, NA, Yes, NA, NA, Yes, Yes, Yes, No, No, … #\u0026gt; $ SexOrientation \u0026lt;fct\u0026gt; Heterosexual, Heterosexual, Heterosexual, NA, Hetero… #\u0026gt; $ PregnantNow \u0026lt;fct\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …       Exercise 2  As you might know, blood pressure consists of two values - systolic and diastolic. Each participant in the NHANES survey had their blood pressure measured three times in succession, giving us the columns: BPSys1, BPDia1, BPSys2, BPDia2, BPSys3, BPDia3. Let\u0026rsquo;s work first with just the three systolic values.\nSubset the dataset to get just the columns BPSys1, BPSys2, and BPSys3. Name the new object \u0026lsquo;sys_values\u0026rsquo;, then get the dimensions of sys_values and preview it.\n  Hints (click here)  \nUse select() from dplyr to get the three columns we want. dim() and glimpse() can be used to get the dimensions and preview the data, respectively.    Solution (click here)  sys_values \u0026lt;- NHANES %\u0026gt;% select(matches(\"BPSys[123]$\")) #I used the 'matches' helper along with a regular expression  #above, but there are a number of ways you could do this.  #One equivalent would be... # sys_values \u0026lt;- NHANES %\u0026gt;% select(BPSys1, BPSys2, BPSys3) dim(sys_values) #\u0026gt; [1] 10000 3 head(sys_values) #\u0026gt; # A tibble: 6 x 3 #\u0026gt; BPSys1 BPSys2 BPSys3 #\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 114 114 112 #\u0026gt; 2 114 114 112 #\u0026gt; 3 114 114 112 #\u0026gt; 4 NA NA NA #\u0026gt; 5 118 108 116 #\u0026gt; 6 84 84 88       Exercise 3  We can see just from the preview in Exercise 2 that the dataset has some missing data - let\u0026rsquo;s remove rows that have NA\u0026rsquo;s. Call the new dataset \u0026lsquo;sys_noNA\u0026rsquo;. Then check the dimensions and preview again.\n  Hints (click here)  \nTry the drop_na function from tidyr to eliminate rows containing missing data.    Solution (click here)  sys_noNA \u0026lt;- sys_values %\u0026gt;% drop_na() dim(sys_noNA) #\u0026gt; [1] 7971 3 head(sys_noNA) #\u0026gt; # A tibble: 6 x 3 #\u0026gt; BPSys1 BPSys2 BPSys3 #\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 114 114 112 #\u0026gt; 2 114 114 112 #\u0026gt; 3 114 114 112 #\u0026gt; 4 118 108 116 #\u0026gt; 5 84 84 88 #\u0026gt; 6 114 108 106       Exercise 4  We\u0026rsquo;ll explore these data a bit to see if there\u0026rsquo;s any evidence of a trend in systolic blood pressure with respect to the sequence of measurements (differences among measurements 1, 2, and 3). First, lets reshape the data so we end up with just two columns named \u0026lsquo;measurement\u0026rsquo; and \u0026lsquo;sys_bp\u0026rsquo;. Save the new objects as \u0026lsquo;sys_long\u0026rsquo;. Then check the dimensions and preview again.\n  Hints (click here)  \nUse pivot_longer() to lengthen the dataset. You\u0026rsquo;ll need to include the arguments \u0026ldquo;cols\u0026rdquo;, \u0026ldquo;names_to\u0026rdquo;, and \u0026ldquo;values_to\u0026rdquo;.    Solution (click here)  sys_long \u0026lt;- sys_noNA %\u0026gt;% pivot_longer(cols = starts_with(\"BP\"), names_to = \"measurement\", values_to = \"sys_bp\") dim(sys_long) #\u0026gt; [1] 23913 2 head(sys_long) #\u0026gt; # A tibble: 6 x 2 #\u0026gt; measurement sys_bp #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 BPSys1 114 #\u0026gt; 2 BPSys2 114 #\u0026gt; 3 BPSys3 112 #\u0026gt; 4 BPSys1 114 #\u0026gt; 5 BPSys2 114 #\u0026gt; 6 BPSys3 112       Exercise 5  Now let\u0026rsquo;s calculate and compare the mean values for each measurement.\n  Hints (click here)  \nUse group_by() and summarize() to get a mean for each of the three measurements.    Solution (click here)  sys_long %\u0026gt;% group_by(measurement) %\u0026gt;% summarize(\"mean_sys\" = mean(sys_bp)) #\u0026gt; `summarise()` ungrouping output (override with `.groups` argument) #\u0026gt; # A tibble: 3 x 2 #\u0026gt; measurement mean_sys #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 BPSys1 119. #\u0026gt; 2 BPSys2 118. #\u0026gt; 3 BPSys3 118.       Exercise 6  The summarise() functions outputs a tibble. Tibbles are intended to be tidy, and as part of that, by default the values they display tend to be truncated/rounded to a greater degree than they would be otherwise in R. In this case, we might want to see a bit more precision in the values. Try adjusting (increasing) the number of significant figures that are displayed in the tibble that was output in Exercise 5.\n  Hints (click here)  \nThis can be done in a couple different ways. One is to convert the tibble to a data frame with as.data.frame(), since data frames, by default, will likely show more significant digits. Alternatively, try setting options(pillar.sigfig) to a new value.    Solution (click here)  sys_long %\u0026gt;% group_by(measurement) %\u0026gt;% summarize(\"mean_sys\" = mean(sys_bp)) %\u0026gt;% as.data.frame() #\u0026gt; `summarise()` ungrouping output (override with `.groups` argument) #\u0026gt; measurement mean_sys #\u0026gt; 1 BPSys1 119.1682 #\u0026gt; 2 BPSys2 118.4333 #\u0026gt; 3 BPSys3 117.8479 #OR options(pillar.sigfig = 6) sys_long %\u0026gt;% group_by(measurement) %\u0026gt;% summarize(\"mean_sys\" = mean(sys_bp)) #\u0026gt; `summarise()` ungrouping output (override with `.groups` argument) #\u0026gt; # A tibble: 3 x 2 #\u0026gt; measurement mean_sys #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 BPSys1 119.168 #\u0026gt; 2 BPSys2 118.433 #\u0026gt; 3 BPSys3 117.848       Bonus 1  Are those differences statistically significant? A one-way anova might be a good option to test that. Check out the help page for the function aov() and try running an ANOVA.\n  Hint 1 (click here)  \nR often uses the tilde (~) to indicate formula notation. So, for example, you can generate a scatterplot in base R by plotting y~x, assuming y and x are numeric vectors of equal lengths. The aov() function requires a formula with the pattern values~group. You can use the column names in the data frame to define these, but then you need to use the \u0026lsquo;data\u0026rsquo; argument to tell the function the name of the data frame where those columns exist.    Hint 2 (click here)  \nOnce you get the aov() function to work, you can get a p-value with the summary function. See info under the \u0026ldquo;Value\u0026rdquo; heading on the help page for aov().    Solution (click here)  aov(sys_bp~measurement, data = sys_long) %\u0026gt;% summary() #\u0026gt; Df Sum Sq Mean Sq F value Pr(\u0026gt;F)  #\u0026gt; measurement 2 6977 3489 11.87 7.05e-06 *** #\u0026gt; Residuals 23910 7028228 294  #\u0026gt; --- #\u0026gt; Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1       Bonus 2  Repeat all of the above for diastolic blood pressure with a couple of modifications along the way. First, when you reshape/lengthen the data, make the values in the \u0026lsquo;measurement\u0026rsquo; column numeric. For example, in the sys_long data frame we created above, the values in the measurement column were characters, and looked like \u0026ldquo;BPsys1\u0026rdquo;. This time, make them a factor with the levels \u0026ldquo;1\u0026rdquo;, \u0026ldquo;2\u0026rdquo;, and \u0026ldquo;3\u0026rdquo;.\n  Hint (click here)  \nUse the pivot_longer() arguments \u0026ldquo;names_prefix\u0026rdquo; and \u0026ldquo;names_transform\u0026rdquo;.    Solution (click here)  dia_data \u0026lt;- NHANES %\u0026gt;% select(matches(\"BPDia[123]$\")) %\u0026gt;% drop_na() %\u0026gt;% pivot_longer(cols = starts_with(\"BP\"), names_to = \"measurement\", values_to = \"dia_bp\", names_prefix = \"BPDia\", names_transform = list(measurement = \"as.factor\")) head(dia_data) #\u0026gt; # A tibble: 6 x 2 #\u0026gt; measurement dia_bp #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 1 88 #\u0026gt; 2 2 88 #\u0026gt; 3 3 82 #\u0026gt; 4 1 88 #\u0026gt; 5 2 88 #\u0026gt; 6 3 82 dia_data %\u0026gt;% group_by(measurement) %\u0026gt;% summarize(\"mean_dia\" = mean(dia_bp)) %\u0026gt;% as.data.frame() #\u0026gt; `summarise()` ungrouping output (override with `.groups` argument) #\u0026gt; measurement mean_dia #\u0026gt; 1 1 68.28830 #\u0026gt; 2 2 67.46280 #\u0026gt; 3 3 67.06762 aov(dia_bp~measurement, data = dia_data) %\u0026gt;% summary() #\u0026gt; Df Sum Sq Mean Sq F value Pr(\u0026gt;F)  #\u0026gt; measurement 2 6185 3092.3 14.91 3.38e-07 *** #\u0026gt; Residuals 23910 4958916 207.4  #\u0026gt; --- #\u0026gt; Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1       ","date":1612224000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612457344,"objectID":"0ce28958d05d4030b2130203b1616eb7","permalink":"https://biodash.github.io/codeclub/08_pivoting/","publishdate":"2021-02-02T00:00:00Z","relpermalink":"/codeclub/08_pivoting/","section":"codeclub","summary":"In this session of Code Club, we'll consider the shape of our datasets and practice with the *tidyr* functions `pivot_longer()` and `pivot_wider()`, which allow us to reformat, or reshape our data - going from a longer form to a wider form, or vice versa.","tags":null,"title":"Session 8: Reshaping Your Data","type":"codeclub"},{"authors":["Jelmer Poelstra"],"categories":null,"content":"\n Introduction R Markdown consists of an amazing ecosystem of R packages to produce many types of technical content. Its signature capability is that is can run R code and print the code along with its results and nicely formatted prose.\nTo understand R Markdown, we need to learn about three new things:\n  Markdown, a very lightweight text formatting language.\n  Code chunks, which allow us to incorporate R code that can be executed and whose results we can display in text, figures, and tables.\n  The YAML header, which encodes metadata about the output, such as the desired output format and specific formatting features.\n  We\u0026rsquo;ll focus on HTML page output, but will glimpse at the many possibilities for the output format: with R Markdown, it is possible to create not just technical reports, but also slide decks, websites, books, scientific articles, and so on.\nSetup At the core of the R Markdown ecosystem is the rmarkdown package. We need to install this but don\u0026rsquo;t need to load it:\ninstall.packages(\"rmarkdown\")   Inside your directory for Code Club, create a directory for this week:\ndir.create('S07')   First, an example Before we go into details, let\u0026rsquo;s first see a quick demonstration of what we are talking about:\n  Go to File =\u0026gt; New File =\u0026gt; R Markdown, change the Title to \u0026ldquo;Markdown demo\u0026rdquo;, and click OK.\n  Take a look at the R Markdown document, and notice that there seems to be some sort of header (=\u0026gt; YAML), followed by R code wrapped in strange constructs with backticks (=\u0026gt; Code chunks), and plain written text (=\u0026gt; Markdown).\n  Before we can create output, we need to save the document. Click the Save button and save the files as demo.Rmd inside your newly created directory.\n  Now click the Knit button in one of the top bars, and a document should show up in a pop-up or the Viewer pane. This is the rendered output from the R Markdown document.\n  Notice that the YAML header is not printed, at least not verbatim, while some of the code is printed, and we also see code output including a plot!\nThis is what the raw and rendered output look side-by-side:\nWe\u0026rsquo;ll now talk about Markdown, code chunks, and the YAML header in turn.\n I: Markdown Markdown is a very lightweight language to format plain text files, which evolved from simple in-line formatting applied in emails before those started using HTML.\nNeed to emphasize a word without being able to make it italic or bold? How about adding emphasis with asterisks *like so*?\nAn overview of commonly used Markdown syntax    Syntax Result     # My Title Header level 1 (largest)   ## My Section Header level 2   ### My Subsection Header level 3 \u0026ndash; and so forth   *italic* or _italic_ italic   **bold** or __bold__ bold   [Markdown Guide](markdownguide.org) Markdown Guide (Link with custom text)   ![](path/to/figure.png) Figure   - List item Unordered (bulleted) list   1. List item Ordered (numbered) list   `inline code` inline code   ``` \u0026hellip;code\u0026hellip; ``` Generic code block (for formatting only) (Alternative syntax: 4 leading spaces.)   ```r \u0026hellip;code\u0026hellip; ``` r code block (for formatting only)   --- Horizontal rule (line)    To see this formatting in action, see below an example of a raw Markdown file on the left, and its rendered (formatted) output on the right:\n \u0026ldquo;Plain\u0026rdquo; Markdown files have the extension .md, whereas R Markdown files have the extension .Rmd.\n   II: Integrating R code As we saw above, plain Markdown has syntax for code formatting, but the code is not actually being executed. In R Markdown, however, we are able run code! The syntax to do so is only slightly modified from what we saw above:\n  For inline code, we add r and a space before the R code that is to be executed, for example:\n   Raw Rendered     There are `r 365*24` hours in a year There are 8760 hours in a year      To generate code blocks, which we call code chunks in an R Markdown context,\nwe add r inside curly braces: ```{r}\nWe can optionally add settings that we want to apply to that chunk and/or chunk labels:\n```{r, option1=value, ...} or ```{r, unique-chunk-label, option1=value, ...}\n RStudio keyboard shortcut to insert a code chunk: Cmd/Ctrl+Alt+I.\n    Code chunk examples   A code chunk with default options\u0026hellip;\n\u0026hellip;will be executed and shown followed by the output of the code:\nmean(penguins$bill_depth_mm, na.rm = TRUE) #\u0026gt; [1] 17.15117     As an example of using a code chunk option, we will disable printing the code using echo=FALSE (the code will still run and the output will still be shown):\n#\u0026gt; [1] 17.15117     Figures can, of course, also be printed:\nggplot(penguins) + geom_point(aes(x = bill_length_mm, y = bill_depth_mm, color = species)) + theme_bw() #\u0026gt; Warning: Removed 2 rows containing missing values (geom_point).  Fig. 1: Bill length and depth are correlated within species, and differ subtly between species.     Above, we added a caption for the figure using the fig.cap argument (with a little trick to force a line break, using the \u0026lt;br\u0026gt; HTML syntax).\n Code chunk options Here is an overview of some the most commonly made changes to defaults for code chunk options. This quickly gets confusing, but you\u0026rsquo;ll get the hang of it after experimenting a bit.\n echo=FALSE \u0026ndash; Don\u0026rsquo;t print the code in the output file. eval=FALSE \u0026ndash; Don\u0026rsquo;t run (evaluate) the code. include=FALSE \u0026ndash; Run but don\u0026rsquo;t print the code, nor any of its results. results=\u0026quot;hide\u0026quot; \u0026ndash; Don\u0026rsquo;t print the text output of the code. fig.show=\u0026quot;hide\u0026quot; \u0026ndash; Don\u0026rsquo;t print figures produced by the code.  Furthermore, you can use message=FALSE and warning=FALSE to suppress any messages (like the output when loading packages) and warnings (like the warning for the penguin figure above), respectively, that R might produce.\nFor figures, the following options are especially useful:\n fig.cap=\u0026quot;My caption\u0026quot; \u0026ndash; Include a caption. fig.asp=0.6 \u0026ndash; Aspect ratio. fig.width=6 \u0026ndash; Width of in inches: same as sizing in regular R code. fig.height=9.6 \u0026ndash; Height in inches: same as sizing in regular R code. out.width=\u0026quot;70%\u0026quot; \u0026ndash; Figure width as printed in the document (in % or pixels, px). out.height=\u0026quot;500px\u0026quot; \u0026ndash; Figure height as printed in the document.  Finally, if your document takes a long time to knit, use cache=TRUE to enable caching of results.\n   Default chunk options It is often practical to set default chunk options for the entire document, and you can do so with the opts_chunk$set() function as shown below:\nThis is usually done in separate \u0026ldquo;global setup chunk\u0026rdquo; at the start of the document.\nWhenever necessary, you can then override these defaults for specific chunks.\n   III: The YAML header YAML (\u0026ldquo;YAML Ain\u0026rsquo;t Markup Language\u0026rdquo;) is a simple format commonly used for configuration files, which allows you to provide key-value pairs such as author: John Doe.\nIn R Markdown files, it is used as a header which configures certain aspects of the output, especially the formatting. Put another way, the YAML header contains the metadata for the output.\nA basic YAML header Here is an example of a very basic YAML header:\n--- author: My name title: The document's title output: html_document ---  Note the lines which just contain three dashes, which mark the beginning and the end of the YAML header.\nAdding options Often, a value (like html_document) can itself be given key-value pairs to specify additional options \u0026ndash; see the example below where we include a Table of Contents (toc) and also set it to \u0026ldquo;float\u0026rdquo;:\n--- output: html_document: toc: true toc_float: true ---    Note the syntax changes (newlines and added indentation) between the above two examples, this is perhaps a little awkward and often leads to mistakes.\n  Indentation in YAML is using two or four spaces (no tabs!) per indentation level, and it is sensitive to indentation errors. (Fortunately, RStudio inserts spaces for tabs by default \u0026ndash; check/set in Tools =\u0026gt; Global Options =\u0026gt; Code =\u0026gt; Editing.)\n   Some options for html_document output html_document is the most commonly used output format for R Markdown documents, and here are few particularly useful options to customize the output:\n code_download: true \u0026ndash; Include a button to download the code. code_folding: hide \u0026ndash; Using hide or show will enable the folding of code chunks, with hide hiding them by default. toc: true \u0026ndash; Include a table of contents (Also: toc_depth: 3 sets depth to 3, toc_float: true lets the TOC \u0026ldquo;float\u0026rdquo; as you scroll down the document). number_sections: true \u0026ndash; Number the section headings. df_print: paged \u0026ndash; Get nicely formatted and paged data frame printing (also try: df_print: kable). theme: cerulean \u0026ndash; Use a pre-built theme, controlling the overall look and feel of the document. See here for a visual overview.    Three HTML document theme options: darkly, flatly, and cerulean.      IV: R Markdown and RStudio The RMarkdown ecosystem of packages is being developed by RStudio, so it should come as no surprise that the RStudio IDE has some nice RMarkdown functionality.\nKnitting and previewing your document The process of rendering an R Markdown file into another format, as specified by the YAML header, is called knitting. We already saw the button to knit the current document (keyboard shortcut: Cmd/Ctrl+Shift+K).\n If you get preview pop-up windows in RStudio, click the cog wheel icon next to the Knit button, and then select \u0026ldquo;Preview in Viewer Pane\u0026rdquo;.\n  Instead of knitting the entire document, you can also run individual code chunks using the green \u0026ldquo;play button\u0026rdquo; (or Cmd/Ctrl+Shift+Enter), or all code chunks up until the current one (button to the left of the play button).\nFor a live preview (!) of R Markdown output for your active document,\nuse the infinite moon reader from the xaringan package:\ninstall.packages(\"xaringan\") # Simply running the function without arguments will start the preview: xaringan::inf_mr() # To shut down the preview server, if needed, run `servr::daemon_stop()`   Visual Markdown Editor If your RStudio version is at least 1.4 (Click Help =\u0026gt; About RStudio), which was released last fall, you can also use the Visual Markdown Editor.\nThis makes writing in R Markdown almost like using a word processor, and also includes many other features such as better citation support with Zotero integration. Read more about the visual editor here.\nTo switch between the visual editor and regular (\u0026ldquo;source\u0026rdquo;) editing mode, click the A-shaped ruler button in the top-right corner or press Cmd/Ctrl+Shift+F4.\nThis is what our document looks like in the visual editor \u0026ndash; kind of intermediate between the raw R Markdown and the rendered output:\n V: A single source doc, many output formats! One of the greatest features of R Markdown is that you can output to many formats. So from one source document, or very similar variants, you can create completely different output depending on what you need.\nBuilt-in output formats The built-in output formats, which can be used with just the rmarkdown package, are listed below. These include HTML, PDF, Word, PowerPoint, and different HTML slide show formats!\nExtension output formats It\u0026rsquo;s worth highlighting a few of the output formats that can be used with the following packages in the R Markdown ecosystem:\n  distill \u0026ndash; An output format geared towards technical content, e.g. with extended support for equations, citations, and footnotes. Can also create websites.\n  rticles \u0026ndash; R Markdown templates to format output for specific scientific journals.\n  flexdashboard \u0026ndash; Create interactive \u0026ldquo;dashboards\u0026rdquo; to present data.\n  bookdown \u0026ndash; A book format, the R Markdown book is an example.\n  xaringan \u0026ndash; Create fancier presentation slides thanks to a JavaScript library.\n  Starting to use these and other output formats is often as simple as changing the YAML header:\n---output:distill::distill_article---\n Breakout rooms! In the exercises, we will work with an .Rmd file that you can download as follows:\n# dir.create(\"S07\") # You should have already done this # Save the URL for the Rmd file: todays_rmd \u0026lt;- 'https://raw.githubusercontent.com/biodash/biodash.github.io/master/content/codeclub/07_rmarkdown/penguins.Rmd' # Download the Rmd file: download.file(url = todays_rmd, destfile = 'S07/penguins.Rmd')   Next, open the document in RStudio, and fire up the infinite moon reader:\n# install.packages(\"xaringan\") xaringan::inf_mr()   This way, you will be able to nearly instantaneously see the effect of your changes: save the document whenever you want the server to update.\nYou can use either the \u0026ldquo;visual editor\u0026rdquo; or the regular (\u0026ldquo;source\u0026rdquo;) editor \u0026ndash; and you could also start by compating the two.\n Exercise 1: Output formatting with YAML In this exercise, you will fiddle with the YAML header to modify aspects of the html_document output format:\n  Add a theme key to html_output, and try a few of the available value options (\u0026quot;default\u0026quot;, \u0026ldquo;cerulean\u0026rdquo;, \u0026ldquo;journal\u0026rdquo;, \u0026ldquo;flatly\u0026rdquo;, \u0026ldquo;darkly\u0026rdquo;, \u0026ldquo;readable\u0026rdquo;, \u0026ldquo;spacelab\u0026rdquo;, \u0026ldquo;united\u0026rdquo;, \u0026ldquo;cosmo\u0026rdquo;, \u0026ldquo;lumen\u0026rdquo;, \u0026ldquo;paper\u0026rdquo;, \u0026ldquo;sandstone\u0026rdquo;, \u0026ldquo;simplex\u0026rdquo;, \u0026ldquo;yeti\u0026quot;).\nDetermine, once and for all, what the best theme is.\n  Try some of the other options mentioned above (code_download, code_folding, toc, toc_float, toc_depth, df_print), and look at the effects on the rendered output.\n    Hints (click here)     To add options to html_document in the YAML header, you\u0026rsquo;ll need to go from output: html_document on a single line, to a multi-line format with indentation, and with a colon added after html_document:\noutput:html_document:\u0026lt;option\u0026gt;     Solutions (click here)   An example YAML header with several options added:  ---title:\u0026#34;Penguins, demystified.\u0026#34;author:\u0026#34;Jelmer Poelstra\u0026#34;date:\u0026#34;1/29/2021\u0026#34;output:html_document:theme:flatlytoc:truetoc_float:truetoc_depth:5number_sections:truecode_download:truecode_folding:hidedf_print:kable---\n    Exercise 2: Code chunks Our output document looks nice, but there is plenty of room for improvement. In this exercise, we\u0026rsquo;ll refine the output using code chunk options.\nBefore you start, take another look at the box Code chunk options above.\n  Did you notice those messages (when the tidyverse is loaded) and warnings (for the two plots) in the output? Let\u0026rsquo;s get rid of those all at once: suppress R messages and warnings for all chunks by adding arguments to the knitr::opts_chunk$set() function in the first code chunk.\n  Currently, the code line in the install-package code chunk is commented out to avoid the code from running, while still printing it. Try to accomplish this using a code chunk option instead, so you can uncomment the line.\n  We do want to print the code in some cases, but not in others. For the chunk labeled print-tibble, which prints penguins, alter the settings such that the code is no longer printed.\n  Our first figure is kind of squished, and the point and font sizes are perhaps too large. Compare this with the second figure, which has a different setting only for out.width.\nPlay around with the values for the three options that are already in the code chunks (fig.width, out.width, and fig.asp), for one or both figures, see what the effects are, and try to make some improvements.\nDo you understand the difference between the two methods to indicate the figure size (fig.width and out.width)?\n  Insert a new code chunk that prints the penguins_raw tibble in some way (this is available in your environment).\n    Hints (click here)     To suppress messages and warnings throughout:\nAdd message=FALSE and warnings=FALSE inside knitr::opts_chunk$set() in the setup chunk.\n  To avoid running the code:\nUse eval=FALSE in the header of the install-package code chunk.\n  To avoid printing the code:\nUse the echo option in the header of the print-tibble code chunk.\n  Figure sizing:\nThere are two types of sizes that you can set: the size at which R creates figures (fig.width and fig.height), and the size at which the figures are inserted in the document (out.width and out.height). The former will effectively only control relative font and point sizes, whereas the latter controls the \u0026ldquo;actual\u0026rdquo; / final size. For more details and advice, see this section in R for Data Science.\nThe aspect ratio (fig.asp) is height/width, so a value smaller than one creates a wide figure and a value larger than one creates a narrow figure.\nHere, we\u0026rsquo;ve been setting width only \u0026ndash; you can also set fig.height and out.height, but these options become redundant when you set the width and the aspect ratio.\n     Solutions (click here)    To suppress messages and warnings throughout:\nknitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)\n  To avoid running the code: {r install-package, eval=FALSE}\n  To avoid printing the code:\n{r print-tibble, echo=FALSE}\n  Figure sizing:\nExample settings for better-sized figures \u0026ndash;\n{r plot-bills, out.width=\u0026quot;80%\u0026quot;, fig.width=6, fig.asp=0.7}\n  A code chunk to print the penguins_raw tibble (replace single quotes by backticks):\n  '''{r} penguins_raw '''      Bonus 1: Markdown and inline code The formatting for the prose in our document could also be improved. For instance:\n  Use inline code formatting in a couple of cases where this is appropriate.\n  Instead of simply saying \u0026ldquo;8 variables (n = 344 penguins)\u0026rdquo; (under the Summary of the dataset\u0026quot; heading), use inline R code that makes these calculations and print the results.\n  Try a couple of other things: heading levels (one of them is currently not right!), italic text, bold text, and/or ordered (numbered) and unordered (bulleted) lists.\n    Hints (click here)     Simply put backticks around the inline text you want have formatted as code. You can do this, for instance, for mentions of palmerpenguins::penguins.\n  For inline code that runs, use `r my-code`.\nThe number of variables and penguins in the penguin dataset are the number of columns and rows, respectively, in the penguin tibble.\n     Solutions (click here)  Inline calculation of the number of variables and penguins:\n[...] that contains `r ncol(penguins)` variables (n = `r nrow(penguins)` penguins).      Bonus 2: Other output formats Try one or more output formats other than html_document, see this website for the list of available options. If you want to try presentations, note that three dashes --- are used to separate slides.\n It might be confusing that on the website linked to above (see also the screenshot in section V), the output formats are listed functions (html_document() rather than html_document) \u0026ndash; but this is simply because under the hood, these functions are called via the YAML header.\n    \n Go further Pitfalls / Tips   The working directory\nBy default, the working directory for an R Markdown document is the directory in which the file resides.\nThis can be a bit annoying if you\u0026rsquo;re used to using your project\u0026rsquo;s root directory as your working directory (which you should be) and the R Markdown file is not in the project\u0026rsquo;s root directory (which it probably shouldn\u0026rsquo;t be). Nevertheless, simply using ../ notation to move one or two directories up should generally work.\nIf you really need to set a different working directory, you should be aware that surprisingly, setting the working directory with setwd() in a code chunk is not persistent across code chunks. To set a different working directory for the entire document, use knitr::opts_knit$set(root.dir = '/my/working/dir') in a setup chunk.\n  Chunk labels\nChunk labels are optional but if you do give them, note that they have to be unique: the document will fail to render if have two chunks with the same label. Also, avoid using spaces and underscores in the labels (good-chunk-label, bad chunk label, bad_chunk_label).\n  Tables   Tables produced by Markdown text\nThe syntax for basic Markdown tables is as follows:\n| Time | Session | Topic | |:--------------|:-------:|---------:| | _left_ | _center_| _right_ | | Wed 5 pm | 1 | Getting started | | Fri 3 pm | | | | Wed 5 pm | 2 | *dplyr* | | Fri 3 pm | | *Break* |     Time Session Topic     left center right   Wed 5 pm 1 Getting started   Fri 3 pm     Wed 5 pm 2 dplyr   Fri 3 pm  Break    In the Visual Markdown editor in RStudio, you can simply insert a table with a little dialogue box after clicking Table =\u0026gt; Insert Table.\n  Tables (dataframes) produced by R code\nUsing kable(my_df) in a code chunk will create nicer output for individual dataframes (recall the df_print: kable YAML option for document-wide \u0026ldquo;kable\u0026rdquo; printing).\nThere are many packages available for more advanced options, such as GT, DT, and reactable.\n  Websites Note that rmarkdown::render_site() can create simple websites that connects multiple pages with a navigation bar. All you need is a simple YAML file called _site.yml with some settings, and a file for the front page which needs to be called index.Rmd. See here in the R Markdown book for more details.\nOptions with more features, like a blog, are distill websites, and the blogdown package for Hugo sites.\nFurther resources  Free online books by the primary creator of R Markdown and other authors:  R Markdown \u0026ndash; The Definitive Guide R Markdown Cookbook   RStudio\u0026rsquo;s 5-page R Markdown Reference PDF RStudio\u0026rsquo;s R Markdown Cheatsheet RStudio R Markdown lessons Markdown tutorial  ","date":1611446400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611941749,"objectID":"c2268ec4049a3a40fece1f72853db533","permalink":"https://biodash.github.io/codeclub/07_rmarkdown/","publishdate":"2021-01-24T00:00:00Z","relpermalink":"/codeclub/07_rmarkdown/","section":"codeclub","summary":"In this 7th session of Code Club, we will learn about Markdown syntax and some of the great functionality of R Markdown.","tags":["codeclub","markdown","rmarkdown"],"title":"Session 7: R Markdown","type":"codeclub"},{"authors":["Stephen Opiyo"],"categories":null,"content":"\n Factors form the basis for many powerful operations in R, including many performed on tabular data. The motivation for factors comes from the notion of categorical variables. These variables are non-numeric in nature corresponding to categories such as male and female, or Democrat, Republican and Independent.\nA factor might be viewed simply as a vector with a bit of more information added. The extra information consists of a record of distinct values in that vector, which are called: levels.\nLet us look at some examples of factors. We will make use of the package forcats, which is one of the 8 core tidyverse packages. Therefore, we start by loading the tidyverse:\nlibrary(tidyverse) #\u0026gt; ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── #\u0026gt; ✔ ggplot2 3.3.3 ✔ purrr  0.3.4 #\u0026gt; ✔ tibble  3.0.4 ✔ dplyr  1.0.2 #\u0026gt; ✔ tidyr  1.1.2 ✔ stringr 1.4.0 #\u0026gt; ✔ readr  1.4.0 ✔ forcats 0.5.0 #\u0026gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── #\u0026gt; ✖ dplyr::filter() masks stats::filter() #\u0026gt; ✖ dplyr::lag() masks stats::lag() ## Check whether \"forcats\" is listed among the loaded packages. ## Alternatively, you could load \"forcats\" (and \"ggplot2\") separately: # install.packages(\"forcats\") # library(forcats) # library(ggplot2)    Example 1: From a numeric vector to a factor Let us create a factor xf from a vector x with the numbers 5, 12, 13, and 12:\nx \u0026lt;- c(5,12, 13,12) x #\u0026gt; [1] 5 12 13 12 # Convert the vector to a factor: xf \u0026lt;- factor(x) xf #\u0026gt; [1] 5 12 13 12 #\u0026gt; Levels: 5 12 13   The distinct values in xf are 5, 12 and 13, and are listed as levels.\nLet us look in a bit more details at our factor using the R functions str and unclass:\nstr(xf) #\u0026gt; Factor w/ 3 levels \"5\",\"12\",\"13\": 1 2 3 2 unclass(xf) #\u0026gt; [1] 1 2 3 2 #\u0026gt; attr(,\"levels\") #\u0026gt; [1] \"5\" \"12\" \"13\"   Notice that the values in the factor are not stored as (5, 12, 13, 12), but rather as (1, 2, 3, 2)!\nThis means that our data consists first of a level-1 value, then level-2 and level 3 values, and finally another level-2 value. So, the data has been recorded by level.\nThe values attached to each level are recorded too, but as characters such as \u0026quot;5\u0026quot; rather than as numbers such as 5.\n Example 2: From a character vector to a factor We will use the levels Democrat (D), Republican (R), and Independent (I). First, we save a vector:\ny \u0026lt;- c(\"D\", \"R\", \"R\", \"I\", \"R\", \"I\", \"D\", \"I\") y #\u0026gt; [1] \"D\" \"R\" \"R\" \"I\" \"R\" \"I\" \"D\" \"I\" str(y) #\u0026gt; chr [1:8] \"D\" \"R\" \"R\" \"I\" \"R\" \"I\" \"D\" \"I\"   Then, we again convert the vector to a factor, and look at the levels:\nfy \u0026lt;- factor(y) fy #\u0026gt; [1] D R R I R I D I #\u0026gt; Levels: D I R unclass(fy) #\u0026gt; [1] 1 3 3 2 3 2 1 2 #\u0026gt; attr(,\"levels\") #\u0026gt; [1] \"D\" \"I\" \"R\"    Example 3: Ordering factor levels Some variables can be challenging to sort automatically, because the desired sorting order is not alphabetical or numeric.\nFor instance, months that are listed using characters:\nmonths_vector \u0026lt;- c(\"Dec\", \"Apr\", \"Jan\", \"Mar\") # Try to sort using the `sort` function sort(months_vector) #\u0026gt; [1] \"Apr\" \"Dec\" \"Jan\" \"Mar\"   That didn\u0026rsquo;t sort in a useful way. But, the problem can be fixed by using a factor.\nFirst, we create a list of the valid levels, which are all 12 months in a year:\nmonth_levels \u0026lt;- c(\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\")   Then we convert the vector into a factor, like before, but now we additionally specify the desired levels of the factor, in order, using the levels argument:\nmonths_factor \u0026lt;- factor(months_vector, levels = month_levels)   Now it sorts the way we want to!\nsort(months_factor) #\u0026gt; [1] Jan Mar Apr Dec #\u0026gt; Levels: Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec    Example 4: Use of factors in plots with forcats 4A: Plot after reordering manually with fct_relevel() We will use the mtcars data, which was extracted from the 1974 Motor Trend US magazine, and comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973\u0026ndash;74 models) \u0026ndash; a data frame with 32 observations for 11 (numeric) variables,\ndata(mtcars) names(mtcars) #\u0026gt; [1] \"mpg\" \"cyl\" \"disp\" \"hp\" \"drat\" \"wt\" \"qsec\" \"vs\" \"am\" \"gear\" #\u0026gt; [11] \"carb\" dim(mtcars) #\u0026gt; [1] 32 11 str(mtcars) #\u0026gt; 'data.frame': 32 obs. of 11 variables: #\u0026gt; $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... #\u0026gt; $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... #\u0026gt; $ disp: num 160 160 108 258 360 ... #\u0026gt; $ hp : num 110 110 93 110 175 105 245 62 95 123 ... #\u0026gt; $ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ... #\u0026gt; $ wt : num 2.62 2.88 2.32 3.21 3.44 ... #\u0026gt; $ qsec: num 16.5 17 18.6 19.4 17 ... #\u0026gt; $ vs : num 0 0 1 1 0 1 0 1 1 1 ... #\u0026gt; $ am : num 1 1 1 0 0 0 0 0 0 0 ... #\u0026gt; $ gear: num 4 4 4 3 3 3 3 4 4 4 ... #\u0026gt; $ carb: num 4 4 1 1 2 1 4 2 2 4 ...   we will select six variables (mpg, cyl, disp, hp, and wt) to create a dataset Data.\n mpg: Miles per (US) gallon, cyl: Number of cylinders disp: Displacement (cu.in.) hp: Horse power wt: Weight (in 1000 lbs)  Data \u0026lt;- mtcars %\u0026gt;% select(\"mpg\", \"cyl\", \"disp\", \"hp\", \"wt\")   Now, we\u0026rsquo;ll add a new column cyl_chr by converting cyl from numeric to character:\nData \u0026lt;- Data %\u0026gt;% mutate(cyl_chr = recode(cyl,`4` = \"Four\", `6` = \"Six\", `8` = \"Eight\")) head(Data) #\u0026gt; mpg cyl disp hp wt cyl_chr #\u0026gt; 1 21.0 6 160 110 2.620 Six #\u0026gt; 2 21.0 6 160 110 2.875 Six #\u0026gt; 3 22.8 4 108 93 2.320 Four #\u0026gt; 4 21.4 6 258 110 3.215 Six #\u0026gt; 5 18.7 8 360 175 3.440 Eight #\u0026gt; 6 18.1 6 225 105 3.460 Six   We plot a bar chart for cyl_chr:\nData %\u0026gt;% ggplot(aes(x = cyl_chr)) + geom_bar()   In the plot, the levels of the factor were arranged in alphabetical order (Eight, Four, and Six).\nInstead, we want the bar graph arranged in the order Four, Six, and Eight.\nAn alternative to using factor(levels = ...) like we did above, is to use the fct_relevel() function from the forcats package:\nData %\u0026gt;% mutate(cyl_chr = fct_relevel(cyl_chr, \"Four\", \"Six\", \"Eight\")) %\u0026gt;% ggplot(aes(x = cyl_chr)) + geom_bar() + labs(x = \"Cylinder\", y = \"Number of cars\")   4B: Plot after reordering by the value of another column (fct_reorder) Create a dataset called Data_a:\nData_a \u0026lt;- data.frame(name = c(\"North\", \"South\", \"East\", \"West\"), var = sample(seq(1, 10), 4))   Plot a bar chart of Data_a:\nData_a %\u0026gt;% ggplot(aes(x = name, y = var)) + geom_bar(stat = \"identity\", fill = \"#f68034\", alpha = 0.6, width = 0.4)   Reorder following the value of another column using the fct_reorder() function, and flip the plot:\nData_a %\u0026gt;% mutate(name = fct_reorder(name, var)) %\u0026gt;% ggplot(aes(x = name, y = var)) + geom_bar(stat = \"identity\", fill = \"#f68034\", alpha = 0.6, width = 0.4) + coord_flip()   There are several more convenient reordering functions in the forcats package, including:\n  fact_infreq() to reorder by occurrence frequencies of each level (see the picture at the top of the post).\n  fct_inorder() to reorder by order of appearance in the dataframe. This can be useful, for example, if your dataframe has already been sorted properly, and you just need to prevent automatic alphabetic reordering when plotting.\n   Breakout rooms! For the Breakout room exercises, we will use datasets from mtcars and the gss_cat dataset from the forcats package.\nExercise 1  Convert the variable gear from mtcars to a character vector with words for each number (link in example 4A), and plot a bar chart.\nThen, use a factor to reorder the bars to appear in the regular \u0026ldquo;numeric\u0026rdquo; order: \u0026ldquo;Three\u0026rdquo; then \u0026ldquo;Four\u0026rdquo; then \u0026ldquo;Five\u0026rdquo;.\n  Hints (click here)     First, create a dataframe with a column that codes the gears as words, using the mutate() and recode() functions.\n  Then, create a factor from this modified gear column, and order it manually using the fct_relevel() function.\n     Solutions (click here)   Start by loading the dataset:  data(\"mtcars\")    Now, create a new dataset Gear from mtcars, adding a column gear_chr:  gear_df \u0026lt;- mtcars %\u0026gt;% mutate(gear_chr = recode(gear, `3`= \"Three\", `4` =\"Four\", `5`= \"Five\")) head(gear_df) #\u0026gt; mpg cyl disp hp drat wt qsec vs am gear carb gear_chr #\u0026gt; 1 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 Four #\u0026gt; 2 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 Four #\u0026gt; 3 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 Four #\u0026gt; 4 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 Three #\u0026gt; 5 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 Three #\u0026gt; 6 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 Three    Finally, use the forcats function fct_relevel() to rearrange gear_chr in nonalphabetical order, and plot the barchart using geom_bar():  gear_df %\u0026gt;% mutate(gear_fct = fct_relevel(gear_chr, \"Three\", \"Four\", \"Five\")) %\u0026gt;% ggplot(aes(x = gear_fct)) + geom_bar() + labs(x = \"Gear\", y = \"Number of cars\")       Exercise 2  Using the gss_cat dataset from the forcats package (available as gsscat in your environment), create a plot that compares the average number of hours spent watching TV per day across religions, and where religions are ordered by the average number of hours.\n(Despite what we\u0026rsquo;ve learned last week, start by merely plotting the mean, and no distributions, using a barplot or with geom_point().)\nSource: (R for Data Science)\n  Hints (click here)  In order to be able to order the factor by the average number of hours spent watching TV, first compute this average per religion, and save the results in a dataframe (use `mutate()` and `summarize()`). Then, use fct_recorder() to reorder the factor.\n   Solutions (click here)  First, have a look at the dataset:\nforcats::gss_cat #\u0026gt; # A tibble: 21,483 x 9 #\u0026gt; year marital age race rincome partyid relig denom tvhours #\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 2000 Never ma… 26 White $8000 to … Ind,near r… Protesta… Souther… 12 #\u0026gt; 2 2000 Divorced 48 White $8000 to … Not str re… Protesta… Baptist… NA #\u0026gt; 3 2000 Widowed 67 White Not appli… Independent Protesta… No deno… 2 #\u0026gt; 4 2000 Never ma… 39 White Not appli… Ind,near r… Orthodox… Not app… 4 #\u0026gt; 5 2000 Divorced 25 White Not appli… Not str de… None Not app… 1 #\u0026gt; 6 2000 Married 25 White $20000 - … Strong dem… Protesta… Souther… NA #\u0026gt; 7 2000 Never ma… 36 White $25000 or… Not str re… Christian Not app… 3 #\u0026gt; 8 2000 Divorced 44 White $7000 to … Ind,near d… Protesta… Luthera… NA #\u0026gt; 9 2000 Married 44 White $25000 or… Not str de… Protesta… Other 0 #\u0026gt; 10 2000 Married 47 White $25000 or… Strong rep… Protesta… Souther… 3 #\u0026gt; # … with 21,473 more rows   Then, calculate the mean number of tv-hours and create a plot:\nrelig \u0026lt;- gss_cat %\u0026gt;% group_by(relig) %\u0026gt;% summarize(tvhours = mean(tvhours, na.rm = TRUE)) #\u0026gt; `summarise()` ungrouping output (override with `.groups` argument) ggplot(relig, aes(tvhours, relig)) + geom_point()   It is difficult to interpret this plot because there is no overall pattern.\nWe can improve the plot by reordering the level of religion using fct_reorder():\nrelig \u0026lt;- gss_cat %\u0026gt;% group_by(relig) %\u0026gt;% summarize(tvhours = mean(tvhours, na.rm = TRUE)) #\u0026gt; `summarise()` ungrouping output (override with `.groups` argument) relig %\u0026gt;% mutate(relig = fct_reorder(relig, tvhours)) %\u0026gt;% ggplot(aes(tvhours, relig)) + geom_point()   Reordering religion makes it much easier to see that people in the \u0026ldquo;Don\u0026rsquo;t know\u0026rdquo; category watch much more TV.\n    Bonus: Exercise 3  In exercise 2, we saw large differences in the average time spent watching TV across religions, but we should perhaps have a closer look at the data by plotting distributions.\nGo back to the previous Code Club session and decide which type of plot could be ideal with so many categories.\n  Hints (click here)  [`geom_density_ridges()`](https://wilkelab.org/ggridges/reference/geom_density_ridges.html) from the *ggridges* package is very well suited for a plot with so many categories.    Solutions (click here)  library(ggridges) ggplot(gss_cat, aes(x = tvhours, y = relig, fill = relig)) + geom_density_ridges(alpha = 0.8) + labs(x = 'Number of hours spent watching TV', y = 'Religion') + guides(fill = FALSE) + theme_minimal() #\u0026gt; Picking joint bandwidth of 0.586 #\u0026gt; Warning: Removed 10146 rows containing non-finite values (stat_density_ridges).      ","date":1610928000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611345251,"objectID":"a8e91bcb80337e5e587f785ed259f392","permalink":"https://biodash.github.io/codeclub/06_factors/","publishdate":"2021-01-18T00:00:00Z","relpermalink":"/codeclub/06_factors/","section":"codeclub","summary":"In this sixth session of Code Club, we'll learn how to use factors to our advantage","tags":["codeclub","factors"],"title":"Session 6: Factors","type":"codeclub"},{"authors":["Jessica Cooperstone"],"categories":null,"content":"\n Prep homework Basic computer setup   If you didn\u0026rsquo;t already do this, please follow the Code Club Computer Setup instructions, which also has pointers for if you\u0026rsquo;re new to R or RStudio.\n  If you\u0026rsquo;re able to do so, please open RStudio a bit before Code Club starts \u0026ndash; and in case you run into issues, please join the Zoom call early and we\u0026rsquo;ll troubleshoot.\n  New to ggplot? Check out the last Code Club Session 4 on Visualizing Data.\nIf you\u0026rsquo;ve never used ggplot2 before (or even if you have), you may find this cheat sheet useful.\n Getting Started Script for today\u0026rsquo;s session # directory for Code Club Session 2: dir.create(\"S05\") # directory for our script # (\"recursive\" to create two levels at once.) dir.create(\"S05/scripts/\") # save the url location for today's script todays_R_script \u0026lt;- 'https://raw.githubusercontent.com/biodash/biodash.github.io/master/content/codeclub/05_ggplot-round-2/Session5_ggplot2.R' # indicate the name of the new script file Session5_script \u0026lt;- \"S05/scripts/Session5_script.R\" # go get that file!  download.file(url = todays_R_script, destfile = Session5_script)    1 - Why visualize our data? Artwork by Allison Horst\nWe make data visualizations for two main reasons:\n To explore our data To share our data with others  Often, we think about figure generation as the last part of the scientic process, something you do as you prepare a manuscript for publication. I hope to convince you that exploring your data, and making exploratory plots is a critical part of the data analysis and interpretation process.\nToday we will be using ggplot2 to make a series of plots that help us better understand the underlying structure in our dataset.\nWhen summary statistics don\u0026rsquo;t cut it\nThis \u0026ldquo;Datasaurus Dozen\u0026rdquo; shows the value of looking at your data beyond means and standard deviations. In the gif above, created by Alberto Cairo, each of these 13 datasets have identical means, standard eviations, and correlations to two decimal places. And one of the datasets is a dinosaur!\n What will we go over today\nThese geoms will help you to get better acquainted with your data.\n geom_col() - makes bar plots. I will show you how to do this and then recommend that you don\u0026rsquo;t. geom_boxplot() - makes infinitely useful boxplots. geom_violin() - makes violin plots, a hybrid between a boxplot and a density plot. Very musical. geom_density_ridges() - a density plot giving you the impression of a side view of a mountain range. Requires the package ggridges geom_jitter() - adds all datapoints to your plot, and jitters them to handle overplotting.  I will also go over a few tricks along the way, including coord_flip(), adding labels using labs(), and changing the overall look of the plot with theme(), or pre-set themes like theme_classic() which is my go-to.\n   2 - Accessing our data Let\u0026rsquo;s get set up and grab some data so that we can learn more about penguins (and ggplot2)\n You can do this locally, or at OSC. You can find instructions if you are having trouble here.  First load your libraries.\nlibrary(tidyverse)   Then let\u0026rsquo;s access the wintry palmerpenguins dataset. We will then look at penguins, the dataset we will be using for the first part of today\u0026rsquo;s Code Club. This data is collected on penguins from the Palmer Station Antarctica Long-Term Ecological Research study area.\nArtwork by Allison Horst\ninstall.packages(\"palmerpenguins\")   library(palmerpenguins)   Let\u0026rsquo;s look at the data in penguins.\n# look at the first 6 rows, all columns head(penguins) #\u0026gt; # A tibble: 6 x 8 #\u0026gt; species island bill_length_mm bill_depth_mm flipper_length_… body_mass_g sex  #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; #\u0026gt; 1 Adelie Torge… 39.1 18.7 181 3750 male  #\u0026gt; 2 Adelie Torge… 39.5 17.4 186 3800 fema… #\u0026gt; 3 Adelie Torge… 40.3 18 195 3250 fema… #\u0026gt; 4 Adelie Torge… NA NA NA NA NA  #\u0026gt; 5 Adelie Torge… 36.7 19.3 193 3450 fema… #\u0026gt; 6 Adelie Torge… 39.3 20.6 190 3650 male  #\u0026gt; # … with 1 more variable: year \u0026lt;int\u0026gt; # check the structure # this tell us what is contained within our df glimpse(penguins) #\u0026gt; Rows: 344 #\u0026gt; Columns: 8 #\u0026gt; $ species \u0026lt;fct\u0026gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Ade… #\u0026gt; $ island \u0026lt;fct\u0026gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgers… #\u0026gt; $ bill_length_mm \u0026lt;dbl\u0026gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1,… #\u0026gt; $ bill_depth_mm \u0026lt;dbl\u0026gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1,… #\u0026gt; $ flipper_length_mm \u0026lt;int\u0026gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 18… #\u0026gt; $ body_mass_g \u0026lt;int\u0026gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475,… #\u0026gt; $ sex \u0026lt;fct\u0026gt; male, female, female, NA, female, male, female, mal… #\u0026gt; $ year \u0026lt;int\u0026gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 200…   This dataset contains the following measurements about penguins at Palmer Station in Antarctica:\n species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex year  We are going to be plotting to get an understanding of bill_length_mm which is the length of the bill from the penguins face, protruding outwards (and more easily understood in the image below).\nArtwork by Allison Horst\n 3 - Removing NAs Sometimes you will have NAs (or missing data). That might be informative to you, but here we are going to remove missing data using drop_na(), and assign it to a new dataframe called penguins_noNA.\n# check dimensions of penguins dim(penguins) #\u0026gt; [1] 344 8 # remove NAs penguins_noNA \u0026lt;- penguins %\u0026gt;% drop_na() dim(penguins_noNA) # we have removed 11 observations #\u0026gt; [1] 333 8   Note - by removing NAs, we have gotten rid of 11 observations\n 4 - Bar charts with geom_col() and stat_summary() Often, people use bar charts, representing the height or the length of the bar as proportional to the average value that it represents. These charts are sometimes called dynamite plots because they resemble (when they have an error bar with whisker) those cartoon style dynamite sticks. Pow!\nHowever, these bar charts, even if you add a standard deviation/error, really can hide the true distribution of your data, and for this reason, I and others hope you don\u0026rsquo;t select to make them.\nI hope after today, you see that there is always a better chart type to make than a bar chart. But I will show you how to make them anyway.\nBefore we plot, let\u0026rsquo;s calculate some summary statistics so we know what we should expect.\n# calculating mean bill_length_mm by species penguins_noNA %\u0026gt;% group_by(species) %\u0026gt;% summarize(mean_bill_length = mean(bill_length_mm)) #\u0026gt; `summarise()` ungrouping output (override with `.groups` argument) #\u0026gt; # A tibble: 3 x 2 #\u0026gt; species mean_bill_length #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 Adelie 38.8 #\u0026gt; 2 Chinstrap 48.8 #\u0026gt; 3 Gentoo 47.6   Just calling geom_col() does not give us what we want. Look at the y-axis scale and how out of line this is with our summary statistics.\n# bar plot with geom_col() # this is wrong! penguins_noNA %\u0026gt;% ggplot(aes(x = species, y = bill_length_mm)) + geom_col()   Using geom_col() the right way.\n# bar plot, the right way with geom_col() penguins_noNA %\u0026gt;% group_by(species) %\u0026gt;% summarize(mean_bill_length = mean(bill_length_mm)) %\u0026gt;% ggplot(aes(x = species, y = mean_bill_length)) + geom_col() #\u0026gt; `summarise()` ungrouping output (override with `.groups` argument)  # or you could do this in a less bulky way with stat_summary() penguins_noNA %\u0026gt;% ggplot(aes(x = species, y = bill_length_mm)) + stat_summary(fun = \"mean\", geom = \"bar\")    5 - Boxplots with geom_boxplot() A boxplot has the benefit of showing you more than the median and the standard deviation, so you can better see the true distribution of your data. In geom_boxplot():\n lower whisker = smallest observation greater than or equal to lower hinge - 1.5 * IQR lower hinge/bottom line of box part of boxplot = 25% quantile middle = median, 50% quantile upper hinge/top line of box part of boxplot = 75% quantile upper whisker = largest observation less than or equal to upper hinge + 1.5 * IQR  # vertical boxplot penguins_noNA %\u0026gt;% ggplot(aes(x = species, y = bill_length_mm)) + geom_boxplot()   Adding coord_flip() makes your vertical boxplot horizontal. You could do the same thing by flipping the variables on the x and y mappings.\n# horizontal boxplot penguins_noNA %\u0026gt;% ggplot(aes(x = species, y = bill_length_mm)) + geom_boxplot() + coord_flip()   Look at how much more information we have here than in our bar plots!\n 5 - Violin plots with geom_violin() A violin plot is boxplot-esque, but shows a mirrored density distribution. This type of plot is useful when you are trying to particularly show data distribution.\nNote here I have also mapped species to color, within the aes statement so it will apply globally to this plot.\n# violin plot penguins_noNA %\u0026gt;% ggplot(aes(x = species, y = bill_length_mm, color = species)) + geom_violin()   Adding geom_point() lets you add another layer of all the actual data points, on top of your violin plot. Remember that this is inherent in the design of ggplot2, that you can layer your plots, of different types, on top of each other.\n# violin plot with data points overlaid penguins_noNA %\u0026gt;% ggplot(aes(x = species, y = bill_length_mm, fill = species)) + geom_violin() + geom_point()   Note, I am now mapping species to fill instead of color. See the difference?\nThis doesn\u0026rsquo;t look too good because of overplotting, i.e., the smear of datapoints that doesn\u0026rsquo;t give you much information about distribution.\nWe can add geom_jitter() to introduce some small amount of randomness to our points to make us able to see them better. Seeing all your data points also lets the reader easily get a sense of your sample size.\n# violin plot with data points jittered penguins_noNA %\u0026gt;% ggplot(aes(x = species, y = bill_length_mm, fill = species)) + geom_violin() + geom_jitter()   geom_jitter() is a specialized version of geom_point(), but you could replace the geom_jitter() call with geom_point(position = \u0026quot;jitter) and get the same result. You can also use geom_point(position = position_jitterdodge()) if you only want jitter in the x, and don\u0026rsquo;t want any jitter in the y.\nWow, we now have so much more information about our data!\n 6 - Dot plots with geom_dotplot() A dot plot plots each individual datapoint, and can stack how you like. These look a lot like the SigmaPlot plots to me.\n binaxis can be set to \u0026ldquo;x\u0026rdquo; or \u0026ldquo;y\u0026rdquo; stackdir indicates how to stack the dots: \u0026ldquo;up\u0026rdquo; (default), \u0026ldquo;down\u0026rdquo;, \u0026ldquo;center\u0026rdquo;, \u0026ldquo;centerwhole\u0026rdquo; (centered, but with dots aligned) dotsize indicates the size of the dots, with 1 as default  # dotplot penguins_noNA %\u0026gt;% ggplot(aes(x = species, y = bill_length_mm, fill = species)) + geom_dotplot(binaxis = \"y\", stackdir = \"center\", dotsize = 0.5) #\u0026gt; `stat_bindot()` using `bins = 30`. Pick better value with `binwidth`.    7 - Density ridge plots with geom_density_ridges() A density ridge plots with geom_density_ridges() requires the packages ggridges, and make multiple density plots in a staggered orientation.\nYou can adjust scale within geom_density_ridges() to adjust the size of each density plot, though I have left it on the default. Adding alpha sets transparency.\n# install.packages(\"ggridges\") library(ggridges) penguins_noNA %\u0026gt;% ggplot(aes(x = bill_length_mm, y = species, fill = species)) + geom_density_ridges(alpha = 0.8) #\u0026gt; Picking joint bandwidth of 1.08    8 - ggplot is made for layering! I have shown you a bunch of different plot types, and you can combine many of them together. Here is an example of combining geom_violin() and geom_jitter(), while mapping new variables to aesthetics.\npenguins_noNA %\u0026gt;% ggplot(aes(x = species, y = bill_length_mm, color = sex, shape = island, group = species)) + geom_violin() + geom_jitter(position = position_jitterdodge(jitter.width = 2))    9 - Increase clarity and visual appeal We can quickly make our plot:\n prettier by setting a theme more clear by setting plot labels (eg., axes, titles, legend) with labs  penguins_noNA %\u0026gt;% ggplot(aes(x = species, y = bill_length_mm, color = sex, shape = island, group = species)) + geom_violin() + geom_jitter(position = position_jitterdodge(jitter.width = 2), alpha = 0.7) + theme_classic() + labs(title = \"Penguin Bill Length by Species, Sex and Location\", subtitle = \"Collected at Palmer Station, Antarctica\", x = \"Penguin Species\", # x axis label y = \"Bill length (mm)\", # y axis label color = \"Sex\", # legend title shape = \"Island\") # legend title    10 - Breakout rooms! Main exercises Get data We are going to use the NHANES dataset we used in Session 3 on joining. What was that data about again? Let\u0026rsquo;s refresh our memory.\n library(NHANES) knitr::kable(head(NHANES))     ID SurveyYr Gender Age AgeDecade AgeMonths Race1 Race3 Education MaritalStatus HHIncome HHIncomeMid Poverty HomeRooms HomeOwn Work Weight Length HeadCirc Height BMI BMICatUnder20yrs BMI_WHO Pulse BPSysAve BPDiaAve BPSys1 BPDia1 BPSys2 BPDia2 BPSys3 BPDia3 Testosterone DirectChol TotChol UrineVol1 UrineFlow1 UrineVol2 UrineFlow2 Diabetes DiabetesAge HealthGen DaysPhysHlthBad DaysMentHlthBad LittleInterest Depressed nPregnancies nBabies Age1stBaby SleepHrsNight SleepTrouble PhysActive PhysActiveDays TVHrsDay CompHrsDay TVHrsDayChild CompHrsDayChild Alcohol12PlusYr AlcoholDay AlcoholYear SmokeNow Smoke100 Smoke100n SmokeAge Marijuana AgeFirstMarij RegularMarij AgeRegMarij HardDrugs SexEver SexAge SexNumPartnLife SexNumPartYear SameSex SexOrientation PregnantNow     51624 2009_10 male 34 30-39 409 White NA High School Married 25000-34999 30000 1.36 6 Own NotWorking 87.4 NA NA 164.7 32.22 NA 30.0_plus 70 113 85 114 88 114 88 112 82 NA 1.29 3.49 352 NA NA NA No NA Good 0 15 Most Several NA NA NA 4 Yes No NA NA NA NA NA Yes NA 0 No Yes Smoker 18 Yes 17 No NA Yes Yes 16 8 1 No Heterosexual NA   51624 2009_10 male 34 30-39 409 White NA High School Married 25000-34999 30000 1.36 6 Own NotWorking 87.4 NA NA 164.7 32.22 NA 30.0_plus 70 113 85 114 88 114 88 112 82 NA 1.29 3.49 352 NA NA NA No NA Good 0 15 Most Several NA NA NA 4 Yes No NA NA NA NA NA Yes NA 0 No Yes Smoker 18 Yes 17 No NA Yes Yes 16 8 1 No Heterosexual NA   51624 2009_10 male 34 30-39 409 White NA High School Married 25000-34999 30000 1.36 6 Own NotWorking 87.4 NA NA 164.7 32.22 NA 30.0_plus 70 113 85 114 88 114 88 112 82 NA 1.29 3.49 352 NA NA NA No NA Good 0 15 Most Several NA NA NA 4 Yes No NA NA NA NA NA Yes NA 0 No Yes Smoker 18 Yes 17 No NA Yes Yes 16 8 1 No Heterosexual NA   51625 2009_10 male 4 0-9 49 Other NA NA NA 20000-24999 22500 1.07 9 Own NA 17.0 NA NA 105.4 15.30 NA 12.0_18.5 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA No NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA 4 1 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA   51630 2009_10 female 49 40-49 596 White NA Some College LivePartner 35000-44999 40000 1.91 5 Rent NotWorking 86.7 NA NA 168.4 30.57 NA 30.0_plus 86 112 75 118 82 108 74 116 76 NA 1.16 6.70 77 0.094 NA NA No NA Good 0 10 Several Several 2 2 27 8 Yes No NA NA NA NA NA Yes 2 20 Yes Yes Smoker 38 Yes 18 No NA Yes Yes 12 10 1 Yes Heterosexual NA   51638 2009_10 male 9 0-9 115 White NA NA NA 75000-99999 87500 1.84 6 Rent NA 29.8 NA NA 133.1 16.82 NA 12.0_18.5 82 86 47 84 50 84 50 88 44 NA 1.34 4.86 123 1.538 NA NA No NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA 5 0 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA    # kable just formats as a scrollable table for this website # you can just use head(NHANES) or glimpse(NHANES)    Exercise 1  Create a new data frame includes the NHANES data only from individuals that are 20 years of age or older, and removes observations where there are NAs for either age subdivided by decade (AgeDecade) or total cholesterol (TotChol).\n  Hints (click here)  Try using a series of filter() statements. Remember, you can tell filter what you want, or what you don\u0026rsquo;t want. You can filter for if specific variables have NAs by using is.na() on your variable of interest. Also remember that ! means \u0026ldquo;not.\u0026rdquo; You will notice that if you want to use drop_NA() you need to specific which specific variables you want to use, or you will inadvertably drop a lot of observations which have missing data for variables other than those we are plotting..    Solutions (click here)  # here are a few ways to do this NHANES_over20_noNA \u0026lt;- NHANES %\u0026gt;% filter(Age \u0026gt;20) %\u0026gt;% drop_na(AgeDecade, TotChol) dim(NHANES_over20_noNA) #\u0026gt; [1] 6408 76 NHANES_over20_noNA \u0026lt;- NHANES %\u0026gt;% filter(Age \u0026gt;20, !is.na(AgeDecade), !is.na(TotChol)) dim(NHANES_over20_noNA) #\u0026gt; [1] 6408 76       Exercise 2  Create a boxplot to show the relationship between total cholesterol (TotChol) and age (AgeDecade).\n  Hints (click here)  Try geom_boxplot(). Map your variables of interest to the x and y aesthetics. Which you variable you put on x and y will determine if your boxplot is vertical or horizontal.    Solutions (click here)  NHANES_over20_noNA %\u0026gt;% ggplot(aes(x = AgeDecade, y = TotChol)) + geom_boxplot()       Exercise 3  Take your plot from Exercise 2 and make it a violin plot instead of a boxplot. Then color by age.\n  Hints (click here)  The geom for a violin plot is geom_violin(). You can change color by mapping to color or to fill.    Solutions (click here)  Note the difference between mapping to color vs. fill.\nNHANES_over20_noNA %\u0026gt;% ggplot(aes(x = AgeDecade, y = TotChol, color = AgeDecade)) + geom_violin()   NHANES_over20_noNA %\u0026gt;% ggplot(aes(x = AgeDecade, y = TotChol, fill = AgeDecade)) + geom_violin()       Exercise 4  Make add a boxplot to your violin plot from Exercise 3. Adjust the parameters so you the plot looks good to you.\n  Hints (click here)  In geom_boxplot(), you can adjust the width of the boxplot by setting width = X. A width of 1 is the default.    Solutions (click here)  NHANES_over20_noNA %\u0026gt;% ggplot(aes(x = AgeDecade, y = TotChol, color = AgeDecade)) + geom_violin() + geom_boxplot(width = 0.2)       Exercise 5  Add all of the data points on top of your boxplot from Exercise 2 of total cholesterol by age. Adjust the parameters so you the plot looks good to you. While you are at it, clean up your plot labels and give your plot a title.\n  Hints (click here)  Remember that ggplot layers your plots, so layers that are further down in your code, will be applied on top of those that come earlier.    Solutions (click here)  geom_boxplot(outlier.shape = NA) removes the outliers from geom_boxplot(), since we are plotting all of the points, we do not want the outliers appearing twice.\nNHANES_over20_noNA %\u0026gt;% ggplot(aes(x = AgeDecade, y = TotChol, color = AgeDecade)) + geom_boxplot(outlier.shape = NA) + geom_jitter(width = 0.3, alpha = 0.1) + labs(title = \"Total Cholesterol by Age\", subtitle = \"Data from the National Health and Nutrition Examination Survey (NHANES)\", x = \"Age, by Decade\", y = \"Total Cholesterol, mmol/L\", color = \"Age (years)\")       Bonus exercises Bonus 1  Make a density ridge plot for age by total cholesterol.\n  Hints (click here)  Try geom_density_ridges(), and remember, this is not a part of ggplot2, so be sure to call library(ggridges).    Solutions (click here)  # install.packages(\"ggridges\") library(ggridges) NHANES_over20_noNA %\u0026gt;% ggplot(aes(x = TotChol, y = AgeDecade, fill = AgeDecade)) + geom_density_ridges(alpha = 0.7) #\u0026gt; Picking joint bandwidth of 0.224       Bonus 2  Take your density ridge plot from Bonus 1, and try applying a theme from hrbrthemes to it.\n  Hints (click here)  hrbrthemes is not part of ggplot2 so remember to install the package, and then call library(hrbrthemes). You can google the package to see what all your theme options are. I like theme_ipsum_rc(), try that one if you like!    Solutions (click here)  # install.packages(\"hrbrthemes\") library(hrbrthemes) #\u0026gt; NOTE: Either Arial Narrow or Roboto Condensed fonts are required to use these themes. #\u0026gt; Please use hrbrthemes::import_roboto_condensed() to install Roboto Condensed and #\u0026gt; if Arial Narrow is not on your system, please see https://bit.ly/arialnarrow NHANES_over20_noNA %\u0026gt;% ggplot(aes(x = TotChol, y = AgeDecade, fill = AgeDecade)) + geom_density_ridges(alpha = 0.7, scale = 0.9) + theme_ipsum_rc() #\u0026gt; Picking joint bandwidth of 0.224       Bonus 3  Tidy up your plot from Bonus 2 by giving it a title, axis labels, and try adding the median total cholesterol to each density ridge plot.\n  Hints (click here)  Using stat_summary() will help you add the median.\n   Solutions (click here)   theme(axis.title.x = element_text(hjust = 0.5)) makes the x-axis title center justified. you can change shape within stat_summary() to be anything you like, either an R shape, a specific keyboard key, or even a pasted emoji. The default is a point. when you set a theme(), anything that comes below will override what code comes previous, so for this reason, if you are going to amend a pre-made theme, first call the pre-made theme, and then make any changes you like below.  NHANES_over20_noNA %\u0026gt;% ggplot(aes(x = TotChol, y = AgeDecade, fill = AgeDecade)) + geom_density_ridges(alpha = 0.7, scale = 0.9) + stat_summary(fun = median) + theme_ipsum_rc() + theme(axis.title.x = element_text(hjust = 0.5), axis.title.y = element_text(hjust = 0.5)) + labs(title = \"Total Cholesterol by Age\", subtitle = \"Data from the National Health and Nutrition Examination Survey (NHANES)\", x = \"Total Cholesterol, mmol/L\", y = \"Age, by Decade\", fill = \"Age (years)\") #\u0026gt; Picking joint bandwidth of 0.224 #\u0026gt; Warning: Removed 6 rows containing missing values (geom_segment).       Bonus 4  Commonly used cutoffs for cholesterol are: \u0026lt; 5.2 mmol/L is normal, 5.2-6.2 mmol/L is borderline high and \u0026gt; 6.2 mmol is high. Add a vertical cutoff line showing the level below which cholesterol would be considered normal.\n  Hints (click here)  Using geom_vline() will let you add a vertical line with an xintercept that is appropriate.    Solutions (click here)  NHANES_over20_noNA %\u0026gt;% ggplot(aes(x = TotChol, y = AgeDecade, fill = AgeDecade)) + geom_density_ridges(alpha = 0.7, scale = 0.9) + stat_summary(fun = median) + geom_vline(aes(xintercept = 5.2)) + theme_ipsum_rc() + theme(axis.title.x = element_text(hjust = 0.5), axis.title.y = element_text(hjust = 0.5)) + labs(title = \"Total Cholesterol by Age\", subtitle = \"Data from the National Health and Nutrition Examination Survey (NHANES)\", caption = \"Vertical line indicates upper limit of normal cholesterol\", x = \"Total Cholesterol, mmol/L\", y = \"Age, by Decade\", fill = \"Age (years)\") #\u0026gt; Picking joint bandwidth of 0.224 #\u0026gt; Warning: Removed 6 rows containing missing values (geom_segment).       ","date":1610668800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611108419,"objectID":"a799f7bb592979c0bb09905b9a132709","permalink":"https://biodash.github.io/codeclub/05_ggplot-round-2/","publishdate":"2021-01-15T00:00:00Z","relpermalink":"/codeclub/05_ggplot-round-2/","section":"codeclub","summary":"During this fifth session of Code Club, we will be continuing to learn to use ggplot2, including techniques that better enable us to see our true data distribution.","tags":null,"title":"Session 5: ggplot2, round 2","type":"codeclub"},{"authors":["Michael Broe"],"categories":null,"content":"\n New To Code Club?   First, check out the Code Club Computer Setup instructions, which also has some pointers that might be helpful if you\u0026rsquo;re new to R or RStudio.\n  Please open RStudio before Code Club to test things out \u0026ndash; if you run into issues, join the Zoom call early and we\u0026rsquo;ll troubleshoot.\n   Session Goals  Learn the philosophy of coding a graphic. Learn the basic template of a ggplot2 graphic, so you can reuse it for multiple chart types. Learn how you can quickly add visual information to a graphic using aesthetics and layers.   Intro: The ggplot2 philosophy We have already seen that in R, instead of manually manipulating data frames as you might do when editing Excel sheets, we code the operations we want to perform using dplyr verbs like select(), mutate(), inner_join(), and so on.\nIn a similar way when performing visualization, instead of clicking on a chart type in Excel, we code the chart in R.\nAnd just as dplyr gives us efficient ways to manipulate data frames, ggplot2 (which is also part of the tidyverse) gives us efficient ways to manipulate charts/plots/graphics (we use these terms interchangeably).\nThe gg in ggplot2 stands for grammar of graphics, a systematic approach for designing statistical plots developed by Leland Wilkinson. The idea behind this was to think about \u0026lsquo;pulling apart\u0026rsquo; various plots into their shared component pieces, then provide code that could put them together again. We can then create new plots like we create new sentences (once we understand this grammar).\nThere are two parts to this. First, the \u0026lsquo;nouns and verbs\u0026rsquo; we need to work with plots are very different than those we need to work with data frames. ggplot2 is like a mini-language of its own, with its own verbs and syntax.\nSecond, this notion of pulling apart a graphic leads to the idea of layers. You can build up a plot of any complexity by overlaying different views of the same data.\nThere\u0026rsquo;s a learning curve here for sure, but there are a couple of things that help us.\nFirst, every graphic shares a common template. This is like thinking about the sentence \u0026ldquo;The cat sat on the mat\u0026rdquo; grammatically as the template NP V PP (Noun Phrase \u0026ldquo;The cat\u0026rdquo;, Verb \u0026ldquo;sat\u0026rdquo;, Prepositional Phrase \u0026ldquo;on the mat\u0026rdquo;). Once you understand this structure you can \u0026ldquo;say\u0026rdquo; a lot of different things.\n(And I mean a lot. The ggplot cheat sheet lists over 40 plot-types, but because this is a language, users can create their own extensions that you can also utilize, adding over 80 more.)\nSecond, the way we put layers together is identical to the way we use pipes. You can read %\u0026gt;% as \u0026ldquo;and then\u0026rdquo;: select() and then mutate() and then summarize(). In graphics, we can say \u0026ldquo;show this layer, and then overlay this layer, and then overlay this layer\u0026rdquo;, etc., using a very similar syntax.\n Examples So how does this work in practice? We\u0026rsquo;ll work through visualizing the iris dataset that you\u0026rsquo;ve seen before. This is an extremely famous dataset that was first analyzed by R. A. Fisher in 1936: The use of multiple measurements in taxonomic problems. He was attempting to use petal and sepal measurements to discriminate one species from another.\nggplot2 is part of the tidyverse package so we need to load that first:\n# this assumes you've already installed tidyverse library(tidyverse) #\u0026gt; ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── #\u0026gt; ✔ ggplot2 3.3.2 ✔ purrr  0.3.4 #\u0026gt; ✔ tibble  3.0.4 ✔ dplyr  0.8.5 #\u0026gt; ✔ tidyr  1.0.3 ✔ stringr 1.4.0 #\u0026gt; ✔ readr  1.3.1 ✔ forcats 0.5.0 #\u0026gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── #\u0026gt; ✖ dplyr::filter() masks stats::filter() #\u0026gt; ✖ dplyr::lag() masks stats::lag()   And recall that the iris dataset (3 species, 50 observations per species) is automatically available to us:\nhead(iris) #\u0026gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #\u0026gt; 1 5.1 3.5 1.4 0.2 setosa #\u0026gt; 2 4.9 3.0 1.4 0.2 setosa #\u0026gt; 3 4.7 3.2 1.3 0.2 setosa #\u0026gt; 4 4.6 3.1 1.5 0.2 setosa #\u0026gt; 5 5.0 3.6 1.4 0.2 setosa #\u0026gt; 6 5.4 3.9 1.7 0.4 setosa   What is the correlation between petal length and width in these species? Are longer petals also wider? We can visualize this with a scatterplot. But first let\u0026rsquo;s look a the ggplot template. (Note the package is ggplot2, the command is ggplot.)\nggplot(data = \u0026lt;DATA\u0026gt;) + \u0026lt;GEOM_FUNCTION\u0026gt;(mapping = aes(\u0026lt;MAPPINGS\u0026gt;))  These are the obligatory parts of any plot. The first argument to ggplot() is the data frame:\nggplot(data = iris)   This is not very interesting! but it\u0026rsquo;s notable that it is something. ggplot() has created a base coordinate system (a base layer) that we can add visual layers to. The add a layer operator is \u0026ldquo;+\u0026rdquo;, which is the ggplot equivalent of the pipe symbol, and it must occur at the end of the line.\nThe next argument specifies the kind plot we want: scatterplot, bar chart, fitted line, boxplot, pie chart, etc. ggplot2 refers to these as geoms: the geometrical object that a plot uses to represent data. You can see an overview of many of these geoms in the cheat sheet. The geom for a scatterplot is geom_point().\nBut we also require a mapping argument, which maps the variables in the dataset we want to focus on to their visual representation in the plot.\nAnd finally we need to specify an aesthetic for the geometric objects in the plot, which will control things like shape, color, transparency, etc. Perhaps surprisingly, for a scatterplot, the x and y coordinates are aesthetics, since these control, not the shape or color, but the relative position of the points in the coordinate system.\nHere is our complete plot:\nggplot(data = iris) + geom_point(mapping = aes(x = Petal.Length, y = Petal.Width))   There is clearly a positive correlation between length and width. And we can make this even more apparent by visually fitting a line to the data, by overlaying another geom in the same plot.\nggplot(data = iris) + geom_point(mapping = aes(x = Petal.Length, y = Petal.Width)) + geom_smooth(mapping = aes(x = Petal.Length, y = Petal.Width)) #\u0026gt; `geom_smooth()` using method = 'loess' and formula 'y ~ x'   There is clearly some code redundancy here, and we really don\u0026rsquo;t want the x, y mapping of these two layers to be independent. We can extract the common mapping information and move it to the top level:\nggplot(data = iris, (mapping = aes(x = Petal.Length, y = Petal.Width))) + geom_point() + geom_smooth() #\u0026gt; `geom_smooth()` using method = 'loess' and formula 'y ~ x'   So we have the possibility of local layer specifications, and global specifications. Global specifications are inherited by all the local layers.\nThe power of aesthetics The aim of Fisher\u0026rsquo;s paper was to try to discriminate different species based on their morphological measurements. It looks from this plot that there are two distinct clusters. Do these clusters correspond to different species? There are two clusters, but three species. How can we explore this further?\nOur current plot uses two numeric variables: Petal.Length and Petal.width. We can add a third categorical variable, like Species, to a two dimensional scatterplot by mapping it to a different visual aesthetic. We\u0026rsquo;ve mapped length and width to x,y coordinates. Now we\u0026rsquo;ll simultaneously map species to color by expanding our list of aesthetics:\nggplot(data = iris) + (mapping = aes(x = Petal.Length, y = Petal.Width, color = Species)) + geom_point()   The R help for a specific geoms will list, among other things, all the aesthetics that geom supports.\nBreakout Rooms In the exercises we\u0026rsquo;ll be looking a little more at the iris data, and in addition, the NHANES data we used last week, and the left-joined bird dataset we built last week in Excercise 7.\nIf you haven\u0026rsquo;t installed the NHANES dataset do:\ninstall.packages(\"NHANES\", repos = \"http://cran.us.r-project.org\") #\u0026gt;  #\u0026gt; The downloaded binary packages are in #\u0026gt; /var/folders/d4/h4yjqs1560zbsgvrrwbmbp5r0000gn/T//RtmpPvm8W9/downloaded_packages   Once installed, load it with:\nlibrary(NHANES)   A prebuilt joined data set has been loaded on github.\n# create a data directory for the new file if you haven't done so yet: dir.create('data/birds', recursive = TRUE) #\u0026gt; Warning in dir.create(\"data/birds\", recursive = TRUE): 'data/birds' already exists # set the url joined_data_url \u0026lt;- 'https://raw.githubusercontent.com/biodash/biodash.github.io/master/content/codeclub/04_ggplot2/joined_data.tsv' # set the path for the downloaded file joined_file \u0026lt;- 'data/birds/joined_data.tsv' #download to file download.file(url = joined_data_url, destfile = joined_file) # read file joined_data \u0026lt;- read_tsv(joined_file) #\u0026gt; Parsed with column specification: #\u0026gt; cols( #\u0026gt; species = col_character(), #\u0026gt; locality = col_character(), #\u0026gt; stateProvince = col_character(), #\u0026gt; eventDate = col_datetime(format = \"\"), #\u0026gt; species_en = col_character(), #\u0026gt; adult_body_mass_g = col_double(), #\u0026gt; adult_svl_cm = col_double(), #\u0026gt; longevity_y = col_double(), #\u0026gt; litter_or_clutch_size_n = col_double() #\u0026gt; )   Exercise 1 Revisit the iris data set, and plot sepal width (y) against sepal length (x) colored by species. Which morphological character, petals or sepals, provides the greatest discrimination between species?\n  Hints (click here)  Simply reuse the code we used for petals. You can often leverage code from an old plot for a new one.    Solution (click here)  ggplot(data = iris) + (mapping = aes(x = Sepal.Length, y = Sepal.Width, color = Species)) + geom_point()   Note this solution shows yet another way to position global mapping information: as its own layer. This can help readability and avoid too many nested parentheses.\n   Exercise 2 Use the NHANES data set to plot body mass index (y) against height (x). Color by gender. Which gender has the highest BMI?\n  Hints (click here)  glimpse() the dataset to identify the variable names.    Solution (click here)  ggplot(data = NHANES) + geom_point(mapping = (aes(x = Height, y = BMI, color = Gender))) #\u0026gt; Warning: Removed 366 rows containing missing values (geom_point).      Exercise 3 Use the same plot but now color by physical activity. How active are those people with the highest BMI?\n  Hints (click here)  Again, glimpse() the dataset to identify the variable names.    Solution (click here)  ggplot(data = NHANES) + geom_point(mapping = (aes(x = Height, y = BMI, color = PhysActive))) #\u0026gt; Warning: Removed 366 rows containing missing values (geom_point).      Exercise 4 Often plotting the data allows us to identify outliers, which may be data-entry errors, or genuinely extreme data. Using the joined_data set, plot adult body mass (y) against longevity (x). Identify extreme data points at the high end of body mass. How can we identify what these points represent?\n  Hints (click here)  Examine the plot to find an appropriate threshold value, and filter the data using that value. How many data points are there passing that threshold? What species are represented by these data points? How many weights are reported? Why is the plot misleading here?    Solution (click here)  ggplot(data = joined_data) + geom_point(mapping = (aes(x = longevity_y, y = adult_body_mass_g))) #\u0026gt; Warning: Removed 24089 rows containing missing values (geom_point).   joined_data %\u0026gt;% filter(adult_body_mass_g \u0026gt; 10000) #\u0026gt; # A tibble: 228 x 9 #\u0026gt; species locality stateProvince eventDate species_en #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dttm\u0026gt; \u0026lt;chr\u0026gt;  #\u0026gt; 1 Cygnus… Findlay Ohio 2008-02-17 00:00:00 Mute Swan  #\u0026gt; 2 Cygnus… Dundee Ohio 2004-02-16 00:00:00 Mute Swan  #\u0026gt; 3 Cygnus… 44805 A… Ohio 2006-02-18 00:00:00 Mute Swan  #\u0026gt; 4 Cygnus… 45011 H… Ohio 2005-02-19 00:00:00 Mute Swan  #\u0026gt; 5 Cygnus… 45042 M… Ohio 2009-02-13 00:00:00 Trumpeter… #\u0026gt; 6 Cygnus… 44813 B… Ohio 2007-02-19 00:00:00 Mute Swan  #\u0026gt; 7 Cygnus… Spencer Ohio 2008-02-16 00:00:00 Mute Swan  #\u0026gt; 8 Cygnus… 44903 M… Ohio 2009-02-16 00:00:00 Mute Swan  #\u0026gt; 9 Cygnus… 44601 A… Ohio 2002-02-16 00:00:00 Mute Swan  #\u0026gt; 10 Cygnus… Avon La… Ohio 2007-02-17 00:00:00 Mute Swan  #\u0026gt; # … with 218 more rows, and 4 more variables: adult_body_mass_g \u0026lt;dbl\u0026gt;, #\u0026gt; # adult_svl_cm \u0026lt;dbl\u0026gt;, longevity_y \u0026lt;dbl\u0026gt;, litter_or_clutch_size_n \u0026lt;dbl\u0026gt;   joined_data %\u0026gt;% filter(adult_body_mass_g \u0026gt; 10000) %\u0026gt;% select(species) %\u0026gt;% distinct() #\u0026gt; # A tibble: 2 x 1 #\u0026gt; species  #\u0026gt; \u0026lt;chr\u0026gt;  #\u0026gt; 1 Cygnus olor  #\u0026gt; 2 Cygnus buccinator   joined_data %\u0026gt;% filter(adult_body_mass_g \u0026gt; 10000) %\u0026gt;% select(adult_body_mass_g) %\u0026gt;% distinct() #\u0026gt; # A tibble: 2 x 1 #\u0026gt; adult_body_mass_g #\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 10230 #\u0026gt; 2 10300     Bonus, a new geom! Revisit the iris data and generate a density histogram for sepal length, categorized by species.\n  Hints (click here)  Use geom_density(). Check the help to see what aesthetics it supports. Note that while you 'color' a point, you 'fill' an area.    Solution (click here)  ggplot(data = iris) + (mapping = (aes(x = Sepal.Length, fill = Species))) + geom_density(alpha = 0.5)   Note, what does the alpha aesthetic control?    \n","date":1607558400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608135268,"objectID":"cbba979dcba464a35bad36bb366cb927","permalink":"https://biodash.github.io/codeclub/04_ggplot2/","publishdate":"2020-12-10T00:00:00Z","relpermalink":"/codeclub/04_ggplot2/","section":"codeclub","summary":"In this session of Code Club, we'll look at how to visualize data in R using **ggplot2**.","tags":null,"title":"Session 4: Visualizing Data","type":"codeclub"},{"authors":["Mike Sovic"],"categories":null,"content":"\n New To Code Club?   First, check out the Code Club Computer Setup instructions, which also has some pointers that might be helpful if you\u0026rsquo;re new to R or RStudio.\n  Please open RStudio before Code Club to test things out \u0026ndash; if you run into issues, join the Zoom call early and we\u0026rsquo;ll troubleshoot.\n   Session Goals  Differentiate between different types of joins\u0026hellip;  inner_join() full_join() left_join() right_join()   Use a join function to add new variables to the birds dataset Keep practicing with dplyr core verbs from last week, esp\u0026hellip;  select() filter()   Answer the question \u0026ldquo;What Ohio bird species have the longest and shortest average lifespans?\u0026rdquo;.   Intro: Merging/Joining Datasets Sometimes you don\u0026rsquo;t have all your data in the same place. For example, maybe you have multiple Excel sheets for a project - each storing a different type of data for the same set of samples. Or maybe you\u0026rsquo;re interested in analyzing various metrics for US states and are getting the data from different places online - economic data from one database, climate data from another, and so on. As part of the process of data wrangling, it\u0026rsquo;s often useful to merge the separate datasets together according to a variable they share, possibly \u0026ldquo;SampleID\u0026rdquo; or \u0026ldquo;State Name\u0026rdquo; for the two above examples, respectively. R offers several ways to do this, but we\u0026rsquo;ll focus here on the set of *_join() functions available in dplyr. They include\u0026hellip;\n inner_join() full_join() left_join() right_join() semi_join() anti_join()  Check out the \u0026lsquo;Combine Data Sets\u0026rsquo; section of this cheat sheet for a brief look at these functions.\nYou can also get more details here, or, as with any R function, by accessing the function\u0026rsquo;s documentation inside R with the \u0026lsquo;?\u0026rsquo;. For example, type ?inner_join at your R prompt and hit Enter. (Make sure the package the function comes from is loaded first! In this case, you need dplyr, which is loaded as part of tidyverse.)\n Examples Below we\u0026rsquo;ll go through a few examples of joins. You\u0026rsquo;re welcome to follow along and run this code on your own, but it\u0026rsquo;s not necessary - the exercises in the breakout rooms are independent of these examples and will give you a chance to try these things out on your own.\nIf you want to follow along, you can find the code here.\n Since the *_join() functions come from the dplyr package, which is part of tidyverse, I\u0026rsquo;ll load that first\u0026hellip;\n#this assumes you've already installed tidyverse library(tidyverse)   The National Health and Nutrition Examination Survey (NHANES) dataset contains survey data obtained annually from ~5,000 individuals on a variety of health and lifestyle-related metrics. A subset of the data are available as an R package - install and load it\u0026hellip;\ninstall.packages(\"NHANES\", repos = \"http://cran.us.r-project.org\") #\u0026gt;  #\u0026gt; The downloaded binary packages are in #\u0026gt; /var/folders/s7/y_mgh3c54h9fjcyw9wqdkb8x4zs_jy/T//RtmpdHHxzY/downloaded_packages library(NHANES)   Now preview the dataset\u0026hellip;\nglimpse(NHANES) #\u0026gt; Rows: 10,000 #\u0026gt; Columns: 76 #\u0026gt; $ ID \u0026lt;int\u0026gt; 51624, 51624, 51624, 51625, 51630, 51638, 51646, 516… #\u0026gt; $ SurveyYr \u0026lt;fct\u0026gt; 2009_10, 2009_10, 2009_10, 2009_10, 2009_10, 2009_10… #\u0026gt; $ Gender \u0026lt;fct\u0026gt; male, male, male, male, female, male, male, female, … #\u0026gt; $ Age \u0026lt;int\u0026gt; 34, 34, 34, 4, 49, 9, 8, 45, 45, 45, 66, 58, 54, 10,… #\u0026gt; $ AgeDecade \u0026lt;fct\u0026gt; 30-39, 30-39, 30-39, 0-9, 40-49, 0-9, 0-9, 4… #\u0026gt; $ AgeMonths \u0026lt;int\u0026gt; 409, 409, 409, 49, 596, 115, 101, 541, 541, 541, 795… #\u0026gt; $ Race1 \u0026lt;fct\u0026gt; White, White, White, Other, White, White, White, Whi… #\u0026gt; $ Race3 \u0026lt;fct\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … #\u0026gt; $ Education \u0026lt;fct\u0026gt; High School, High School, High School, NA, Some Coll… #\u0026gt; $ MaritalStatus \u0026lt;fct\u0026gt; Married, Married, Married, NA, LivePartner, NA, NA, … #\u0026gt; $ HHIncome \u0026lt;fct\u0026gt; 25000-34999, 25000-34999, 25000-34999, 20000-24999, … #\u0026gt; $ HHIncomeMid \u0026lt;int\u0026gt; 30000, 30000, 30000, 22500, 40000, 87500, 60000, 875… #\u0026gt; $ Poverty \u0026lt;dbl\u0026gt; 1.36, 1.36, 1.36, 1.07, 1.91, 1.84, 2.33, 5.00, 5.00… #\u0026gt; $ HomeRooms \u0026lt;int\u0026gt; 6, 6, 6, 9, 5, 6, 7, 6, 6, 6, 5, 10, 6, 10, 10, 4, 3… #\u0026gt; $ HomeOwn \u0026lt;fct\u0026gt; Own, Own, Own, Own, Rent, Rent, Own, Own, Own, Own, … #\u0026gt; $ Work \u0026lt;fct\u0026gt; NotWorking, NotWorking, NotWorking, NA, NotWorking, … #\u0026gt; $ Weight \u0026lt;dbl\u0026gt; 87.4, 87.4, 87.4, 17.0, 86.7, 29.8, 35.2, 75.7, 75.7… #\u0026gt; $ Length \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … #\u0026gt; $ HeadCirc \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … #\u0026gt; $ Height \u0026lt;dbl\u0026gt; 164.7, 164.7, 164.7, 105.4, 168.4, 133.1, 130.6, 166… #\u0026gt; $ BMI \u0026lt;dbl\u0026gt; 32.22, 32.22, 32.22, 15.30, 30.57, 16.82, 20.64, 27.… #\u0026gt; $ BMICatUnder20yrs \u0026lt;fct\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … #\u0026gt; $ BMI_WHO \u0026lt;fct\u0026gt; 30.0_plus, 30.0_plus, 30.0_plus, 12.0_18.5, 30.0_plu… #\u0026gt; $ Pulse \u0026lt;int\u0026gt; 70, 70, 70, NA, 86, 82, 72, 62, 62, 62, 60, 62, 76, … #\u0026gt; $ BPSysAve \u0026lt;int\u0026gt; 113, 113, 113, NA, 112, 86, 107, 118, 118, 118, 111,… #\u0026gt; $ BPDiaAve \u0026lt;int\u0026gt; 85, 85, 85, NA, 75, 47, 37, 64, 64, 64, 63, 74, 85, … #\u0026gt; $ BPSys1 \u0026lt;int\u0026gt; 114, 114, 114, NA, 118, 84, 114, 106, 106, 106, 124,… #\u0026gt; $ BPDia1 \u0026lt;int\u0026gt; 88, 88, 88, NA, 82, 50, 46, 62, 62, 62, 64, 76, 86, … #\u0026gt; $ BPSys2 \u0026lt;int\u0026gt; 114, 114, 114, NA, 108, 84, 108, 118, 118, 118, 108,… #\u0026gt; $ BPDia2 \u0026lt;int\u0026gt; 88, 88, 88, NA, 74, 50, 36, 68, 68, 68, 62, 72, 88, … #\u0026gt; $ BPSys3 \u0026lt;int\u0026gt; 112, 112, 112, NA, 116, 88, 106, 118, 118, 118, 114,… #\u0026gt; $ BPDia3 \u0026lt;int\u0026gt; 82, 82, 82, NA, 76, 44, 38, 60, 60, 60, 64, 76, 82, … #\u0026gt; $ Testosterone \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … #\u0026gt; $ DirectChol \u0026lt;dbl\u0026gt; 1.29, 1.29, 1.29, NA, 1.16, 1.34, 1.55, 2.12, 2.12, … #\u0026gt; $ TotChol \u0026lt;dbl\u0026gt; 3.49, 3.49, 3.49, NA, 6.70, 4.86, 4.09, 5.82, 5.82, … #\u0026gt; $ UrineVol1 \u0026lt;int\u0026gt; 352, 352, 352, NA, 77, 123, 238, 106, 106, 106, 113,… #\u0026gt; $ UrineFlow1 \u0026lt;dbl\u0026gt; NA, NA, NA, NA, 0.094, 1.538, 1.322, 1.116, 1.116, 1… #\u0026gt; $ UrineVol2 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … #\u0026gt; $ UrineFlow2 \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … #\u0026gt; $ Diabetes \u0026lt;fct\u0026gt; No, No, No, No, No, No, No, No, No, No, No, No, No, … #\u0026gt; $ DiabetesAge \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … #\u0026gt; $ HealthGen \u0026lt;fct\u0026gt; Good, Good, Good, NA, Good, NA, NA, Vgood, Vgood, Vg… #\u0026gt; $ DaysPhysHlthBad \u0026lt;int\u0026gt; 0, 0, 0, NA, 0, NA, NA, 0, 0, 0, 10, 0, 4, NA, NA, 0… #\u0026gt; $ DaysMentHlthBad \u0026lt;int\u0026gt; 15, 15, 15, NA, 10, NA, NA, 3, 3, 3, 0, 0, 0, NA, NA… #\u0026gt; $ LittleInterest \u0026lt;fct\u0026gt; Most, Most, Most, NA, Several, NA, NA, None, None, N… #\u0026gt; $ Depressed \u0026lt;fct\u0026gt; Several, Several, Several, NA, Several, NA, NA, None… #\u0026gt; $ nPregnancies \u0026lt;int\u0026gt; NA, NA, NA, NA, 2, NA, NA, 1, 1, 1, NA, NA, NA, NA, … #\u0026gt; $ nBabies \u0026lt;int\u0026gt; NA, NA, NA, NA, 2, NA, NA, NA, NA, NA, NA, NA, NA, N… #\u0026gt; $ Age1stBaby \u0026lt;int\u0026gt; NA, NA, NA, NA, 27, NA, NA, NA, NA, NA, NA, NA, NA, … #\u0026gt; $ SleepHrsNight \u0026lt;int\u0026gt; 4, 4, 4, NA, 8, NA, NA, 8, 8, 8, 7, 5, 4, NA, 5, 7, … #\u0026gt; $ SleepTrouble \u0026lt;fct\u0026gt; Yes, Yes, Yes, NA, Yes, NA, NA, No, No, No, No, No, … #\u0026gt; $ PhysActive \u0026lt;fct\u0026gt; No, No, No, NA, No, NA, NA, Yes, Yes, Yes, Yes, Yes,… #\u0026gt; $ PhysActiveDays \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, 5, 5, 5, 7, 5, 1, NA, 2,… #\u0026gt; $ TVHrsDay \u0026lt;fct\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … #\u0026gt; $ CompHrsDay \u0026lt;fct\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … #\u0026gt; $ TVHrsDayChild \u0026lt;int\u0026gt; NA, NA, NA, 4, NA, 5, 1, NA, NA, NA, NA, NA, NA, 4, … #\u0026gt; $ CompHrsDayChild \u0026lt;int\u0026gt; NA, NA, NA, 1, NA, 0, 6, NA, NA, NA, NA, NA, NA, 3, … #\u0026gt; $ Alcohol12PlusYr \u0026lt;fct\u0026gt; Yes, Yes, Yes, NA, Yes, NA, NA, Yes, Yes, Yes, Yes, … #\u0026gt; $ AlcoholDay \u0026lt;int\u0026gt; NA, NA, NA, NA, 2, NA, NA, 3, 3, 3, 1, 2, 6, NA, NA,… #\u0026gt; $ AlcoholYear \u0026lt;int\u0026gt; 0, 0, 0, NA, 20, NA, NA, 52, 52, 52, 100, 104, 364, … #\u0026gt; $ SmokeNow \u0026lt;fct\u0026gt; No, No, No, NA, Yes, NA, NA, NA, NA, NA, No, NA, NA,… #\u0026gt; $ Smoke100 \u0026lt;fct\u0026gt; Yes, Yes, Yes, NA, Yes, NA, NA, No, No, No, Yes, No,… #\u0026gt; $ Smoke100n \u0026lt;fct\u0026gt; Smoker, Smoker, Smoker, NA, Smoker, NA, NA, Non-Smok… #\u0026gt; $ SmokeAge \u0026lt;int\u0026gt; 18, 18, 18, NA, 38, NA, NA, NA, NA, NA, 13, NA, NA, … #\u0026gt; $ Marijuana \u0026lt;fct\u0026gt; Yes, Yes, Yes, NA, Yes, NA, NA, Yes, Yes, Yes, NA, Y… #\u0026gt; $ AgeFirstMarij \u0026lt;int\u0026gt; 17, 17, 17, NA, 18, NA, NA, 13, 13, 13, NA, 19, 15, … #\u0026gt; $ RegularMarij \u0026lt;fct\u0026gt; No, No, No, NA, No, NA, NA, No, No, No, NA, Yes, Yes… #\u0026gt; $ AgeRegMarij \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 20, 15, … #\u0026gt; $ HardDrugs \u0026lt;fct\u0026gt; Yes, Yes, Yes, NA, Yes, NA, NA, No, No, No, No, Yes,… #\u0026gt; $ SexEver \u0026lt;fct\u0026gt; Yes, Yes, Yes, NA, Yes, NA, NA, Yes, Yes, Yes, Yes, … #\u0026gt; $ SexAge \u0026lt;int\u0026gt; 16, 16, 16, NA, 12, NA, NA, 13, 13, 13, 17, 22, 12, … #\u0026gt; $ SexNumPartnLife \u0026lt;int\u0026gt; 8, 8, 8, NA, 10, NA, NA, 20, 20, 20, 15, 7, 100, NA,… #\u0026gt; $ SexNumPartYear \u0026lt;int\u0026gt; 1, 1, 1, NA, 1, NA, NA, 0, 0, 0, NA, 1, 1, NA, NA, 1… #\u0026gt; $ SameSex \u0026lt;fct\u0026gt; No, No, No, NA, Yes, NA, NA, Yes, Yes, Yes, No, No, … #\u0026gt; $ SexOrientation \u0026lt;fct\u0026gt; Heterosexual, Heterosexual, Heterosexual, NA, Hetero… #\u0026gt; $ PregnantNow \u0026lt;fct\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …   To try out merging/joining, we\u0026rsquo;ll create two separate data frames by pulling out some variables from this NHANES dataset. One will contain demographic variables, and the other with have some physical measurements. Then we\u0026rsquo;ll join them back together. Let\u0026rsquo;s create the two sub-datasets first\u0026hellip;\n#Filter out rows with data from 2009-2010 and Age \u0026gt; 5,  #select a subset (4) of the variables, then get rid of  #all duplicate rows. Assign the output to object 'dem_data'. dem_data \u0026lt;- NHANES %\u0026gt;% filter(SurveyYr == \"2009_10\") %\u0026gt;% filter(Age \u0026gt; 5) %\u0026gt;% select(ID, Gender, Age, Education) %\u0026gt;% distinct() #similar as above, but with a different filter and  #selecting different variables. Save as 'phys_data' phys_data \u0026lt;- NHANES %\u0026gt;% filter(SurveyYr == \"2009_10\") %\u0026gt;% filter(Height \u0026lt; 180) %\u0026gt;% select(ID, Height, BMI, Pulse) %\u0026gt;% distinct()   Now explore them a bit\u0026hellip;\n#view the first 6 rows of each - note the shared ID column head(dem_data) #\u0026gt; # A tibble: 6 x 4 #\u0026gt; ID Gender Age Education  #\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt;  #\u0026gt; 1 51624 male 34 High School  #\u0026gt; 2 51630 female 49 Some College #\u0026gt; 3 51638 male 9 NA  #\u0026gt; 4 51646 male 8 NA  #\u0026gt; 5 51647 female 45 College Grad #\u0026gt; 6 51654 male 66 Some College head(phys_data) #\u0026gt; # A tibble: 6 x 4 #\u0026gt; ID Height BMI Pulse #\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 51624 165. 32.2 70 #\u0026gt; 2 51625 105. 15.3 NA #\u0026gt; 3 51630 168. 30.6 86 #\u0026gt; 4 51638 133. 16.8 82 #\u0026gt; 5 51646 131. 20.6 72 #\u0026gt; 6 51647 167. 27.2 62 #preview in another way - note the different numbers of observations (rows) glimpse(dem_data) #\u0026gt; Rows: 3,217 #\u0026gt; Columns: 4 #\u0026gt; $ ID \u0026lt;int\u0026gt; 51624, 51630, 51638, 51646, 51647, 51654, 51656, 51657, 516… #\u0026gt; $ Gender \u0026lt;fct\u0026gt; male, female, male, male, female, male, male, male, female,… #\u0026gt; $ Age \u0026lt;int\u0026gt; 34, 49, 9, 8, 45, 66, 58, 54, 10, 58, 50, 9, 33, 60, 16, 56… #\u0026gt; $ Education \u0026lt;fct\u0026gt; High School, Some College, NA, NA, College Grad, Some Colle… glimpse(phys_data) #\u0026gt; Rows: 3,021 #\u0026gt; Columns: 4 #\u0026gt; $ ID \u0026lt;int\u0026gt; 51624, 51625, 51630, 51638, 51646, 51647, 51654, 51657, 51659,… #\u0026gt; $ Height \u0026lt;dbl\u0026gt; 164.7, 105.4, 168.4, 133.1, 130.6, 166.7, 169.5, 169.4, 141.8,… #\u0026gt; $ BMI \u0026lt;dbl\u0026gt; 32.22, 15.30, 30.57, 16.82, 20.64, 27.24, 23.67, 26.03, 19.20,… #\u0026gt; $ Pulse \u0026lt;int\u0026gt; 70, NA, 86, 82, 72, 62, 60, 76, 80, 94, 74, 92, 84, 76, 64, 70…   Let\u0026rsquo;s use the shared ID column to join the two datasets together. We\u0026rsquo;ll do this in 4 different ways to compare different types of joins: inner_join(), left_join(), right_join(), and full_join(). Pay attention to the number of rows in the joined dataset each time and how it relates to the number of rows in each of the two individual datasets.\nThe basic structure of the dplyr *_join() functions is\u0026hellip;\n*_join(dataframe 'x', dataframe 'y', by = shared column name)\n 1 - inner_join() #perform an inner join join_inner \u0026lt;- inner_join(dem_data, phys_data, by = \"ID\") #preview the new object head(join_inner) #\u0026gt; # A tibble: 6 x 7 #\u0026gt; ID Gender Age Education Height BMI Pulse #\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 51624 male 34 High School 165. 32.2 70 #\u0026gt; 2 51630 female 49 Some College 168. 30.6 86 #\u0026gt; 3 51638 male 9 NA 133. 16.8 82 #\u0026gt; 4 51646 male 8 NA 131. 20.6 72 #\u0026gt; 5 51647 female 45 College Grad 167. 27.2 62 #\u0026gt; 6 51654 male 66 Some College 170. 23.7 60 #get dimensions dim(join_inner) #\u0026gt; [1] 2806 7   2 - left_join() #perform an left join join_left \u0026lt;- left_join(dem_data, phys_data, by = \"ID\") #preview the new object head(join_left) #\u0026gt; # A tibble: 6 x 7 #\u0026gt; ID Gender Age Education Height BMI Pulse #\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 51624 male 34 High School 165. 32.2 70 #\u0026gt; 2 51630 female 49 Some College 168. 30.6 86 #\u0026gt; 3 51638 male 9 NA 133. 16.8 82 #\u0026gt; 4 51646 male 8 NA 131. 20.6 72 #\u0026gt; 5 51647 female 45 College Grad 167. 27.2 62 #\u0026gt; 6 51654 male 66 Some College 170. 23.7 60 #get dimensions dim(join_left) #\u0026gt; [1] 3217 7   3 - right_join() #perform an right join join_right \u0026lt;- right_join(dem_data, phys_data, by = \"ID\") #preview the new object head(join_right) #\u0026gt; # A tibble: 6 x 7 #\u0026gt; ID Gender Age Education Height BMI Pulse #\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 51624 male 34 High School 165. 32.2 70 #\u0026gt; 2 51630 female 49 Some College 168. 30.6 86 #\u0026gt; 3 51638 male 9 NA 133. 16.8 82 #\u0026gt; 4 51646 male 8 NA 131. 20.6 72 #\u0026gt; 5 51647 female 45 College Grad 167. 27.2 62 #\u0026gt; 6 51654 male 66 Some College 170. 23.7 60 #get dimensions dim(join_right) #\u0026gt; [1] 3021 7   4 - full_join() #perform an full join join_full \u0026lt;- full_join(dem_data, phys_data, by = \"ID\") #preview the new object head(join_full) #\u0026gt; # A tibble: 6 x 7 #\u0026gt; ID Gender Age Education Height BMI Pulse #\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 51624 male 34 High School 165. 32.2 70 #\u0026gt; 2 51630 female 49 Some College 168. 30.6 86 #\u0026gt; 3 51638 male 9 NA 133. 16.8 82 #\u0026gt; 4 51646 male 8 NA 131. 20.6 72 #\u0026gt; 5 51647 female 45 College Grad 167. 27.2 62 #\u0026gt; 6 51654 male 66 Some College 170. 23.7 60 #get dimensions dim(join_full) #\u0026gt; [1] 3432 7    Breakout rooms We\u0026rsquo;re going to add to our backyard birds dataset. I found a dataset that has life history data for a large number of species (birds and others). We\u0026rsquo;ll use species names to merge some of these life history variables in to the occurrence data we already have.\nIf you\u0026rsquo;re new and haven\u0026rsquo;t yet gotten the backyard bird dataset, get it first by running the code below. Otherwise, you can skip this step\u0026hellip;\n# create a directory called data that contains a subdirectory called birds dir.create('data/birds/', recursive = TRUE) # set the location of the file birds_file_url \u0026lt;- 'https://raw.githubusercontent.com/biodash/biodash.github.io/master/assets/data/birds/backyard-birds_Ohio.tsv' # set the path for the downloaded file birds_file \u0026lt;- 'data/birds/backyard-birds_Ohio.tsv' #download download.file(url = birds_file_url, destfile = birds_file)   Now (everybody), read in the bird data for this session\u0026hellip;\nbirds_file \u0026lt;- 'data/birds/backyard-birds_Ohio.tsv' birds \u0026lt;- read_tsv(birds_file) #\u0026gt; Parsed with column specification: #\u0026gt; cols( #\u0026gt; class = col_character(), #\u0026gt; order = col_character(), #\u0026gt; family = col_character(), #\u0026gt; genus = col_character(), #\u0026gt; species = col_character(), #\u0026gt; locality = col_character(), #\u0026gt; stateProvince = col_character(), #\u0026gt; decimalLatitude = col_double(), #\u0026gt; decimalLongitude = col_double(), #\u0026gt; eventDate = col_datetime(format = \"\"), #\u0026gt; species_en = col_character(), #\u0026gt; range = col_character() #\u0026gt; )   Exercise 1  Reduce the backyard bird dataset and keep just the following columns: species, locality, stateProvince, eventDate, species_en\n  Hints (click here)  Use select() to pull out the columns you want.    Solution (click here)  birds \u0026lt;- birds %\u0026gt;% select(species, locality, stateProvince, eventDate, species_en)      Exercise 2  Check to make sure things look right - how many columns does the birds dataset now have?\n  Hints (click here)  Use the dim() function. Or the ncol() function. Or glimpse(). Or head(). Or str(). Or even summary(). There\u0026rsquo;s lots of ways to do this.    Solution (click here)  dim(birds) #\u0026gt; [1] 311441 5       Exercise 3  Now download and read in the new life history dataset (tab separated) available at https://github.com/biodash/biodash.github.io/raw/master/assets/data/birds/esa_life_history_data_cc.tsv. Then explore it a bit - how many rows and columns are there?\n  Hints (click here)  Use the download.file() function like we did previously for the bird dataset. You\u0026rsquo;ll need to define the arguments \u0026lsquo;url\u0026rsquo; and \u0026lsquo;destfile\u0026rsquo; inside the parentheses. You can put the file anywhere you want, but I\u0026rsquo;d suggest in the same directory as the bird file we got, so, for example, the destination file could be \u0026ldquo;data/birds/life_history_data.tsv\u0026rdquo;.    Solution (click here)  #download the file from online and save it as a '.tsv' file (since it's tab delimited) download.file(url = \"https://github.com/biodash/biodash.github.io/raw/master/assets/data/birds/esa_life_history_data_cc.tsv\", destfile = \"data/birds/life_history_data.tsv\") #read the data in to R as an object named 'life_hist' life_hist \u0026lt;- read_tsv(file = \"data/birds/life_history_data.tsv\") #\u0026gt; Parsed with column specification: #\u0026gt; cols( #\u0026gt; class = col_character(), #\u0026gt; order = col_character(), #\u0026gt; family = col_character(), #\u0026gt; genus = col_character(), #\u0026gt; species = col_character(), #\u0026gt; common_name = col_character(), #\u0026gt; female_maturity_d = col_double(), #\u0026gt; litter_or_clutch_size_n = col_double(), #\u0026gt; litters_or_clutches_per_y = col_double(), #\u0026gt; adult_body_mass_g = col_double(), #\u0026gt; maximum_longevity_y = col_double(), #\u0026gt; egg_mass_g = col_double(), #\u0026gt; incubation_d = col_double(), #\u0026gt; fledging_age_d = col_double(), #\u0026gt; longevity_y = col_double(), #\u0026gt; adult_svl_cm = col_double() #\u0026gt; ) #preview the data glimpse(life_hist) #\u0026gt; Rows: 21,322 #\u0026gt; Columns: 16 #\u0026gt; $ class \u0026lt;chr\u0026gt; \"Aves\", \"Aves\", \"Aves\", \"Aves\", \"Aves\", \"Av… #\u0026gt; $ order \u0026lt;chr\u0026gt; \"Accipitriformes\", \"Accipitriformes\", \"Acci… #\u0026gt; $ family \u0026lt;chr\u0026gt; \"Accipitridae\", \"Accipitridae\", \"Accipitrid… #\u0026gt; $ genus \u0026lt;chr\u0026gt; \"Accipiter\", \"Accipiter\", \"Accipiter\", \"Acc… #\u0026gt; $ species \u0026lt;chr\u0026gt; \"Accipiter albogularis\", \"Accipiter badius\"… #\u0026gt; $ common_name \u0026lt;chr\u0026gt; \"Pied Goshawk\", \"Shikra\", \"Bicolored Hawk\",… #\u0026gt; $ female_maturity_d \u0026lt;dbl\u0026gt; NA, 363.468, NA, NA, 363.468, NA, NA, 547.8… #\u0026gt; $ litter_or_clutch_size_n \u0026lt;dbl\u0026gt; NA, 3.250, 2.700, NA, 4.000, NA, 2.700, 4.2… #\u0026gt; $ litters_or_clutches_per_y \u0026lt;dbl\u0026gt; NA, 1, NA, NA, 1, NA, NA, 1, NA, 1, NA, 1, … #\u0026gt; $ adult_body_mass_g \u0026lt;dbl\u0026gt; 251.500, 140.000, 345.000, 142.000, 203.500… #\u0026gt; $ maximum_longevity_y \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, NA, 19.90000, NA, 2… #\u0026gt; $ egg_mass_g \u0026lt;dbl\u0026gt; NA, 21.00, 32.00, NA, 21.85, NA, 32.00, 19.… #\u0026gt; $ incubation_d \u0026lt;dbl\u0026gt; NA, 30.00, NA, NA, 32.50, NA, NA, 33.00, NA… #\u0026gt; $ fledging_age_d \u0026lt;dbl\u0026gt; NA, 32.00, NA, NA, 42.50, NA, NA, 24.25, NA… #\u0026gt; $ longevity_y \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, NA, 12.58333, NA, 1… #\u0026gt; $ adult_svl_cm \u0026lt;dbl\u0026gt; NA, 30.00, 39.50, NA, 33.50, NA, 39.50, 29.…       Exercise 4  This new dataset contains life history data for more than just birds. What Classes of organisms are represented in the \u0026lsquo;Class\u0026rsquo; variable?\n  Hints (click here)  Try using a combination of the select() and distinct() functions to pull out the column you\u0026rsquo;re interested in, and then to get the distinct values, respectively.    Solutions (click here)  life_hist %\u0026gt;% select(class) %\u0026gt;% distinct() #\u0026gt; # A tibble: 3 x 1 #\u0026gt; class  #\u0026gt; \u0026lt;chr\u0026gt;  #\u0026gt; 1 Aves  #\u0026gt; 2 Mammalia #\u0026gt; 3 Reptilia       Exercise 5  Reduce the life history dataset down to keep just the rows for Class Aves and the columns species, adult_body_mass_g, adult_svl_cm, longevity_y, litter_or_clutch_size_n. What are the dimensions now?\n  Hints (click here)  Use filter() along with an appropriate logical expression to keep the rows we want. Use select() to get the desired columns.    Solutions (click here)  # pull out target rows and columns life_hist_aves \u0026lt;- life_hist %\u0026gt;% filter(class == \"Aves\") %\u0026gt;% select(species, adult_body_mass_g, adult_svl_cm, longevity_y, litter_or_clutch_size_n) dim(life_hist_aves) #\u0026gt; [1] 9802 5       Exercise 6  Preview each dataset again, just to make sure you\u0026rsquo;re clear about what\u0026rsquo;s in each one. Are there any columns that are shared between the two?\n  Hints (click here)  Consider glimpse() or head() to preview the datasets (tibbles/data frames). If you want to use a function to find shared columns, try a combination of intersect() and names().\n   Solutions (click here)  glimpse(birds) #\u0026gt; Rows: 311,441 #\u0026gt; Columns: 12 #\u0026gt; $ class \u0026lt;chr\u0026gt; \"Aves\", \"Aves\", \"Aves\", \"Aves\", \"Aves\", \"Aves\", \"Ave… #\u0026gt; $ order \u0026lt;chr\u0026gt; \"Passeriformes\", \"Passeriformes\", \"Passeriformes\", \"… #\u0026gt; $ family \u0026lt;chr\u0026gt; \"Corvidae\", \"Corvidae\", \"Corvidae\", \"Corvidae\", \"Cor… #\u0026gt; $ genus \u0026lt;chr\u0026gt; \"Cyanocitta\", \"Cyanocitta\", \"Cyanocitta\", \"Cyanocitt… #\u0026gt; $ species \u0026lt;chr\u0026gt; \"Cyanocitta cristata\", \"Cyanocitta cristata\", \"Cyano… #\u0026gt; $ locality \u0026lt;chr\u0026gt; \"44805 Ashland\", \"45244 Cincinnati\", \"44132 Euclid\",… #\u0026gt; $ stateProvince \u0026lt;chr\u0026gt; \"Ohio\", \"Ohio\", \"Ohio\", \"Ohio\", \"Ohio\", \"Ohio\", \"Ohi… #\u0026gt; $ decimalLatitude \u0026lt;dbl\u0026gt; 40.86166, 39.10666, 41.60768, 39.24236, 39.28207, 41… #\u0026gt; $ decimalLongitude \u0026lt;dbl\u0026gt; -82.31558, -84.32972, -81.50085, -84.35545, -84.4688… #\u0026gt; $ eventDate \u0026lt;dttm\u0026gt; 2007-02-16, 2007-02-17, 2007-02-17, 2007-02-19, 200… #\u0026gt; $ species_en \u0026lt;chr\u0026gt; \"Blue Jay\", \"Blue Jay\", \"Blue Jay\", \"Blue Jay\", \"Blu… #\u0026gt; $ range \u0026lt;chr\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … glimpse(life_hist_aves) #\u0026gt; Rows: 9,802 #\u0026gt; Columns: 5 #\u0026gt; $ species \u0026lt;chr\u0026gt; \"Accipiter albogularis\", \"Accipiter badius\", … #\u0026gt; $ adult_body_mass_g \u0026lt;dbl\u0026gt; 251.500, 140.000, 345.000, 142.000, 203.500, … #\u0026gt; $ adult_svl_cm \u0026lt;dbl\u0026gt; NA, 30.00, 39.50, NA, 33.50, NA, 39.50, 29.00… #\u0026gt; $ longevity_y \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, NA, 12.58333, NA, 12.… #\u0026gt; $ litter_or_clutch_size_n \u0026lt;dbl\u0026gt; NA, 3.250, 2.700, NA, 4.000, NA, 2.700, 4.250… intersect(names(birds), names(life_hist_aves)) #\u0026gt; [1] \"species\"       Exercise 7  Now lets join them together based on their shared variable. Not all species in the backyard bird (Ohio) dataset are included in the life history dataset. Likewise, there are life history data for many species that aren\u0026rsquo;t in the Ohio dataset. We want to keep all the Ohio observations, and merge in life history data for species where it\u0026rsquo;s availble, but we also don\u0026rsquo;t want to add in life history data for species that aren\u0026rsquo;t in the Ohio dataset. Choose an appropriate join function with those things in mind.\n  Hints (click here)  Try a left_join(), defining the Ohio backyard bird dataset as the \u0026lsquo;x\u0026rsquo; dataset in the join and the life history data as the \u0026lsquo;y\u0026rsquo; dataset. Get details on that function with ?left_join.    Solutions (click here)  joined_data \u0026lt;- left_join(x = birds, y = life_hist_aves, by = \"species\")       Exercise 8  What are the longest- and shortest-living bird species in Ohio based on the data in the longevity_y column?\n  Hints (click here)  Try using select() to pull out just the columns species and longevity_y, then use distinct() to get the unique rows, then arrange() based on the longevity_y column. You might also find the dplyr function desc() helpful.\nAlternatively, you could try grouping by species, then use summarise() to get either the max, min, or mean value for longevity_y for each species (there\u0026rsquo;s just one value for each species, so all of those statistics give the same value in this case). Then sort (arrange) the resulting summarized data frame on the longevity value.\n   Solutions (click here)  #option 1 - shortest-lived birds joined_data %\u0026gt;% select(species, longevity_y) %\u0026gt;% distinct() %\u0026gt;% arrange(longevity_y) #\u0026gt; # A tibble: 171 x 2 #\u0026gt; species longevity_y #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 Loxia leucoptera 4  #\u0026gt; 2 Spiza americana 4  #\u0026gt; 3 Certhia americana 4.6  #\u0026gt; 4 Acanthis hornemanni 4.6  #\u0026gt; 5 Tringa flavipes 4.75 #\u0026gt; 6 Podiceps grisegena 4.8  #\u0026gt; 7 Calcarius lapponicus 5  #\u0026gt; 8 Anthus rubescens 5.1  #\u0026gt; 9 Perdix perdix 5.17 #\u0026gt; 10 Regulus satrapa 5.32 #\u0026gt; # … with 161 more rows #option 1 - longest-lived birds joined_data %\u0026gt;% select(species, longevity_y) %\u0026gt;% distinct() %\u0026gt;% arrange(desc(longevity_y)) #\u0026gt; # A tibble: 171 x 2 #\u0026gt; species longevity_y #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 Larus argentatus 33.4 #\u0026gt; 2 Larus glaucoides 33  #\u0026gt; 3 Larus thayeri 33  #\u0026gt; 4 Haliaeetus leucocephalus 33.0 #\u0026gt; 5 Larus fuscus 32.8 #\u0026gt; 6 Aquila chrysaetos 32  #\u0026gt; 7 Anas platyrhynchos 29  #\u0026gt; 8 Larus delawarensis 28.6 #\u0026gt; 9 Asio otus 27.8 #\u0026gt; 10 Cygnus olor 27.7 #\u0026gt; # … with 161 more rows #option 2 - shortest-lived birds joined_data %\u0026gt;% group_by(species) %\u0026gt;% summarise(longevity = max(longevity_y)) %\u0026gt;% arrange(longevity) #\u0026gt; `summarise()` ungrouping output (override with `.groups` argument) #\u0026gt; # A tibble: 171 x 2 #\u0026gt; species longevity #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 Loxia leucoptera 4  #\u0026gt; 2 Spiza americana 4  #\u0026gt; 3 Acanthis hornemanni 4.6  #\u0026gt; 4 Certhia americana 4.6  #\u0026gt; 5 Tringa flavipes 4.75 #\u0026gt; 6 Podiceps grisegena 4.8  #\u0026gt; 7 Calcarius lapponicus 5  #\u0026gt; 8 Anthus rubescens 5.1  #\u0026gt; 9 Perdix perdix 5.17 #\u0026gt; 10 Regulus satrapa 5.32 #\u0026gt; # … with 161 more rows #option 2 - longest-lived birds joined_data %\u0026gt;% group_by(species) %\u0026gt;% summarise(longevity = max(longevity_y)) %\u0026gt;% arrange(desc(longevity)) #\u0026gt; `summarise()` ungrouping output (override with `.groups` argument) #\u0026gt; # A tibble: 171 x 2 #\u0026gt; species longevity #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 Larus argentatus 33.4 #\u0026gt; 2 Larus glaucoides 33  #\u0026gt; 3 Larus thayeri 33  #\u0026gt; 4 Haliaeetus leucocephalus 33.0 #\u0026gt; 5 Larus fuscus 32.8 #\u0026gt; 6 Aquila chrysaetos 32  #\u0026gt; 7 Anas platyrhynchos 29  #\u0026gt; 8 Larus delawarensis 28.6 #\u0026gt; 9 Asio otus 27.8 #\u0026gt; 10 Cygnus olor 27.7 #\u0026gt; # … with 161 more rows       Bonus time! Bonus 1  What species in Ohio has the largest ratio of adult body mass to length (measured as snout vent length, or \u0026lsquo;adult_svl_cm\u0026rsquo;)?\n  Hints (click here)  Use mutate() to create a new variable containing the body mass divided by svl, then arrange the dataset using that new variable to get the species with the highest value.\n   Solutions (click here)  joined_data %\u0026gt;% mutate(ratio = adult_body_mass_g/adult_svl_cm) %\u0026gt;% select(species_en, ratio) %\u0026gt;% distinct() %\u0026gt;% arrange(desc(ratio)) #\u0026gt; # A tibble: 170 x 2 #\u0026gt; species_en ratio #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 Mute Swan 71.8 #\u0026gt; 2 Wild Turkey 68.0 #\u0026gt; 3 Trumpeter Swan 64.9 #\u0026gt; 4 Bald Eagle 59.2 #\u0026gt; 5 Golden Eagle 56.2 #\u0026gt; 6 Canada Goose 48.3 #\u0026gt; 7 Tundra Swan 47.0 #\u0026gt; 8 Cackling Goose 44.4 #\u0026gt; 9 Snow Goose 35.1 #\u0026gt; 10 Snowy Owl 32.8 #\u0026gt; # … with 160 more rows       Bonus 2  There are 2 additional joins we haven\u0026rsquo;t talked about - semi_join() and anti_join(). Take a look at the documentation to see what these do. Use one of them to find what species in the backyard birds dataset are not in the life history dataset.\n  Hints (click here)  Use anti_join() and distinct().\n   Solutions (click here)  anti_join(birds, life_hist_aves, by = \"species\") %\u0026gt;% select(species, species_en) %\u0026gt;% distinct() #\u0026gt; # A tibble: 6 x 2 #\u0026gt; species species_en  #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt;  #\u0026gt; 1 Dendrocopos pubescens Downy Woodpecker  #\u0026gt; 2 Spizelloides arborea American Tree Sparrow #\u0026gt; 3 Otus asio Eastern Screech Owl  #\u0026gt; 4 Larus minutus Little Gull  #\u0026gt; 5 Anas rubripes x platyrhynchos NA  #\u0026gt; 6 NA NA       Bonus 3  The life history dataset we downloaded above is actually a modified version of the original file, which is located at \u0026lsquo;http://www.esapubs.org/archive/ecol/E096/269/Data_Files/Amniote_Database_Aug_2015.csv\u0026rsquo;\nTry starting with the original file and repeating what we did above - merging the variables species, adult_body_mass_g, adult_svl_cm, longevity_y, litter_or_clutch_size_n in to the original birds dataset. First, make sure to get it read in correctly. Then pay attention to the species column in the life history dataset - what needs to be done before a join/merge can be performed?\n  Hints (click here)  Pay attention to how missing data are coded in this dataset (it\u0026rsquo;s -999). Also, data are very sparse for some of the variables - in other words, they have lots of missing data. This seems to cause a problem with the read_csv() function, as it only considers the first 1000 rows for the purpose of defining the class of each column. This can be a problem if all of the first 1000 rows are missing. Finally, it appears that even though this is a comma separated file (commas define the column breaks), there are a few instances where commas are used within a field. This happens in the \u0026lsquo;common name\u0026rsquo; column in a few cases where multiple common names are listed for a specific observation. This is one example of something that can become quite frustrating when trying to get data loaded in, and is worth keeping an eye out for. Fortunately, in our case, it only seems to happen for non-bird species in this dataset, which we filter out anyway, so it can be dealt with. However, if it had impacted any of the bird observations, I think fixing this might require a solution outside of R - possibly a command line approach.\n   Solutions (click here)  #download download.file(url = \"http://www.esapubs.org/archive/ecol/E096/269/Data_Files/Amniote_Database_Aug_2015.csv\", destfile = \"data/birds/orig_life_history.csv\") #read the data in to R as an object named 'full_life_hist' full_life_hist \u0026lt;- read_csv(\"data/birds/orig_life_history.csv\", na = \"-999\", col_types = cols(birth_or_hatching_svl_cm = col_double(), weaning_d = col_double(),gestation_d = col_double(), weaning_weight_g = col_double(), male_svl_cm = col_double(), female_svl_cm = col_double(), no_sex_svl_cm = col_double(), female_body_mass_at_maturity_g = col_double(), female_svl_at_maturity_cm = col_double())) #get the original version of the birds dataset birds \u0026lt;- read_tsv('data/birds/backyard-birds_Ohio.tsv') #\u0026gt; Parsed with column specification: #\u0026gt; cols( #\u0026gt; class = col_character(), #\u0026gt; order = col_character(), #\u0026gt; family = col_character(), #\u0026gt; genus = col_character(), #\u0026gt; species = col_character(), #\u0026gt; locality = col_character(), #\u0026gt; stateProvince = col_character(), #\u0026gt; decimalLatitude = col_double(), #\u0026gt; decimalLongitude = col_double(), #\u0026gt; eventDate = col_datetime(format = \"\"), #\u0026gt; species_en = col_character(), #\u0026gt; range = col_character() #\u0026gt; ) #subset each for the columns and rows we want life_hist_aves \u0026lt;- full_life_hist %\u0026gt;% filter(class == \"Aves\") %\u0026gt;% select(species, adult_body_mass_g, adult_svl_cm, longevity_y, litter_or_clutch_size_n) birds \u0026lt;- birds %\u0026gt;% select(species, locality, stateProvince, eventDate, species_en) glimpse(birds) #\u0026gt; Rows: 311,441 #\u0026gt; Columns: 5 #\u0026gt; $ species \u0026lt;chr\u0026gt; \"Cyanocitta cristata\", \"Cyanocitta cristata\", \"Cyanocit… #\u0026gt; $ locality \u0026lt;chr\u0026gt; \"44805 Ashland\", \"45244 Cincinnati\", \"44132 Euclid\", \"4… #\u0026gt; $ stateProvince \u0026lt;chr\u0026gt; \"Ohio\", \"Ohio\", \"Ohio\", \"Ohio\", \"Ohio\", \"Ohio\", \"Ohio\",… #\u0026gt; $ eventDate \u0026lt;dttm\u0026gt; 2007-02-16, 2007-02-17, 2007-02-17, 2007-02-19, 2007-0… #\u0026gt; $ species_en \u0026lt;chr\u0026gt; \"Blue Jay\", \"Blue Jay\", \"Blue Jay\", \"Blue Jay\", \"Blue J… glimpse(life_hist_aves) #\u0026gt; Rows: 9,802 #\u0026gt; Columns: 5 #\u0026gt; $ species \u0026lt;chr\u0026gt; \"albogularis\", \"badius\", \"bicolor\", \"brachyur… #\u0026gt; $ adult_body_mass_g \u0026lt;dbl\u0026gt; 251.500, 140.000, 345.000, 142.000, 203.500, … #\u0026gt; $ adult_svl_cm \u0026lt;dbl\u0026gt; NA, 30.00, 39.50, NA, 33.50, NA, 39.50, 29.00… #\u0026gt; $ longevity_y \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, NA, 12.58333, NA, 12.… #\u0026gt; $ litter_or_clutch_size_n \u0026lt;dbl\u0026gt; NA, 3.250, 2.700, NA, 4.000, NA, 2.700, 4.250… #notice the species column in the life history data doesn't include the genus name. Since the names don't match in the species column from each dataset, a join won't work. Add the genus variable in from the original life history data... life_hist_aves \u0026lt;- full_life_hist %\u0026gt;% filter(class == \"Aves\") %\u0026gt;% select(genus, species, adult_body_mass_g, adult_svl_cm, longevity_y, litter_or_clutch_size_n) #now use mutate to replace the species column so it includes both the genus and species... life_hist_aves \u0026lt;- life_hist_aves %\u0026gt;% mutate(species = paste0(genus, \" \", species)) %\u0026gt;% select(-genus) #preview again glimpse(birds) #\u0026gt; Rows: 311,441 #\u0026gt; Columns: 5 #\u0026gt; $ species \u0026lt;chr\u0026gt; \"Cyanocitta cristata\", \"Cyanocitta cristata\", \"Cyanocit… #\u0026gt; $ locality \u0026lt;chr\u0026gt; \"44805 Ashland\", \"45244 Cincinnati\", \"44132 Euclid\", \"4… #\u0026gt; $ stateProvince \u0026lt;chr\u0026gt; \"Ohio\", \"Ohio\", \"Ohio\", \"Ohio\", \"Ohio\", \"Ohio\", \"Ohio\",… #\u0026gt; $ eventDate \u0026lt;dttm\u0026gt; 2007-02-16, 2007-02-17, 2007-02-17, 2007-02-19, 2007-0… #\u0026gt; $ species_en \u0026lt;chr\u0026gt; \"Blue Jay\", \"Blue Jay\", \"Blue Jay\", \"Blue Jay\", \"Blue J… glimpse(life_hist_aves) #\u0026gt; Rows: 9,802 #\u0026gt; Columns: 5 #\u0026gt; $ species \u0026lt;chr\u0026gt; \"Accipiter albogularis\", \"Accipiter badius\", … #\u0026gt; $ adult_body_mass_g \u0026lt;dbl\u0026gt; 251.500, 140.000, 345.000, 142.000, 203.500, … #\u0026gt; $ adult_svl_cm \u0026lt;dbl\u0026gt; NA, 30.00, 39.50, NA, 33.50, NA, 39.50, 29.00… #\u0026gt; $ longevity_y \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, NA, 12.58333, NA, 12.… #\u0026gt; $ litter_or_clutch_size_n \u0026lt;dbl\u0026gt; NA, 3.250, 2.700, NA, 4.000, NA, 2.700, 4.250… #now we can join joined_data \u0026lt;- left_join(birds, life_hist_aves, by = \"species\")       \n","date":1607040000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629215857,"objectID":"27c93c3c265ed2c0052748376945f941","permalink":"https://biodash.github.io/codeclub/03_joining-datasets/","publishdate":"2020-12-04T00:00:00Z","relpermalink":"/codeclub/03_joining-datasets/","section":"codeclub","summary":"In this session of Code Club, we'll explore some methods for combining datasets according to a shared variable, with primary focus on the `*_join()` set of functions from **dplyr**. We'll also keep practicing with some of the core dplyr verbs from last session.","tags":null,"title":"Session 3: Joining Datasets","type":"codeclub"},{"authors":["Mike Sovic"],"categories":null,"content":"\n New To Code Club?   First, check out the Code Club Computer Setup instructions, which also has some pointers that might be helpful if you\u0026rsquo;re new to R or RStudio.\n  Please open RStudio before Code Club to test things out \u0026ndash; if you run into issues, join the Zoom call early and we\u0026rsquo;ll troubleshoot.\n   Session Goals  Differentiate between different types of joins\u0026hellip;  inner_join() full_join() left_join() right_join()   Use a join function to add new variables to the birds dataset Keep practicing with dplyr core verbs from last week, esp\u0026hellip;  select() filter()   Answer the question \u0026ldquo;What Ohio bird species have the longest and shortest average lifespans?\u0026rdquo;.   Intro: Merging/Joining Datasets Sometimes you don\u0026rsquo;t have all your data in the same place. For example, maybe you have multiple Excel sheets for a project - each storing a different type of data for the same set of samples. Or maybe you\u0026rsquo;re interested in analyzing various metrics for US states and are getting the data from different places online - economic data from one database, climate data from another, and so on. As part of the process of data wrangling, it\u0026rsquo;s often useful to merge the separate datasets together according to a variable they share, possibly \u0026ldquo;SampleID\u0026rdquo; or \u0026ldquo;State Name\u0026rdquo; for the two above examples, respectively. R offers several ways to do this, but we\u0026rsquo;ll focus here on the set of *_join() functions available in dplyr. They include\u0026hellip;\n inner_join() full_join() left_join() right_join() semi_join() anti_join()  Check out the \u0026lsquo;Combine Data Sets\u0026rsquo; section of this cheat sheet for a brief look at these functions.\nYou can also get more details here, or, as with any R function, by accessing the function\u0026rsquo;s documentation inside R with the \u0026lsquo;?\u0026rsquo;. For example, type ?inner_join at your R prompt and hit Enter. (Make sure the package the function comes from is loaded first! In this case, you need dplyr, which is loaded as part of tidyverse.)\n Examples Below we\u0026rsquo;ll go through a few examples of joins. You\u0026rsquo;re welcome to follow along and run this code on your own, but it\u0026rsquo;s not necessary - the exercises in the breakout rooms are independent of these examples and will give you a chance to try these things out on your own.\nIf you want to follow along, you can find the code here.\n Since the *_join() functions come from the dplyr package, which is part of tidyverse, I\u0026rsquo;ll load that first\u0026hellip;\n#this assumes you've already installed tidyverse library(tidyverse)   The National Health and Nutrition Examination Survey (NHANES) dataset contains survey data obtained annually from ~5,000 individuals on a variety of health and lifestyle-related metrics. A subset of the data are available as an R package - install and load it\u0026hellip;\ninstall.packages(\"NHANES\", repos = \"http://cran.us.r-project.org\") #\u0026gt;  #\u0026gt; The downloaded binary packages are in #\u0026gt; /var/folders/s7/y_mgh3c54h9fjcyw9wqdkb8x4zs_jy/T//RtmpdHHxzY/downloaded_packages library(NHANES)   Now preview the dataset\u0026hellip;\nglimpse(NHANES) #\u0026gt; Rows: 10,000 #\u0026gt; Columns: 76 #\u0026gt; $ ID \u0026lt;int\u0026gt; 51624, 51624, 51624, 51625, 51630, 51638, 51646, 516… #\u0026gt; $ SurveyYr \u0026lt;fct\u0026gt; 2009_10, 2009_10, 2009_10, 2009_10, 2009_10, 2009_10… #\u0026gt; $ Gender \u0026lt;fct\u0026gt; male, male, male, male, female, male, male, female, … #\u0026gt; $ Age \u0026lt;int\u0026gt; 34, 34, 34, 4, 49, 9, 8, 45, 45, 45, 66, 58, 54, 10,… #\u0026gt; $ AgeDecade \u0026lt;fct\u0026gt; 30-39, 30-39, 30-39, 0-9, 40-49, 0-9, 0-9, 4… #\u0026gt; $ AgeMonths \u0026lt;int\u0026gt; 409, 409, 409, 49, 596, 115, 101, 541, 541, 541, 795… #\u0026gt; $ Race1 \u0026lt;fct\u0026gt; White, White, White, Other, White, White, White, Whi… #\u0026gt; $ Race3 \u0026lt;fct\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … #\u0026gt; $ Education \u0026lt;fct\u0026gt; High School, High School, High School, NA, Some Coll… #\u0026gt; $ MaritalStatus \u0026lt;fct\u0026gt; Married, Married, Married, NA, LivePartner, NA, NA, … #\u0026gt; $ HHIncome \u0026lt;fct\u0026gt; 25000-34999, 25000-34999, 25000-34999, 20000-24999, … #\u0026gt; $ HHIncomeMid \u0026lt;int\u0026gt; 30000, 30000, 30000, 22500, 40000, 87500, 60000, 875… #\u0026gt; $ Poverty \u0026lt;dbl\u0026gt; 1.36, 1.36, 1.36, 1.07, 1.91, 1.84, 2.33, 5.00, 5.00… #\u0026gt; $ HomeRooms \u0026lt;int\u0026gt; 6, 6, 6, 9, 5, 6, 7, 6, 6, 6, 5, 10, 6, 10, 10, 4, 3… #\u0026gt; $ HomeOwn \u0026lt;fct\u0026gt; Own, Own, Own, Own, Rent, Rent, Own, Own, Own, Own, … #\u0026gt; $ Work \u0026lt;fct\u0026gt; NotWorking, NotWorking, NotWorking, NA, NotWorking, … #\u0026gt; $ Weight \u0026lt;dbl\u0026gt; 87.4, 87.4, 87.4, 17.0, 86.7, 29.8, 35.2, 75.7, 75.7… #\u0026gt; $ Length \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … #\u0026gt; $ HeadCirc \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … #\u0026gt; $ Height \u0026lt;dbl\u0026gt; 164.7, 164.7, 164.7, 105.4, 168.4, 133.1, 130.6, 166… #\u0026gt; $ BMI \u0026lt;dbl\u0026gt; 32.22, 32.22, 32.22, 15.30, 30.57, 16.82, 20.64, 27.… #\u0026gt; $ BMICatUnder20yrs \u0026lt;fct\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … #\u0026gt; $ BMI_WHO \u0026lt;fct\u0026gt; 30.0_plus, 30.0_plus, 30.0_plus, 12.0_18.5, 30.0_plu… #\u0026gt; $ Pulse \u0026lt;int\u0026gt; 70, 70, 70, NA, 86, 82, 72, 62, 62, 62, 60, 62, 76, … #\u0026gt; $ BPSysAve \u0026lt;int\u0026gt; 113, 113, 113, NA, 112, 86, 107, 118, 118, 118, 111,… #\u0026gt; $ BPDiaAve \u0026lt;int\u0026gt; 85, 85, 85, NA, 75, 47, 37, 64, 64, 64, 63, 74, 85, … #\u0026gt; $ BPSys1 \u0026lt;int\u0026gt; 114, 114, 114, NA, 118, 84, 114, 106, 106, 106, 124,… #\u0026gt; $ BPDia1 \u0026lt;int\u0026gt; 88, 88, 88, NA, 82, 50, 46, 62, 62, 62, 64, 76, 86, … #\u0026gt; $ BPSys2 \u0026lt;int\u0026gt; 114, 114, 114, NA, 108, 84, 108, 118, 118, 118, 108,… #\u0026gt; $ BPDia2 \u0026lt;int\u0026gt; 88, 88, 88, NA, 74, 50, 36, 68, 68, 68, 62, 72, 88, … #\u0026gt; $ BPSys3 \u0026lt;int\u0026gt; 112, 112, 112, NA, 116, 88, 106, 118, 118, 118, 114,… #\u0026gt; $ BPDia3 \u0026lt;int\u0026gt; 82, 82, 82, NA, 76, 44, 38, 60, 60, 60, 64, 76, 82, … #\u0026gt; $ Testosterone \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … #\u0026gt; $ DirectChol \u0026lt;dbl\u0026gt; 1.29, 1.29, 1.29, NA, 1.16, 1.34, 1.55, 2.12, 2.12, … #\u0026gt; $ TotChol \u0026lt;dbl\u0026gt; 3.49, 3.49, 3.49, NA, 6.70, 4.86, 4.09, 5.82, 5.82, … #\u0026gt; $ UrineVol1 \u0026lt;int\u0026gt; 352, 352, 352, NA, 77, 123, 238, 106, 106, 106, 113,… #\u0026gt; $ UrineFlow1 \u0026lt;dbl\u0026gt; NA, NA, NA, NA, 0.094, 1.538, 1.322, 1.116, 1.116, 1… #\u0026gt; $ UrineVol2 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … #\u0026gt; $ UrineFlow2 \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … #\u0026gt; $ Diabetes \u0026lt;fct\u0026gt; No, No, No, No, No, No, No, No, No, No, No, No, No, … #\u0026gt; $ DiabetesAge \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … #\u0026gt; $ HealthGen \u0026lt;fct\u0026gt; Good, Good, Good, NA, Good, NA, NA, Vgood, Vgood, Vg… #\u0026gt; $ DaysPhysHlthBad \u0026lt;int\u0026gt; 0, 0, 0, NA, 0, NA, NA, 0, 0, 0, 10, 0, 4, NA, NA, 0… #\u0026gt; $ DaysMentHlthBad \u0026lt;int\u0026gt; 15, 15, 15, NA, 10, NA, NA, 3, 3, 3, 0, 0, 0, NA, NA… #\u0026gt; $ LittleInterest \u0026lt;fct\u0026gt; Most, Most, Most, NA, Several, NA, NA, None, None, N… #\u0026gt; $ Depressed \u0026lt;fct\u0026gt; Several, Several, Several, NA, Several, NA, NA, None… #\u0026gt; $ nPregnancies \u0026lt;int\u0026gt; NA, NA, NA, NA, 2, NA, NA, 1, 1, 1, NA, NA, NA, NA, … #\u0026gt; $ nBabies \u0026lt;int\u0026gt; NA, NA, NA, NA, 2, NA, NA, NA, NA, NA, NA, NA, NA, N… #\u0026gt; $ Age1stBaby \u0026lt;int\u0026gt; NA, NA, NA, NA, 27, NA, NA, NA, NA, NA, NA, NA, NA, … #\u0026gt; $ SleepHrsNight \u0026lt;int\u0026gt; 4, 4, 4, NA, 8, NA, NA, 8, 8, 8, 7, 5, 4, NA, 5, 7, … #\u0026gt; $ SleepTrouble \u0026lt;fct\u0026gt; Yes, Yes, Yes, NA, Yes, NA, NA, No, No, No, No, No, … #\u0026gt; $ PhysActive \u0026lt;fct\u0026gt; No, No, No, NA, No, NA, NA, Yes, Yes, Yes, Yes, Yes,… #\u0026gt; $ PhysActiveDays \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, 5, 5, 5, 7, 5, 1, NA, 2,… #\u0026gt; $ TVHrsDay \u0026lt;fct\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … #\u0026gt; $ CompHrsDay \u0026lt;fct\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … #\u0026gt; $ TVHrsDayChild \u0026lt;int\u0026gt; NA, NA, NA, 4, NA, 5, 1, NA, NA, NA, NA, NA, NA, 4, … #\u0026gt; $ CompHrsDayChild \u0026lt;int\u0026gt; NA, NA, NA, 1, NA, 0, 6, NA, NA, NA, NA, NA, NA, 3, … #\u0026gt; $ Alcohol12PlusYr \u0026lt;fct\u0026gt; Yes, Yes, Yes, NA, Yes, NA, NA, Yes, Yes, Yes, Yes, … #\u0026gt; $ AlcoholDay \u0026lt;int\u0026gt; NA, NA, NA, NA, 2, NA, NA, 3, 3, 3, 1, 2, 6, NA, NA,… #\u0026gt; $ AlcoholYear \u0026lt;int\u0026gt; 0, 0, 0, NA, 20, NA, NA, 52, 52, 52, 100, 104, 364, … #\u0026gt; $ SmokeNow \u0026lt;fct\u0026gt; No, No, No, NA, Yes, NA, NA, NA, NA, NA, No, NA, NA,… #\u0026gt; $ Smoke100 \u0026lt;fct\u0026gt; Yes, Yes, Yes, NA, Yes, NA, NA, No, No, No, Yes, No,… #\u0026gt; $ Smoke100n \u0026lt;fct\u0026gt; Smoker, Smoker, Smoker, NA, Smoker, NA, NA, Non-Smok… #\u0026gt; $ SmokeAge \u0026lt;int\u0026gt; 18, 18, 18, NA, 38, NA, NA, NA, NA, NA, 13, NA, NA, … #\u0026gt; $ Marijuana \u0026lt;fct\u0026gt; Yes, Yes, Yes, NA, Yes, NA, NA, Yes, Yes, Yes, NA, Y… #\u0026gt; $ AgeFirstMarij \u0026lt;int\u0026gt; 17, 17, 17, NA, 18, NA, NA, 13, 13, 13, NA, 19, 15, … #\u0026gt; $ RegularMarij \u0026lt;fct\u0026gt; No, No, No, NA, No, NA, NA, No, No, No, NA, Yes, Yes… #\u0026gt; $ AgeRegMarij \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 20, 15, … #\u0026gt; $ HardDrugs \u0026lt;fct\u0026gt; Yes, Yes, Yes, NA, Yes, NA, NA, No, No, No, No, Yes,… #\u0026gt; $ SexEver \u0026lt;fct\u0026gt; Yes, Yes, Yes, NA, Yes, NA, NA, Yes, Yes, Yes, Yes, … #\u0026gt; $ SexAge \u0026lt;int\u0026gt; 16, 16, 16, NA, 12, NA, NA, 13, 13, 13, 17, 22, 12, … #\u0026gt; $ SexNumPartnLife \u0026lt;int\u0026gt; 8, 8, 8, NA, 10, NA, NA, 20, 20, 20, 15, 7, 100, NA,… #\u0026gt; $ SexNumPartYear \u0026lt;int\u0026gt; 1, 1, 1, NA, 1, NA, NA, 0, 0, 0, NA, 1, 1, NA, NA, 1… #\u0026gt; $ SameSex \u0026lt;fct\u0026gt; No, No, No, NA, Yes, NA, NA, Yes, Yes, Yes, No, No, … #\u0026gt; $ SexOrientation \u0026lt;fct\u0026gt; Heterosexual, Heterosexual, Heterosexual, NA, Hetero… #\u0026gt; $ PregnantNow \u0026lt;fct\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …   To try out merging/joining, we\u0026rsquo;ll create two separate data frames by pulling out some variables from this NHANES dataset. One will contain demographic variables, and the other with have some physical measurements. Then we\u0026rsquo;ll join them back together. Let\u0026rsquo;s create the two sub-datasets first\u0026hellip;\n#Filter out rows with data from 2009-2010 and Age \u0026gt; 5,  #select a subset (4) of the variables, then get rid of  #all duplicate rows. Assign the output to object 'dem_data'. dem_data \u0026lt;- NHANES %\u0026gt;% filter(SurveyYr == \"2009_10\") %\u0026gt;% filter(Age \u0026gt; 5) %\u0026gt;% select(ID, Gender, Age, Education) %\u0026gt;% distinct() #similar as above, but with a different filter and  #selecting different variables. Save as 'phys_data' phys_data \u0026lt;- NHANES %\u0026gt;% filter(SurveyYr == \"2009_10\") %\u0026gt;% filter(Height \u0026lt; 180) %\u0026gt;% select(ID, Height, BMI, Pulse) %\u0026gt;% distinct()   Now explore them a bit\u0026hellip;\n#view the first 6 rows of each - note the shared ID column head(dem_data) #\u0026gt; # A tibble: 6 x 4 #\u0026gt; ID Gender Age Education  #\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt;  #\u0026gt; 1 51624 male 34 High School  #\u0026gt; 2 51630 female 49 Some College #\u0026gt; 3 51638 male 9 NA  #\u0026gt; 4 51646 male 8 NA  #\u0026gt; 5 51647 female 45 College Grad #\u0026gt; 6 51654 male 66 Some College head(phys_data) #\u0026gt; # A tibble: 6 x 4 #\u0026gt; ID Height BMI Pulse #\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 51624 165. 32.2 70 #\u0026gt; 2 51625 105. 15.3 NA #\u0026gt; 3 51630 168. 30.6 86 #\u0026gt; 4 51638 133. 16.8 82 #\u0026gt; 5 51646 131. 20.6 72 #\u0026gt; 6 51647 167. 27.2 62 #preview in another way - note the different numbers of observations (rows) glimpse(dem_data) #\u0026gt; Rows: 3,217 #\u0026gt; Columns: 4 #\u0026gt; $ ID \u0026lt;int\u0026gt; 51624, 51630, 51638, 51646, 51647, 51654, 51656, 51657, 516… #\u0026gt; $ Gender \u0026lt;fct\u0026gt; male, female, male, male, female, male, male, male, female,… #\u0026gt; $ Age \u0026lt;int\u0026gt; 34, 49, 9, 8, 45, 66, 58, 54, 10, 58, 50, 9, 33, 60, 16, 56… #\u0026gt; $ Education \u0026lt;fct\u0026gt; High School, Some College, NA, NA, College Grad, Some Colle… glimpse(phys_data) #\u0026gt; Rows: 3,021 #\u0026gt; Columns: 4 #\u0026gt; $ ID \u0026lt;int\u0026gt; 51624, 51625, 51630, 51638, 51646, 51647, 51654, 51657, 51659,… #\u0026gt; $ Height \u0026lt;dbl\u0026gt; 164.7, 105.4, 168.4, 133.1, 130.6, 166.7, 169.5, 169.4, 141.8,… #\u0026gt; $ BMI \u0026lt;dbl\u0026gt; 32.22, 15.30, 30.57, 16.82, 20.64, 27.24, 23.67, 26.03, 19.20,… #\u0026gt; $ Pulse \u0026lt;int\u0026gt; 70, NA, 86, 82, 72, 62, 60, 76, 80, 94, 74, 92, 84, 76, 64, 70…   Let\u0026rsquo;s use the shared ID column to join the two datasets together. We\u0026rsquo;ll do this in 4 different ways to compare different types of joins: inner_join(), left_join(), right_join(), and full_join(). Pay attention to the number of rows in the joined dataset each time and how it relates to the number of rows in each of the two individual datasets.\nThe basic structure of the dplyr *_join() functions is\u0026hellip;\n*_join(dataframe 'x', dataframe 'y', by = shared column name)\n 1 - inner_join() #perform an inner join join_inner \u0026lt;- inner_join(dem_data, phys_data, by = \"ID\") #preview the new object head(join_inner) #\u0026gt; # A tibble: 6 x 7 #\u0026gt; ID Gender Age Education Height BMI Pulse #\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 51624 male 34 High School 165. 32.2 70 #\u0026gt; 2 51630 female 49 Some College 168. 30.6 86 #\u0026gt; 3 51638 male 9 NA 133. 16.8 82 #\u0026gt; 4 51646 male 8 NA 131. 20.6 72 #\u0026gt; 5 51647 female 45 College Grad 167. 27.2 62 #\u0026gt; 6 51654 male 66 Some College 170. 23.7 60 #get dimensions dim(join_inner) #\u0026gt; [1] 2806 7   2 - left_join() #perform an left join join_left \u0026lt;- left_join(dem_data, phys_data, by = \"ID\") #preview the new object head(join_left) #\u0026gt; # A tibble: 6 x 7 #\u0026gt; ID Gender Age Education Height BMI Pulse #\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 51624 male 34 High School 165. 32.2 70 #\u0026gt; 2 51630 female 49 Some College 168. 30.6 86 #\u0026gt; 3 51638 male 9 NA 133. 16.8 82 #\u0026gt; 4 51646 male 8 NA 131. 20.6 72 #\u0026gt; 5 51647 female 45 College Grad 167. 27.2 62 #\u0026gt; 6 51654 male 66 Some College 170. 23.7 60 #get dimensions dim(join_left) #\u0026gt; [1] 3217 7   3 - right_join() #perform an right join join_right \u0026lt;- right_join(dem_data, phys_data, by = \"ID\") #preview the new object head(join_right) #\u0026gt; # A tibble: 6 x 7 #\u0026gt; ID Gender Age Education Height BMI Pulse #\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 51624 male 34 High School 165. 32.2 70 #\u0026gt; 2 51630 female 49 Some College 168. 30.6 86 #\u0026gt; 3 51638 male 9 NA 133. 16.8 82 #\u0026gt; 4 51646 male 8 NA 131. 20.6 72 #\u0026gt; 5 51647 female 45 College Grad 167. 27.2 62 #\u0026gt; 6 51654 male 66 Some College 170. 23.7 60 #get dimensions dim(join_right) #\u0026gt; [1] 3021 7   4 - full_join() #perform an full join join_full \u0026lt;- full_join(dem_data, phys_data, by = \"ID\") #preview the new object head(join_full) #\u0026gt; # A tibble: 6 x 7 #\u0026gt; ID Gender Age Education Height BMI Pulse #\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 51624 male 34 High School 165. 32.2 70 #\u0026gt; 2 51630 female 49 Some College 168. 30.6 86 #\u0026gt; 3 51638 male 9 NA 133. 16.8 82 #\u0026gt; 4 51646 male 8 NA 131. 20.6 72 #\u0026gt; 5 51647 female 45 College Grad 167. 27.2 62 #\u0026gt; 6 51654 male 66 Some College 170. 23.7 60 #get dimensions dim(join_full) #\u0026gt; [1] 3432 7    Breakout rooms We\u0026rsquo;re going to add to our backyard birds dataset. I found a dataset that has life history data for a large number of species (birds and others). We\u0026rsquo;ll use species names to merge some of these life history variables in to the occurrence data we already have.\nIf you\u0026rsquo;re new and haven\u0026rsquo;t yet gotten the backyard bird dataset, get it first by running the code below. Otherwise, you can skip this step\u0026hellip;\n# create a directory called data that contains a subdirectory called birds dir.create('data/birds/', recursive = TRUE) # set the location of the file birds_file_url \u0026lt;- 'https://raw.githubusercontent.com/biodash/biodash.github.io/master/assets/data/birds/backyard-birds_Ohio.tsv' # set the path for the downloaded file birds_file \u0026lt;- 'data/birds/backyard-birds_Ohio.tsv' #download download.file(url = birds_file_url, destfile = birds_file)   Now (everybody), read in the bird data for this session\u0026hellip;\nbirds_file \u0026lt;- 'data/birds/backyard-birds_Ohio.tsv' birds \u0026lt;- read_tsv(birds_file) #\u0026gt; Parsed with column specification: #\u0026gt; cols( #\u0026gt; class = col_character(), #\u0026gt; order = col_character(), #\u0026gt; family = col_character(), #\u0026gt; genus = col_character(), #\u0026gt; species = col_character(), #\u0026gt; locality = col_character(), #\u0026gt; stateProvince = col_character(), #\u0026gt; decimalLatitude = col_double(), #\u0026gt; decimalLongitude = col_double(), #\u0026gt; eventDate = col_datetime(format = \"\"), #\u0026gt; species_en = col_character(), #\u0026gt; range = col_character() #\u0026gt; )   Exercise 1  Reduce the backyard bird dataset and keep just the following columns: species, locality, stateProvince, eventDate, species_en\n  Hints (click here)  Use select() to pull out the columns you want.    Solution (click here)  birds \u0026lt;- birds %\u0026gt;% select(species, locality, stateProvince, eventDate, species_en)      Exercise 2  Check to make sure things look right - how many columns does the birds dataset now have?\n  Hints (click here)  Use the dim() function. Or the ncol() function. Or glimpse(). Or head(). Or str(). Or even summary(). There\u0026rsquo;s lots of ways to do this.    Solution (click here)  dim(birds) #\u0026gt; [1] 311441 5       Exercise 3  Now download and read in the new life history dataset (tab separated) available at https://github.com/biodash/biodash.github.io/raw/master/assets/data/birds/esa_life_history_data_cc.tsv. Then explore it a bit - how many rows and columns are there?\n  Hints (click here)  Use the download.file() function like we did previously for the bird dataset. You\u0026rsquo;ll need to define the arguments \u0026lsquo;url\u0026rsquo; and \u0026lsquo;destfile\u0026rsquo; inside the parentheses. You can put the file anywhere you want, but I\u0026rsquo;d suggest in the same directory as the bird file we got, so, for example, the destination file could be \u0026ldquo;data/birds/life_history_data.tsv\u0026rdquo;.    Solution (click here)  #download the file from online and save it as a '.tsv' file (since it's tab delimited) download.file(url = \"https://github.com/biodash/biodash.github.io/raw/master/assets/data/birds/esa_life_history_data_cc.tsv\", destfile = \"data/birds/life_history_data.tsv\") #read the data in to R as an object named 'life_hist' life_hist \u0026lt;- read_tsv(file = \"data/birds/life_history_data.tsv\") #\u0026gt; Parsed with column specification: #\u0026gt; cols( #\u0026gt; class = col_character(), #\u0026gt; order = col_character(), #\u0026gt; family = col_character(), #\u0026gt; genus = col_character(), #\u0026gt; species = col_character(), #\u0026gt; common_name = col_character(), #\u0026gt; female_maturity_d = col_double(), #\u0026gt; litter_or_clutch_size_n = col_double(), #\u0026gt; litters_or_clutches_per_y = col_double(), #\u0026gt; adult_body_mass_g = col_double(), #\u0026gt; maximum_longevity_y = col_double(), #\u0026gt; egg_mass_g = col_double(), #\u0026gt; incubation_d = col_double(), #\u0026gt; fledging_age_d = col_double(), #\u0026gt; longevity_y = col_double(), #\u0026gt; adult_svl_cm = col_double() #\u0026gt; ) #preview the data glimpse(life_hist) #\u0026gt; Rows: 21,322 #\u0026gt; Columns: 16 #\u0026gt; $ class \u0026lt;chr\u0026gt; \"Aves\", \"Aves\", \"Aves\", \"Aves\", \"Aves\", \"Av… #\u0026gt; $ order \u0026lt;chr\u0026gt; \"Accipitriformes\", \"Accipitriformes\", \"Acci… #\u0026gt; $ family \u0026lt;chr\u0026gt; \"Accipitridae\", \"Accipitridae\", \"Accipitrid… #\u0026gt; $ genus \u0026lt;chr\u0026gt; \"Accipiter\", \"Accipiter\", \"Accipiter\", \"Acc… #\u0026gt; $ species \u0026lt;chr\u0026gt; \"Accipiter albogularis\", \"Accipiter badius\"… #\u0026gt; $ common_name \u0026lt;chr\u0026gt; \"Pied Goshawk\", \"Shikra\", \"Bicolored Hawk\",… #\u0026gt; $ female_maturity_d \u0026lt;dbl\u0026gt; NA, 363.468, NA, NA, 363.468, NA, NA, 547.8… #\u0026gt; $ litter_or_clutch_size_n \u0026lt;dbl\u0026gt; NA, 3.250, 2.700, NA, 4.000, NA, 2.700, 4.2… #\u0026gt; $ litters_or_clutches_per_y \u0026lt;dbl\u0026gt; NA, 1, NA, NA, 1, NA, NA, 1, NA, 1, NA, 1, … #\u0026gt; $ adult_body_mass_g \u0026lt;dbl\u0026gt; 251.500, 140.000, 345.000, 142.000, 203.500… #\u0026gt; $ maximum_longevity_y \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, NA, 19.90000, NA, 2… #\u0026gt; $ egg_mass_g \u0026lt;dbl\u0026gt; NA, 21.00, 32.00, NA, 21.85, NA, 32.00, 19.… #\u0026gt; $ incubation_d \u0026lt;dbl\u0026gt; NA, 30.00, NA, NA, 32.50, NA, NA, 33.00, NA… #\u0026gt; $ fledging_age_d \u0026lt;dbl\u0026gt; NA, 32.00, NA, NA, 42.50, NA, NA, 24.25, NA… #\u0026gt; $ longevity_y \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, NA, 12.58333, NA, 1… #\u0026gt; $ adult_svl_cm \u0026lt;dbl\u0026gt; NA, 30.00, 39.50, NA, 33.50, NA, 39.50, 29.…       Exercise 4  This new dataset contains life history data for more than just birds. What Classes of organisms are represented in the \u0026lsquo;Class\u0026rsquo; variable?\n  Hints (click here)  Try using a combination of the select() and distinct() functions to pull out the column you\u0026rsquo;re interested in, and then to get the distinct values, respectively.    Solutions (click here)  life_hist %\u0026gt;% select(class) %\u0026gt;% distinct() #\u0026gt; # A tibble: 3 x 1 #\u0026gt; class  #\u0026gt; \u0026lt;chr\u0026gt;  #\u0026gt; 1 Aves  #\u0026gt; 2 Mammalia #\u0026gt; 3 Reptilia       Exercise 5  Reduce the life history dataset down to keep just the rows for Class Aves and the columns species, adult_body_mass_g, adult_svl_cm, longevity_y, litter_or_clutch_size_n. What are the dimensions now?\n  Hints (click here)  Use filter() along with an appropriate logical expression to keep the rows we want. Use select() to get the desired columns.    Solutions (click here)  # pull out target rows and columns life_hist_aves \u0026lt;- life_hist %\u0026gt;% filter(class == \"Aves\") %\u0026gt;% select(species, adult_body_mass_g, adult_svl_cm, longevity_y, litter_or_clutch_size_n) dim(life_hist_aves) #\u0026gt; [1] 9802 5       Exercise 6  Preview each dataset again, just to make sure you\u0026rsquo;re clear about what\u0026rsquo;s in each one. Are there any columns that are shared between the two?\n  Hints (click here)  Consider glimpse() or head() to preview the datasets (tibbles/data frames). If you want to use a function to find shared columns, try a combination of intersect() and names().\n   Solutions (click here)  glimpse(birds) #\u0026gt; Rows: 311,441 #\u0026gt; Columns: 12 #\u0026gt; $ class \u0026lt;chr\u0026gt; \"Aves\", \"Aves\", \"Aves\", \"Aves\", \"Aves\", \"Aves\", \"Ave… #\u0026gt; $ order \u0026lt;chr\u0026gt; \"Passeriformes\", \"Passeriformes\", \"Passeriformes\", \"… #\u0026gt; $ family \u0026lt;chr\u0026gt; \"Corvidae\", \"Corvidae\", \"Corvidae\", \"Corvidae\", \"Cor… #\u0026gt; $ genus \u0026lt;chr\u0026gt; \"Cyanocitta\", \"Cyanocitta\", \"Cyanocitta\", \"Cyanocitt… #\u0026gt; $ species \u0026lt;chr\u0026gt; \"Cyanocitta cristata\", \"Cyanocitta cristata\", \"Cyano… #\u0026gt; $ locality \u0026lt;chr\u0026gt; \"44805 Ashland\", \"45244 Cincinnati\", \"44132 Euclid\",… #\u0026gt; $ stateProvince \u0026lt;chr\u0026gt; \"Ohio\", \"Ohio\", \"Ohio\", \"Ohio\", \"Ohio\", \"Ohio\", \"Ohi… #\u0026gt; $ decimalLatitude \u0026lt;dbl\u0026gt; 40.86166, 39.10666, 41.60768, 39.24236, 39.28207, 41… #\u0026gt; $ decimalLongitude \u0026lt;dbl\u0026gt; -82.31558, -84.32972, -81.50085, -84.35545, -84.4688… #\u0026gt; $ eventDate \u0026lt;dttm\u0026gt; 2007-02-16, 2007-02-17, 2007-02-17, 2007-02-19, 200… #\u0026gt; $ species_en \u0026lt;chr\u0026gt; \"Blue Jay\", \"Blue Jay\", \"Blue Jay\", \"Blue Jay\", \"Blu… #\u0026gt; $ range \u0026lt;chr\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … glimpse(life_hist_aves) #\u0026gt; Rows: 9,802 #\u0026gt; Columns: 5 #\u0026gt; $ species \u0026lt;chr\u0026gt; \"Accipiter albogularis\", \"Accipiter badius\", … #\u0026gt; $ adult_body_mass_g \u0026lt;dbl\u0026gt; 251.500, 140.000, 345.000, 142.000, 203.500, … #\u0026gt; $ adult_svl_cm \u0026lt;dbl\u0026gt; NA, 30.00, 39.50, NA, 33.50, NA, 39.50, 29.00… #\u0026gt; $ longevity_y \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, NA, 12.58333, NA, 12.… #\u0026gt; $ litter_or_clutch_size_n \u0026lt;dbl\u0026gt; NA, 3.250, 2.700, NA, 4.000, NA, 2.700, 4.250… intersect(names(birds), names(life_hist_aves)) #\u0026gt; [1] \"species\"       Exercise 7  Now lets join them together based on their shared variable. Not all species in the backyard bird (Ohio) dataset are included in the life history dataset. Likewise, there are life history data for many species that aren\u0026rsquo;t in the Ohio dataset. We want to keep all the Ohio observations, and merge in life history data for species where it\u0026rsquo;s availble, but we also don\u0026rsquo;t want to add in life history data for species that aren\u0026rsquo;t in the Ohio dataset. Choose an appropriate join function with those things in mind.\n  Hints (click here)  Try a left_join(), defining the Ohio backyard bird dataset as the \u0026lsquo;x\u0026rsquo; dataset in the join and the life history data as the \u0026lsquo;y\u0026rsquo; dataset. Get details on that function with ?left_join.    Solutions (click here)  joined_data \u0026lt;- left_join(x = birds, y = life_hist_aves, by = \"species\")       Exercise 8  What are the longest- and shortest-living bird species in Ohio based on the data in the longevity_y column?\n  Hints (click here)  Try using select() to pull out just the columns species and longevity_y, then use distinct() to get the unique rows, then arrange() based on the longevity_y column. You might also find the dplyr function desc() helpful.\nAlternatively, you could try grouping by species, then use summarise() to get either the max, min, or mean value for longevity_y for each species (there\u0026rsquo;s just one value for each species, so all of those statistics give the same value in this case). Then sort (arrange) the resulting summarized data frame on the longevity value.\n   Solutions (click here)  #option 1 - shortest-lived birds joined_data %\u0026gt;% select(species, longevity_y) %\u0026gt;% distinct() %\u0026gt;% arrange(longevity_y) #\u0026gt; # A tibble: 171 x 2 #\u0026gt; species longevity_y #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 Loxia leucoptera 4  #\u0026gt; 2 Spiza americana 4  #\u0026gt; 3 Certhia americana 4.6  #\u0026gt; 4 Acanthis hornemanni 4.6  #\u0026gt; 5 Tringa flavipes 4.75 #\u0026gt; 6 Podiceps grisegena 4.8  #\u0026gt; 7 Calcarius lapponicus 5  #\u0026gt; 8 Anthus rubescens 5.1  #\u0026gt; 9 Perdix perdix 5.17 #\u0026gt; 10 Regulus satrapa 5.32 #\u0026gt; # … with 161 more rows #option 1 - longest-lived birds joined_data %\u0026gt;% select(species, longevity_y) %\u0026gt;% distinct() %\u0026gt;% arrange(desc(longevity_y)) #\u0026gt; # A tibble: 171 x 2 #\u0026gt; species longevity_y #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 Larus argentatus 33.4 #\u0026gt; 2 Larus glaucoides 33  #\u0026gt; 3 Larus thayeri 33  #\u0026gt; 4 Haliaeetus leucocephalus 33.0 #\u0026gt; 5 Larus fuscus 32.8 #\u0026gt; 6 Aquila chrysaetos 32  #\u0026gt; 7 Anas platyrhynchos 29  #\u0026gt; 8 Larus delawarensis 28.6 #\u0026gt; 9 Asio otus 27.8 #\u0026gt; 10 Cygnus olor 27.7 #\u0026gt; # … with 161 more rows #option 2 - shortest-lived birds joined_data %\u0026gt;% group_by(species) %\u0026gt;% summarise(longevity = max(longevity_y)) %\u0026gt;% arrange(longevity) #\u0026gt; `summarise()` ungrouping output (override with `.groups` argument) #\u0026gt; # A tibble: 171 x 2 #\u0026gt; species longevity #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 Loxia leucoptera 4  #\u0026gt; 2 Spiza americana 4  #\u0026gt; 3 Acanthis hornemanni 4.6  #\u0026gt; 4 Certhia americana 4.6  #\u0026gt; 5 Tringa flavipes 4.75 #\u0026gt; 6 Podiceps grisegena 4.8  #\u0026gt; 7 Calcarius lapponicus 5  #\u0026gt; 8 Anthus rubescens 5.1  #\u0026gt; 9 Perdix perdix 5.17 #\u0026gt; 10 Regulus satrapa 5.32 #\u0026gt; # … with 161 more rows #option 2 - longest-lived birds joined_data %\u0026gt;% group_by(species) %\u0026gt;% summarise(longevity = max(longevity_y)) %\u0026gt;% arrange(desc(longevity)) #\u0026gt; `summarise()` ungrouping output (override with `.groups` argument) #\u0026gt; # A tibble: 171 x 2 #\u0026gt; species longevity #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 Larus argentatus 33.4 #\u0026gt; 2 Larus glaucoides 33  #\u0026gt; 3 Larus thayeri 33  #\u0026gt; 4 Haliaeetus leucocephalus 33.0 #\u0026gt; 5 Larus fuscus 32.8 #\u0026gt; 6 Aquila chrysaetos 32  #\u0026gt; 7 Anas platyrhynchos 29  #\u0026gt; 8 Larus delawarensis 28.6 #\u0026gt; 9 Asio otus 27.8 #\u0026gt; 10 Cygnus olor 27.7 #\u0026gt; # … with 161 more rows       Bonus time! Bonus 1  What species in Ohio has the largest ratio of adult body mass to length (measured as snout vent length, or \u0026lsquo;adult_svl_cm\u0026rsquo;)?\n  Hints (click here)  Use mutate() to create a new variable containing the body mass divided by svl, then arrange the dataset using that new variable to get the species with the highest value.\n   Solutions (click here)  joined_data %\u0026gt;% mutate(ratio = adult_body_mass_g/adult_svl_cm) %\u0026gt;% select(species_en, ratio) %\u0026gt;% distinct() %\u0026gt;% arrange(desc(ratio)) #\u0026gt; # A tibble: 170 x 2 #\u0026gt; species_en ratio #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 Mute Swan 71.8 #\u0026gt; 2 Wild Turkey 68.0 #\u0026gt; 3 Trumpeter Swan 64.9 #\u0026gt; 4 Bald Eagle 59.2 #\u0026gt; 5 Golden Eagle 56.2 #\u0026gt; 6 Canada Goose 48.3 #\u0026gt; 7 Tundra Swan 47.0 #\u0026gt; 8 Cackling Goose 44.4 #\u0026gt; 9 Snow Goose 35.1 #\u0026gt; 10 Snowy Owl 32.8 #\u0026gt; # … with 160 more rows       Bonus 2  There are 2 additional joins we haven\u0026rsquo;t talked about - semi_join() and anti_join(). Take a look at the documentation to see what these do. Use one of them to find what species in the backyard birds dataset are not in the life history dataset.\n  Hints (click here)  Use anti_join() and distinct().\n   Solutions (click here)  anti_join(birds, life_hist_aves, by = \"species\") %\u0026gt;% select(species, species_en) %\u0026gt;% distinct() #\u0026gt; # A tibble: 6 x 2 #\u0026gt; species species_en  #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt;  #\u0026gt; 1 Dendrocopos pubescens Downy Woodpecker  #\u0026gt; 2 Spizelloides arborea American Tree Sparrow #\u0026gt; 3 Otus asio Eastern Screech Owl  #\u0026gt; 4 Larus minutus Little Gull  #\u0026gt; 5 Anas rubripes x platyrhynchos NA  #\u0026gt; 6 NA NA       Bonus 3  The life history dataset we downloaded above is actually a modified version of the original file, which is located at \u0026lsquo;http://www.esapubs.org/archive/ecol/E096/269/Data_Files/Amniote_Database_Aug_2015.csv\u0026rsquo;\nTry starting with the original file and repeating what we did above - merging the variables species, adult_body_mass_g, adult_svl_cm, longevity_y, litter_or_clutch_size_n in to the original birds dataset. First, make sure to get it read in correctly. Then pay attention to the species column in the life history dataset - what needs to be done before a join/merge can be performed?\n  Hints (click here)  Pay attention to how missing data are coded in this dataset (it\u0026rsquo;s -999). Also, data are very sparse for some of the variables - in other words, they have lots of missing data. This seems to cause a problem with the read_csv() function, as it only considers the first 1000 rows for the purpose of defining the class of each column. This can be a problem if all of the first 1000 rows are missing. Finally, it appears that even though this is a comma separated file (commas define the column breaks), there are a few instances where commas are used within a field. This happens in the \u0026lsquo;common name\u0026rsquo; column in a few cases where multiple common names are listed for a specific observation. This is one example of something that can become quite frustrating when trying to get data loaded in, and is worth keeping an eye out for. Fortunately, in our case, it only seems to happen for non-bird species in this dataset, which we filter out anyway, so it can be dealt with. However, if it had impacted any of the bird observations, I think fixing this might require a solution outside of R - possibly a command line approach.\n   Solutions (click here)  #download download.file(url = \"http://www.esapubs.org/archive/ecol/E096/269/Data_Files/Amniote_Database_Aug_2015.csv\", destfile = \"data/birds/orig_life_history.csv\") #read the data in to R as an object named 'full_life_hist' full_life_hist \u0026lt;- read_csv(\"data/birds/orig_life_history.csv\", na = \"-999\", col_types = cols(birth_or_hatching_svl_cm = col_double(), weaning_d = col_double(),gestation_d = col_double(), weaning_weight_g = col_double(), male_svl_cm = col_double(), female_svl_cm = col_double(), no_sex_svl_cm = col_double(), female_body_mass_at_maturity_g = col_double(), female_svl_at_maturity_cm = col_double())) #get the original version of the birds dataset birds \u0026lt;- read_tsv('data/birds/backyard-birds_Ohio.tsv') #\u0026gt; Parsed with column specification: #\u0026gt; cols( #\u0026gt; class = col_character(), #\u0026gt; order = col_character(), #\u0026gt; family = col_character(), #\u0026gt; genus = col_character(), #\u0026gt; species = col_character(), #\u0026gt; locality = col_character(), #\u0026gt; stateProvince = col_character(), #\u0026gt; decimalLatitude = col_double(), #\u0026gt; decimalLongitude = col_double(), #\u0026gt; eventDate = col_datetime(format = \"\"), #\u0026gt; species_en = col_character(), #\u0026gt; range = col_character() #\u0026gt; ) #subset each for the columns and rows we want life_hist_aves \u0026lt;- full_life_hist %\u0026gt;% filter(class == \"Aves\") %\u0026gt;% select(species, adult_body_mass_g, adult_svl_cm, longevity_y, litter_or_clutch_size_n) birds \u0026lt;- birds %\u0026gt;% select(species, locality, stateProvince, eventDate, species_en) glimpse(birds) #\u0026gt; Rows: 311,441 #\u0026gt; Columns: 5 #\u0026gt; $ species \u0026lt;chr\u0026gt; \"Cyanocitta cristata\", \"Cyanocitta cristata\", \"Cyanocit… #\u0026gt; $ locality \u0026lt;chr\u0026gt; \"44805 Ashland\", \"45244 Cincinnati\", \"44132 Euclid\", \"4… #\u0026gt; $ stateProvince \u0026lt;chr\u0026gt; \"Ohio\", \"Ohio\", \"Ohio\", \"Ohio\", \"Ohio\", \"Ohio\", \"Ohio\",… #\u0026gt; $ eventDate \u0026lt;dttm\u0026gt; 2007-02-16, 2007-02-17, 2007-02-17, 2007-02-19, 2007-0… #\u0026gt; $ species_en \u0026lt;chr\u0026gt; \"Blue Jay\", \"Blue Jay\", \"Blue Jay\", \"Blue Jay\", \"Blue J… glimpse(life_hist_aves) #\u0026gt; Rows: 9,802 #\u0026gt; Columns: 5 #\u0026gt; $ species \u0026lt;chr\u0026gt; \"albogularis\", \"badius\", \"bicolor\", \"brachyur… #\u0026gt; $ adult_body_mass_g \u0026lt;dbl\u0026gt; 251.500, 140.000, 345.000, 142.000, 203.500, … #\u0026gt; $ adult_svl_cm \u0026lt;dbl\u0026gt; NA, 30.00, 39.50, NA, 33.50, NA, 39.50, 29.00… #\u0026gt; $ longevity_y \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, NA, 12.58333, NA, 12.… #\u0026gt; $ litter_or_clutch_size_n \u0026lt;dbl\u0026gt; NA, 3.250, 2.700, NA, 4.000, NA, 2.700, 4.250… #notice the species column in the life history data doesn't include the genus name. Since the names don't match in the species column from each dataset, a join won't work. Add the genus variable in from the original life history data... life_hist_aves \u0026lt;- full_life_hist %\u0026gt;% filter(class == \"Aves\") %\u0026gt;% select(genus, species, adult_body_mass_g, adult_svl_cm, longevity_y, litter_or_clutch_size_n) #now use mutate to replace the species column so it includes both the genus and species... life_hist_aves \u0026lt;- life_hist_aves %\u0026gt;% mutate(species = paste0(genus, \" \", species)) %\u0026gt;% select(-genus) #preview again glimpse(birds) #\u0026gt; Rows: 311,441 #\u0026gt; Columns: 5 #\u0026gt; $ species \u0026lt;chr\u0026gt; \"Cyanocitta cristata\", \"Cyanocitta cristata\", \"Cyanocit… #\u0026gt; $ locality \u0026lt;chr\u0026gt; \"44805 Ashland\", \"45244 Cincinnati\", \"44132 Euclid\", \"4… #\u0026gt; $ stateProvince \u0026lt;chr\u0026gt; \"Ohio\", \"Ohio\", \"Ohio\", \"Ohio\", \"Ohio\", \"Ohio\", \"Ohio\",… #\u0026gt; $ eventDate \u0026lt;dttm\u0026gt; 2007-02-16, 2007-02-17, 2007-02-17, 2007-02-19, 2007-0… #\u0026gt; $ species_en \u0026lt;chr\u0026gt; \"Blue Jay\", \"Blue Jay\", \"Blue Jay\", \"Blue Jay\", \"Blue J… glimpse(life_hist_aves) #\u0026gt; Rows: 9,802 #\u0026gt; Columns: 5 #\u0026gt; $ species \u0026lt;chr\u0026gt; \"Accipiter albogularis\", \"Accipiter badius\", … #\u0026gt; $ adult_body_mass_g \u0026lt;dbl\u0026gt; 251.500, 140.000, 345.000, 142.000, 203.500, … #\u0026gt; $ adult_svl_cm \u0026lt;dbl\u0026gt; NA, 30.00, 39.50, NA, 33.50, NA, 39.50, 29.00… #\u0026gt; $ longevity_y \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, NA, 12.58333, NA, 12.… #\u0026gt; $ litter_or_clutch_size_n \u0026lt;dbl\u0026gt; NA, 3.250, 2.700, NA, 4.000, NA, 2.700, 4.250… #now we can join joined_data \u0026lt;- left_join(birds, life_hist_aves, by = \"species\")       \n","date":1607040000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638999211,"objectID":"ac21371122a93dfd4641f808ad457bc3","permalink":"https://biodash.github.io/codeclub/s03_joining-datasets/","publishdate":"2020-12-04T00:00:00Z","relpermalink":"/codeclub/s03_joining-datasets/","section":"codeclub","summary":"In this session of Code Club, we'll explore some methods for combining datasets according to a shared variable, with primary focus on the `*_join()` set of functions from **dplyr**. We'll also keep practicing with some of the core dplyr verbs from last session.","tags":null,"title":"Session 3: Joining Datasets","type":"codeclub"},{"authors":["Jessica Cooperstone"],"categories":null,"content":"\n Prep homework Basic computer setup   If you didn\u0026rsquo;t already do this, please follow the Code Club Computer Setup instructions, which also has pointers for if you\u0026rsquo;re new to R or RStudio.\n  If you\u0026rsquo;re able to do so, please open RStudio a bit before Code Club starts \u0026ndash; and in case you run into issues, please join the Zoom call early and we\u0026rsquo;ll troubleshoot.\n  New to dplyr? If you\u0026rsquo;ve never used dplyr before (or even if you have), you may find this cheat sheet useful.\n Getting Started Want to download an R script with the content from today\u0026rsquo;s session? # directory for Code Club Session 2: dir.create(\"S02\") # directory for our script # (\"recursive\" to create two levels at once.) dir.create(\"S02/scripts/\") # save the url location for today's script todays_R_script \u0026lt;- 'https://raw.githubusercontent.com/biodash/biodash.github.io/master/content/codeclub/02_dplyr-core-verbs/2_Dplyr_one-table_verbs.R' # indicate the name of the new script file Session2_dplyr_core \u0026lt;- \"S02/scripts/Session2_script.R\" # go get that file!  download.file(url = todays_R_script, destfile = Session2_dplyr_core)    1 - What is data wrangling? It has been estimated that the process of getting your data into the appropriate formats takes about 80% of the total time of analysis. We will talk about formatting as tidy data (e.g., such that each column is a single variable, each row is a single observation, and each cell is a single value, you can learn more about tidy data here) in a future session of Code Club.\nThe package dplyr, as part of the tidyverse has a number of very helpful functions that will help you get your data into a format suitable for your analysis.\n What will we go over today\nThese five core dplyr() verbs will help you get wrangling.\n select() - picks variables (i.e., columns) based on their names filter() - picks observations (i.e., rows) based on their values mutate() - makes new variables, keeps existing columns arrange() - sorts rows based on values in columns summarize() - reduces values down to a summary form     2 - Get ready to wrangle Let\u0026rsquo;s get set up and grab some data so that we can get familiar with these verbs\n You can do this locally, or at OSC. You can find instructions if you are having trouble here.  First load your libraries.\nlibrary(tidyverse) #\u0026gt; ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── #\u0026gt; ✔ ggplot2 3.3.2 ✔ purrr  0.3.4 #\u0026gt; ✔ tibble  3.0.4 ✔ dplyr  1.0.2 #\u0026gt; ✔ tidyr  1.1.2 ✔ stringr 1.4.0 #\u0026gt; ✔ readr  1.4.0 ✔ forcats 0.5.0 #\u0026gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── #\u0026gt; ✖ dplyr::filter() masks stats::filter() #\u0026gt; ✖ dplyr::lag() masks stats::lag()   Then let\u0026rsquo;s access the iris dataset that comes pre-loaded in base R. We will take that data frame and assign it to a new object called iris_data. Then we will look at our data.\niris_data \u0026lt;- iris # look at the first 6 rows, all columns head(iris_data) #\u0026gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #\u0026gt; 1 5.1 3.5 1.4 0.2 setosa #\u0026gt; 2 4.9 3.0 1.4 0.2 setosa #\u0026gt; 3 4.7 3.2 1.3 0.2 setosa #\u0026gt; 4 4.6 3.1 1.5 0.2 setosa #\u0026gt; 5 5.0 3.6 1.4 0.2 setosa #\u0026gt; 6 5.4 3.9 1.7 0.4 setosa # check the structure of iris_data glimpse(iris_data) #\u0026gt; Rows: 150 #\u0026gt; Columns: 5 #\u0026gt; $ Sepal.Length \u0026lt;dbl\u0026gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4… #\u0026gt; $ Sepal.Width \u0026lt;dbl\u0026gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3… #\u0026gt; $ Petal.Length \u0026lt;dbl\u0026gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1… #\u0026gt; $ Petal.Width \u0026lt;dbl\u0026gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0… #\u0026gt; $ Species \u0026lt;fct\u0026gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, …   This dataset contains the measurements (in cm) of Sepal.Length, Sepal.Width, Petal.Length, and Petal.Width for three different Species of iris, setosa, versicolor, and virginica.\n 3 - Using select() select() allows you to pick certain columns to be included in your data frame.\nWe will create a dew data frame called iris_petals_species that includes the columns Species, Petal.Length and Petal.Width.\niris_petals_species \u0026lt;- iris_data %\u0026gt;% select(Species, Petal.Length, Petal.Width)   What does our new data frame look like?\nhead(iris_petals_species) #\u0026gt; Species Petal.Length Petal.Width #\u0026gt; 1 setosa 1.4 0.2 #\u0026gt; 2 setosa 1.4 0.2 #\u0026gt; 3 setosa 1.3 0.2 #\u0026gt; 4 setosa 1.5 0.2 #\u0026gt; 5 setosa 1.4 0.2 #\u0026gt; 6 setosa 1.7 0.4   Note - look what happened to the order of the columns!\nThis is not the only way to select columns.\nYou could also subset by indexing with the square brackets, but you can see how much more readable using select() is. It\u0026rsquo;s nice not to have to refer back to remember what column is which index.\niris_data_indexing \u0026lt;- iris_data[,3:5] head(iris_data_indexing) #\u0026gt; Petal.Length Petal.Width Species #\u0026gt; 1 1.4 0.2 setosa #\u0026gt; 2 1.4 0.2 setosa #\u0026gt; 3 1.3 0.2 setosa #\u0026gt; 4 1.5 0.2 setosa #\u0026gt; 5 1.4 0.2 setosa #\u0026gt; 6 1.7 0.4 setosa   iris_data_c \u0026lt;- iris_data[,c(\"Petal.Length\", \"Petal.Width\", \"Species\")] head(iris_data_c) #\u0026gt; Petal.Length Petal.Width Species #\u0026gt; 1 1.4 0.2 setosa #\u0026gt; 2 1.4 0.2 setosa #\u0026gt; 3 1.3 0.2 setosa #\u0026gt; 4 1.5 0.2 setosa #\u0026gt; 5 1.4 0.2 setosa #\u0026gt; 6 1.7 0.4 setosa     4 - Using filter()  Artwork by Allison Horst.  filter() allows you to pick certain observations (i.e, rows) based on their values to be included in your data frame.\nWe will create a new data frame that only includes information about the irises where their Species is setosa.\niris_setosa \u0026lt;- iris_data %\u0026gt;% filter(Species == \"setosa\")   Let\u0026rsquo;s check the dimensions of our data frame. Remember, our whole data set is 150 observations, and we are expecting 50 observations per Species.\ndim(iris_setosa) #\u0026gt; [1] 50 5    5 - Using mutate()  Artwork by Allison Horst.  mutate() allows you to make new variables, while keeping all your existing columns.\nLet\u0026rsquo;s make a new column that is the ratio of Sepal.Length/Sepal.Width\niris_sepal_length_to_width \u0026lt;- iris_data %\u0026gt;% mutate(Sepal.Length_div_Sepal.Width = Sepal.Length/Sepal.Width)   head(iris_sepal_length_to_width) #\u0026gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #\u0026gt; 1 5.1 3.5 1.4 0.2 setosa #\u0026gt; 2 4.9 3.0 1.4 0.2 setosa #\u0026gt; 3 4.7 3.2 1.3 0.2 setosa #\u0026gt; 4 4.6 3.1 1.5 0.2 setosa #\u0026gt; 5 5.0 3.6 1.4 0.2 setosa #\u0026gt; 6 5.4 3.9 1.7 0.4 setosa #\u0026gt; Sepal.Length_div_Sepal.Width #\u0026gt; 1 1.457143 #\u0026gt; 2 1.633333 #\u0026gt; 3 1.468750 #\u0026gt; 4 1.483871 #\u0026gt; 5 1.388889 #\u0026gt; 6 1.384615   Note \u0026ndash; see the new column location\n 6 - Using arrange() Very often you will want to order your data frame by some values. To do this, you can use arrange().\nLet\u0026rsquo;s arrange the values in our iris_data by Sepal.Length.\niris_data_sort_Sepal.Length \u0026lt;- iris_data %\u0026gt;% arrange(Sepal.Length) head(iris_data_sort_Sepal.Length) #\u0026gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #\u0026gt; 1 4.3 3.0 1.1 0.1 setosa #\u0026gt; 2 4.4 2.9 1.4 0.2 setosa #\u0026gt; 3 4.4 3.0 1.3 0.2 setosa #\u0026gt; 4 4.4 3.2 1.3 0.2 setosa #\u0026gt; 5 4.5 2.3 1.3 0.3 setosa #\u0026gt; 6 4.6 3.1 1.5 0.2 setosa   What if we want to arrange by Sepal.Length, but within Species? We can do that using the helper group_by().\niris_data %\u0026gt;% group_by(Species) %\u0026gt;% arrange(Sepal.Length) #\u0026gt; # A tibble: 150 x 5 #\u0026gt; # Groups: Species [3] #\u0026gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt;  #\u0026gt; 1 4.3 3 1.1 0.1 setosa  #\u0026gt; 2 4.4 2.9 1.4 0.2 setosa  #\u0026gt; 3 4.4 3 1.3 0.2 setosa  #\u0026gt; 4 4.4 3.2 1.3 0.2 setosa  #\u0026gt; 5 4.5 2.3 1.3 0.3 setosa  #\u0026gt; 6 4.6 3.1 1.5 0.2 setosa  #\u0026gt; 7 4.6 3.4 1.4 0.3 setosa  #\u0026gt; 8 4.6 3.6 1 0.2 setosa  #\u0026gt; 9 4.6 3.2 1.4 0.2 setosa  #\u0026gt; 10 4.7 3.2 1.3 0.2 setosa  #\u0026gt; # … with 140 more rows    7 - Using summarize() By using summarize(), you can create a new data frame that has the summary output you have requested.\nWe can calculate the mean Sepal.Length across our dataset.\niris_data %\u0026gt;% summarize(mean = mean(Sepal.Length)) #\u0026gt; mean #\u0026gt; 1 5.843333   What if we want to calculate means for each Species?\niris_data %\u0026gt;% group_by(Species) %\u0026gt;% summarize(mean = mean(Sepal.Length)) #\u0026gt; `summarise()` ungrouping output (override with `.groups` argument) #\u0026gt; # A tibble: 3 x 2 #\u0026gt; Species mean #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 setosa 5.01 #\u0026gt; 2 versicolor 5.94 #\u0026gt; 3 virginica 6.59   We can integrate some helper functions into our code to simply get out a variety of outputs. We can use across() to apply our summary aross a set of columns. I really like this function.\niris_data %\u0026gt;% group_by(Species) %\u0026gt;% summarize(across(where(is.numeric), mean)) #\u0026gt; `summarise()` ungrouping output (override with `.groups` argument) #\u0026gt; # A tibble: 3 x 5 #\u0026gt; Species Sepal.Length Sepal.Width Petal.Length Petal.Width #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 setosa 5.01 3.43 1.46 0.246 #\u0026gt; 2 versicolor 5.94 2.77 4.26 1.33  #\u0026gt; 3 virginica 6.59 2.97 5.55 2.03   This can also be useful for counting observations per group. Here, how many iris observations do we have per Species?\niris_data %\u0026gt;% group_by(Species) %\u0026gt;% tally() #\u0026gt; # A tibble: 3 x 2 #\u0026gt; Species n #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 setosa 50 #\u0026gt; 2 versicolor 50 #\u0026gt; 3 virginica 50 iris_data %\u0026gt;% count(Species) #\u0026gt; Species n #\u0026gt; 1 setosa 50 #\u0026gt; 2 versicolor 50 #\u0026gt; 3 virginica 50 iris_data %\u0026gt;% group_by(Species) %\u0026gt;% summarize(n = n()) #\u0026gt; `summarise()` ungrouping output (override with `.groups` argument) #\u0026gt; # A tibble: 3 x 2 #\u0026gt; Species n #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 setosa 50 #\u0026gt; 2 versicolor 50 #\u0026gt; 3 virginica 50    8 - Breakout rooms! Read in data Now you try! We are going to use the Great Backyard Birds dataset we downloaded two weeks ago and you will apply the functions we have learned above to investigate this dataset.\nIf you weren\u0026rsquo;t here for Session 1, get the birds data set.\n# create a directory called S02 dir.create('S02') # within S02, create a directory called data, within, a directory called birds dir.create('data/birds/', recursive = TRUE)   Download the file from the internet.\n# set the location of the file birds_file_url \u0026lt;- 'https://raw.githubusercontent.com/biodash/biodash.github.io/master/assets/data/birds/backyard-birds_Ohio.tsv' # set the path for the downloaded file birds_file \u0026lt;- 'data/birds/backyard-birds_Ohio.tsv' # download  download.file(url = birds_file_url, destfile = birds_file)   If you were here for Session 1, join back in! Let\u0026rsquo;s read in our data.\nbirds_file \u0026lt;- 'data/birds/backyard-birds_Ohio.tsv' birds \u0026lt;- read_tsv(birds_file) #\u0026gt;  #\u0026gt; ── Column specification ──────────────────────────────────────────────────────── #\u0026gt; cols( #\u0026gt; class = col_character(), #\u0026gt; order = col_character(), #\u0026gt; family = col_character(), #\u0026gt; genus = col_character(), #\u0026gt; species = col_character(), #\u0026gt; locality = col_character(), #\u0026gt; stateProvince = col_character(), #\u0026gt; decimalLatitude = col_double(), #\u0026gt; decimalLongitude = col_double(), #\u0026gt; eventDate = col_datetime(format = \"\"), #\u0026gt; species_en = col_character(), #\u0026gt; range = col_character() #\u0026gt; )   Exercises Below you can find our breakout room exercises for today.\nExercise 1  Investigate the structure of the birds dataset.\n  Solution (click here)  glimpse(birds) #\u0026gt; Rows: 311,441 #\u0026gt; Columns: 12 #\u0026gt; $ class \u0026lt;chr\u0026gt; \"Aves\", \"Aves\", \"Aves\", \"Aves\", \"Aves\", \"Aves\", \"Ave… #\u0026gt; $ order \u0026lt;chr\u0026gt; \"Passeriformes\", \"Passeriformes\", \"Passeriformes\", \"… #\u0026gt; $ family \u0026lt;chr\u0026gt; \"Corvidae\", \"Corvidae\", \"Corvidae\", \"Corvidae\", \"Cor… #\u0026gt; $ genus \u0026lt;chr\u0026gt; \"Cyanocitta\", \"Cyanocitta\", \"Cyanocitta\", \"Cyanocitt… #\u0026gt; $ species \u0026lt;chr\u0026gt; \"Cyanocitta cristata\", \"Cyanocitta cristata\", \"Cyano… #\u0026gt; $ locality \u0026lt;chr\u0026gt; \"44805 Ashland\", \"45244 Cincinnati\", \"44132 Euclid\",… #\u0026gt; $ stateProvince \u0026lt;chr\u0026gt; \"Ohio\", \"Ohio\", \"Ohio\", \"Ohio\", \"Ohio\", \"Ohio\", \"Ohi… #\u0026gt; $ decimalLatitude \u0026lt;dbl\u0026gt; 40.86166, 39.10666, 41.60768, 39.24236, 39.28207, 41… #\u0026gt; $ decimalLongitude \u0026lt;dbl\u0026gt; -82.31558, -84.32972, -81.50085, -84.35545, -84.4688… #\u0026gt; $ eventDate \u0026lt;dttm\u0026gt; 2007-02-16, 2007-02-17, 2007-02-17, 2007-02-19, 200… #\u0026gt; $ species_en \u0026lt;chr\u0026gt; \"Blue Jay\", \"Blue Jay\", \"Blue Jay\", \"Blue Jay\", \"Blu… #\u0026gt; $ range \u0026lt;chr\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …       Exercise 2  Create a new data frame that removes the column range.\n  Hints (click here)  \nTry using select(). Remember, you can tell select() what you want to keep, and what you want to remove.    Solutions (click here)  birds_no_range \u0026lt;- birds %\u0026gt;% select(-range) head(birds_no_range) #\u0026gt; # A tibble: 6 x 11 #\u0026gt; class order family genus species locality stateProvince decimalLatitude #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 Aves Pass… Corvi… Cyan… Cyanoc… 44805 A… Ohio 40.9 #\u0026gt; 2 Aves Pass… Corvi… Cyan… Cyanoc… 45244 C… Ohio 39.1 #\u0026gt; 3 Aves Pass… Corvi… Cyan… Cyanoc… 44132 E… Ohio 41.6 #\u0026gt; 4 Aves Pass… Corvi… Cyan… Cyanoc… 45242 C… Ohio 39.2 #\u0026gt; 5 Aves Pass… Corvi… Cyan… Cyanoc… 45246 C… Ohio 39.3 #\u0026gt; 6 Aves Pass… Corvi… Cyan… Cyanoc… 44484 W… Ohio 41.2 #\u0026gt; # … with 3 more variables: decimalLongitude \u0026lt;dbl\u0026gt;, eventDate \u0026lt;dttm\u0026gt;, #\u0026gt; # species_en \u0026lt;chr\u0026gt;       Exercise 3  How many unique species of birds have been observed?.\n  Hints (click here)  Try using summarize() with a group_by() helper.    Solutions (click here)  # using a combo of group_by() and summarize() unique_birds \u0026lt;- birds %\u0026gt;% group_by(species_en) %\u0026gt;% summarize() #\u0026gt; `summarise()` ungrouping output (override with `.groups` argument) dim(unique_birds) # question - are there really 170 different birds observed? take a look at this summary #\u0026gt; [1] 170 1 # a one line, base R approach length(unique(birds$species_en)) #\u0026gt; [1] 170 # another base R approach using distinct() and nrow() birds %\u0026gt;% distinct(species_en) %\u0026gt;% # find distinct occurences nrow() # counts rows #\u0026gt; [1] 170 # using n_distinct() birds %\u0026gt;% summarize(n_distinct(species_en)) #\u0026gt; # A tibble: 1 x 1 #\u0026gt; `n_distinct(species_en)` #\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 170       Exercise 4  How many times have Bald Eagles been observed?.\n  Hints (click here)  Try using filter(). Remember the syntax you need to use to indicate you are looking for a Bald Eagle.    Solutions (click here)  birds_bald_eagle \u0026lt;- birds %\u0026gt;% filter(species_en == \"Bald Eagle\") dim(birds_bald_eagle) #\u0026gt; [1] 381 12       Exercise 5  How many times have any kind of eagle been observed?. Group hint: there are only Bald Eagle and Golden Eagle in this dataset.\n  Hints (click here)  There is a way to denote OR within filter().    More Hints (click here)  You denote OR by using the vertical bar.    Solutions (click here)  birds_alleagles \u0026lt;- birds %\u0026gt;% filter(species_en == \"Bald Eagle\" | species_en == \"Golden Eagle\") dim(birds_alleagles) #\u0026gt; [1] 386 12       Exercise 6  What is the northern most location of the bird observations in Ohio?\n  Hints (click here)  Try using arrange(). You can arrange in both ascending and descending order. You can also use your Ohio knowledge to check if you\u0026rsquo;ve done this correctly.    Solutions (click here)  birds_sort_lat \u0026lt;- birds %\u0026gt;% arrange(-decimalLatitude) head(birds_sort_lat) #\u0026gt; # A tibble: 6 x 12 #\u0026gt; class order family genus species locality stateProvince decimalLatitude #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 Aves Pass… Cardi… Card… Cardin… Conneaut Ohio 41.9 #\u0026gt; 2 Aves Pass… Ember… Zono… Zonotr… Conneaut Ohio 41.9 #\u0026gt; 3 Aves Colu… Colum… Zena… Zenaid… Conneaut Ohio 41.9 #\u0026gt; 4 Aves Pici… Picid… Dend… Dendro… Conneaut Ohio 41.9 #\u0026gt; 5 Aves Anse… Anati… Anas Anas p… Conneaut Ohio 41.9 #\u0026gt; 6 Aves Pass… Turdi… Sial… Sialia… Conneaut Ohio 41.9 #\u0026gt; # … with 4 more variables: decimalLongitude \u0026lt;dbl\u0026gt;, eventDate \u0026lt;dttm\u0026gt;, #\u0026gt; # species_en \u0026lt;chr\u0026gt;, range \u0026lt;chr\u0026gt;       Bonus time! Bonus 1  What is the most commonly observed bird in Ohio?\n  Hints (click here)  Try using tally() and a little helper term.\n   Solutions (click here)  unique_birds_tally \u0026lt;- birds %\u0026gt;% group_by(species_en) %\u0026gt;% tally(sort = TRUE) head(unique_birds_tally) #\u0026gt; # A tibble: 6 x 2 #\u0026gt; species_en n #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 Northern Cardinal 23064 #\u0026gt; 2 Mourning Dove 19135 #\u0026gt; 3 Dark-eyed Junco 18203 #\u0026gt; 4 Downy Woodpecker 17196 #\u0026gt; 5 House Sparrow 15939 #\u0026gt; 6 Blue Jay 15611 # another option birds %\u0026gt;% count(species_en, sort = TRUE) #\u0026gt; # A tibble: 170 x 2 #\u0026gt; species_en n #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 Northern Cardinal 23064 #\u0026gt; 2 Mourning Dove 19135 #\u0026gt; 3 Dark-eyed Junco 18203 #\u0026gt; 4 Downy Woodpecker 17196 #\u0026gt; 5 House Sparrow 15939 #\u0026gt; 6 Blue Jay 15611 #\u0026gt; 7 American Goldfinch 14732 #\u0026gt; 8 House Finch 14551 #\u0026gt; 9 Tufted Titmouse 14409 #\u0026gt; 10 Black-capped Chickadee 13471 #\u0026gt; # … with 160 more rows       Bonus 2  What is the least commonly observed bird (or birds) in Ohio?\n  Hints (click here)  Try using the data frame you\u0026rsquo;ve created in the previous exercise.    Solutions (click here)  unique_birds_tally %\u0026gt;% arrange(n) #\u0026gt; # A tibble: 170 x 2 #\u0026gt; species_en n #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 Arctic Redpoll 1 #\u0026gt; 2 Clay-colored Sparrow 1 #\u0026gt; 3 Dickcissel 1 #\u0026gt; 4 Eurasian Wigeon 1 #\u0026gt; 5 Great Egret 1 #\u0026gt; 6 Green Heron 1 #\u0026gt; 7 Grey Partridge 1 #\u0026gt; 8 Harris's Sparrow 1 #\u0026gt; 9 Lesser Yellowlegs 1 #\u0026gt; 10 Lincoln's Sparrow 1 #\u0026gt; # … with 160 more rows # or, if you knew the rarest was those observed only once  unique_birds_tally %\u0026gt;% filter(n == 1) #\u0026gt; # A tibble: 19 x 2 #\u0026gt; species_en n #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 Arctic Redpoll 1 #\u0026gt; 2 Clay-colored Sparrow 1 #\u0026gt; 3 Dickcissel 1 #\u0026gt; 4 Eurasian Wigeon 1 #\u0026gt; 5 Great Egret 1 #\u0026gt; 6 Green Heron 1 #\u0026gt; 7 Grey Partridge 1 #\u0026gt; 8 Harris's Sparrow 1 #\u0026gt; 9 Lesser Yellowlegs 1 #\u0026gt; 10 Lincoln's Sparrow 1 #\u0026gt; 11 Loggerhead Shrike 1 #\u0026gt; 12 Nelson's Sparrow 1 #\u0026gt; 13 Northern Rough-winged Swallow 1 #\u0026gt; 14 Orchard Oriole 1 #\u0026gt; 15 Prairie Falcon 1 #\u0026gt; 16 Red-throated Loon 1 #\u0026gt; 17 Ross's Goose 1 #\u0026gt; 18 Warbling Vireo 1 #\u0026gt; 19 Western Osprey 1       Bonus 3  In what year were the most Bald Eagles observed?\n  Hints (click here)  You may want to convert your date column to a more simplified year-only date. Check out the package lubridate.    Solutions (click here)  library(lubridate) #\u0026gt;  #\u0026gt; Attaching package: 'lubridate' #\u0026gt; The following objects are masked from 'package:base': #\u0026gt;  #\u0026gt; date, intersect, setdiff, union birds_bald_eagle_year \u0026lt;- birds_bald_eagle %\u0026gt;% mutate(year = year(eventDate)) %\u0026gt;% # year() takes a date and outputs only year group_by(year) %\u0026gt;% tally() arrange(birds_bald_eagle_year, -n) #\u0026gt; # A tibble: 11 x 2 #\u0026gt; year n #\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 2008 81 #\u0026gt; 2 2006 66 #\u0026gt; 3 2009 58 #\u0026gt; 4 2007 40 #\u0026gt; 5 2005 30 #\u0026gt; 6 2004 26 #\u0026gt; 7 2000 23 #\u0026gt; 8 2001 23 #\u0026gt; 9 2003 15 #\u0026gt; 10 2002 14 #\u0026gt; 11 1999 5      \n","date":1606694400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606593313,"objectID":"83eb9b3aa4eba14930c3c05a7e4ad0bc","permalink":"https://biodash.github.io/codeclub/02_dplyr-core-verbs/","publishdate":"2020-11-30T00:00:00Z","relpermalink":"/codeclub/02_dplyr-core-verbs/","section":"codeclub","summary":"During this second session of Code Club, we will be learning how to use some of the most popular dplyr one-table functions, including filter, select, mutate, arrange, and summarize.","tags":null,"title":"Session 2: dplyr core verbs","type":"codeclub"},{"authors":["Jelmer Poelstra"],"categories":[],"content":"\n Prep homework Basic computer setup If you didn\u0026rsquo;t already do this, please follow the Code Club Computer Setup instructions.\nTest if it works Please open RStudio locally or start an OSC RStudio Server session.\nNov 19 addition: If you\u0026rsquo;re working locally, test if you can load the tidyverse package with library(\u0026quot;tidyverse\u0026quot;) inside R. (If you haven\u0026rsquo;t installed the tidyverse yet, please go to the Code Club Computer Setup instructions.)\nIf you have not used RStudio before, take a moment to explore what\u0026rsquo;s in the panels and tabs. (It may help to check out Mike Sovic\u0026rsquo;s 1-minute intro to the RStudio interface or RStudio\u0026rsquo;s 3-minute intro.)\nIf you\u0026rsquo;re able to do so, please open RStudio again a bit before Code Club starts \u0026ndash; and in case you run into issues, please join the Zoom call early and we\u0026rsquo;ll troubleshoot.\nNew to R? If you\u0026rsquo;re completely new to R, it will be useful to have a look at some of the resources listed on our New to R? page prior to Code Club.\n Slides On Friday, we started with a couple of introductory slides.\n  1 - Create an RStudio Project Projects are an RStudio-specific concept that create a special file (.Rproj), primarily to designate a directory as the working directory for everything within it. We recommend creating exactly one separate Project for each research project with an R component \u0026ndash; and for things like Code Club.\n Why use Projects?\nIn brief, Projects help you to organize your work and to make it more portable.\n  They record which scripts (and R Markdown files) are open in RStudio, and will reopen all of those when you reopen the project. This becomes quite handy, say, when you work on three different projects, each of which uses a number of scripts.\n  When using Projects, you generally don\u0026rsquo;t have to manually set your working directory, and can use relative file paths to refer to files within the project. This way, even if you move the project directory, or copy it to a different computer, the same paths will still work. (This would not be the case if you used setwd() which will generally require you to use an absolute path, e.g. setwd(\u0026quot;C:/Users/Jelmer/Documents/\u0026quot;).)\n  Projects encourage you to organize research projects inside self-contained directories, rather than with files spread around your computer. This can save you a lot of headaches and increases reproducibility. And because R will restart whenever you switch Projects, there is no risk of unwanted cross-talk between your projects.\n    Let\u0026rsquo;s create an RStudio Project for Code Club:\n  Open RStudio locally or start an OSC RStudio Server session.\n(If you\u0026rsquo;re at OSC, you should see a file 0_CODECLUB.md that\u0026rsquo;s open in your top-left panel. You can ignore/close this file.)\n  If you\u0026rsquo;re working locally, create a directory wherever you like on your computer for all things Code Club. You can do this in R using dir.create(\u0026quot;path/to/your/dir\u0026quot;), or outside of R.\n(If you\u0026rsquo;re at OSC, skip this step because you\u0026rsquo;re automatically inside a Code Club-specific, personal directory.)\n  Click File (top menu bar) \u0026gt; New Project, and then select Existing Directory.\n  If you\u0026rsquo;re working locally, select the Code Club directory that you created in the previous step.\n  If you\u0026rsquo;re working at OSC, keep the default choice \u0026ldquo;~\u0026rdquo; (i.e., home), which is the directory you started in when entering the RStudio Server session.\n    After RStudio automatically reloads, you should see the file ending in .Rproj in the RStudio Files tab in the lower right pane, and you will have the Project open. All done for now!\n  (For future Code Club sessions: RStudio will by default reopen the most recently used Project, and therefore, OSC users will have the Project automatically opened. If you\u0026rsquo;re working locally and are also using other Projects, you can open this Project with File \u0026gt; Open Project inside RStudio, or by clicking the .Rproj file in your file browser, which will open RStudio and the Project.)\n 2 - Orienting ourselves Where are we? We don\u0026rsquo;t need to set our working directory, because our newly created Project is open, and therefore, our working directory is the directory that contains the .Rproj file.\nTo see where you are, type or copy into the console (bottom left):\n# Print the working directory: getwd() # List the files in your current directory: dir() # This should print at least the `.RProj` file.   Create directories Create two new directories \u0026ndash; one for this session, and one for a dataset that we will download shortly (and will be reusing across sessions):\n# Dir for Code Club Session 1: dir.create(\"S01\") # Dir for our bird data: # (\"recursive\" to create two levels at once.) dir.create(\"data/birds/\", recursive = TRUE)   Create a script To keep a record of what we are doing, and to easily modify and rerun earlier commands, we\u0026rsquo;ll want to save our commands in a script and execute them from there, rather than typing our commands directly in the console.\n  Click File (top menu bar) \u0026gt; New File \u0026gt; R script.\n  Save the script (File \u0026gt; Save) as S01.R inside your S01 directory.\n  First line of the script We will now load the core set of 8 tidyverse packages all at once. To do so, type/copy the command below on the first line of the script, and then execute it by clicking Run (top right of script pane) or by pressing Ctrl Enter (Windows/Linux, this should also work in your browser) or ⌘ Enter (Mac).\n# If you're working locally, and did not install it yet: # install.packages(\"tidyverse\") # Load the tidyverse (meta)package: library(tidyverse) #\u0026gt; ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── #\u0026gt; ✔ ggplot2 3.3.2 ✔ purrr  0.3.4 #\u0026gt; ✔ tibble  3.0.4 ✔ dplyr  1.0.2 #\u0026gt; ✔ tidyr  1.1.2 ✔ stringr 1.4.0 #\u0026gt; ✔ readr  1.3.1 ✔ forcats 0.5.0 #\u0026gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── #\u0026gt; ✖ dplyr::filter() masks stats::filter() #\u0026gt; ✖ dplyr::lag() masks stats::lag()   If this worked, you should get the same output as shown in the code block above: it attached 8 packages, and it warns that some of its functions are now \u0026ldquo;masking\u0026rdquo; base R functions.\n The tidyverse is a very popular and useful ecosystem of R packages for data analysis, which we will be using a lot in Code Club.\nWhen we refer to \u0026ldquo;base R\u0026rdquo; as opposed to the tidyverse, we mean functions that are loaded in R by default (without loading a package), and that can perform similar operations in a different way.\n   3 - Getting our dataset We downloaded a Great Backyard Bird Count (GBBC) dataset from the Global Biodiversity Information Facility (GBIF). Because the file was 3.1 GB large, we selected only the records from Ohio and removed some uninformative columns. We also added columns with English names and the breeding range for each species. We\u0026rsquo;ll download the resulting much smaller file (41.5 MB) from our Github repo.\n The Great Backyard Bird Count The GBBC is an annual citizen science event where everyone is encouraged to to identify and count birds in their backyard \u0026ndash; or anywhere else \u0026ndash; for at least 15 minutes, and report their sightings online. Since 2013, it is a global event, but it has been organized in the US and Canada since 1998.\n  Download the data Let\u0026rsquo;s download the dataset using the download.file() function:\n# The URL to our file: birds_file_url \u0026lt;- \"https://raw.githubusercontent.com/biodash/biodash.github.io/master/assets/data/birds/backyard-birds_Ohio.tsv\" # The path to the file we want to download to: birds_file \u0026lt;- \"data/birds/backyard-birds_Ohio.tsv\" # Download: download.file(url = birds_file_url, destfile = birds_file)   Read the data Now, let\u0026rsquo;s read the file into R. The .tsv extension (\u0026ldquo;tab-separated values\u0026rdquo;) tells us this is a plain text file in which columns are separated by tabs, so we will use a convenience function from the readr package (which is loaded as part of the core set tidyverse packages) for exactly this type of file:\n# Read the data: birds \u0026lt;- read_tsv(file = birds_file) #\u0026gt; Parsed with column specification: #\u0026gt; cols( #\u0026gt; class = col_character(), #\u0026gt; order = col_character(), #\u0026gt; family = col_character(), #\u0026gt; genus = col_character(), #\u0026gt; species = col_character(), #\u0026gt; locality = col_character(), #\u0026gt; stateProvince = col_character(), #\u0026gt; decimalLatitude = col_double(), #\u0026gt; decimalLongitude = col_double(), #\u0026gt; eventDate = col_datetime(format = \"\"), #\u0026gt; species_en = col_character(), #\u0026gt; range = col_character() #\u0026gt; )   Done! We have now read our data into a tibble, which is a type of data frame (formally a data.frame): R\u0026rsquo;s object class to deal with tabular data wherein each column can contain a different type of data (numeric, characters/strings, etc).\n 4 - Exploring backyard birds Exercise 1 What\u0026rsquo;s in the dataset?\n  Explore the dataset using some functions and methods you may know to get a quick overview of data(frames), and try to understand what you see. What does a single row represent, and what is in each column? (Be sure to check out the hints below at some point, especially if you\u0026rsquo;re stuck.)\n  Pay attention to the data types (e.g., \u0026ldquo;character\u0026rdquo; or chr) of the different columns, which several of these functions print. The output of our read_tsv() command also printed this information \u0026ndash; this function parsed our columns as the types we see now. Were all the columns parsed correctly?\n  How many rows and how many columns does the dataset have?\n  What are some questions you would like to explore with this dataset? We\u0026rsquo;ll collect some of these and try to answer them in later sessions. If your group has sufficient R skills already, you are also welcome to go ahead and try to answer one or more of these questions.\n    Hints (click here)  # Type an object's name to print it to screen: birds # Same as above, but explicitly calling print(): print(birds) # For column-wise information (short for \"structure\"): str(birds) # tidyverse version of str(): glimpse(birds) # In RStudio, open object in a separate tab: View(birds)     Note that in R, dbl (for \u0026ldquo;double\u0026rdquo;) and num (for \u0026ldquo;numeric\u0026rdquo;) are both used, and almost interchangeably so, for floating point numbers. (Integers are a separate type that are simply called \u0026ldquo;integers\u0026rdquo; and abbreviated as int, but we have no integer columns in this dataset.)\n  read_tsv() parsed our date as a \u0026ldquo;date-time\u0026rdquo; (dttm or POSIXct for short), which contains both a date and a time. In our case, it looks like the time is always \u0026ldquo;00:00:00\u0026rdquo; and thus doesn\u0026rsquo;t provide any information.\n     Solutions (click here)  # Just printing the glimpse() output, # which will show the number of rows and columns: glimpse(birds) #\u0026gt; Rows: 311,441 #\u0026gt; Columns: 12 #\u0026gt; $ class \u0026lt;chr\u0026gt; \"Aves\", \"Aves\", \"Aves\", \"Aves\", \"Aves\", \"Aves\", \"Ave… #\u0026gt; $ order \u0026lt;chr\u0026gt; \"Passeriformes\", \"Passeriformes\", \"Passeriformes\", \"… #\u0026gt; $ family \u0026lt;chr\u0026gt; \"Corvidae\", \"Corvidae\", \"Corvidae\", \"Corvidae\", \"Cor… #\u0026gt; $ genus \u0026lt;chr\u0026gt; \"Cyanocitta\", \"Cyanocitta\", \"Cyanocitta\", \"Cyanocitt… #\u0026gt; $ species \u0026lt;chr\u0026gt; \"Cyanocitta cristata\", \"Cyanocitta cristata\", \"Cyano… #\u0026gt; $ locality \u0026lt;chr\u0026gt; \"44805 Ashland\", \"45244 Cincinnati\", \"44132 Euclid\",… #\u0026gt; $ stateProvince \u0026lt;chr\u0026gt; \"Ohio\", \"Ohio\", \"Ohio\", \"Ohio\", \"Ohio\", \"Ohio\", \"Ohi… #\u0026gt; $ decimalLatitude \u0026lt;dbl\u0026gt; 40.86166, 39.10666, 41.60768, 39.24236, 39.28207, 41… #\u0026gt; $ decimalLongitude \u0026lt;dbl\u0026gt; -82.31558, -84.32972, -81.50085, -84.35545, -84.4688… #\u0026gt; $ eventDate \u0026lt;dttm\u0026gt; 2007-02-16, 2007-02-17, 2007-02-17, 2007-02-19, 200… #\u0026gt; $ species_en \u0026lt;chr\u0026gt; \"Blue Jay\", \"Blue Jay\", \"Blue Jay\", \"Blue Jay\", \"Blu… #\u0026gt; $ range \u0026lt;chr\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …   # You can also check the number of rows and columns directly using: dim(birds) # Will return the number of rows and columns #\u0026gt; [1] 311441 12 nrow(birds) # Will return the number of rows #\u0026gt; [1] 311441 ncol(birds) # Will return the number of columns #\u0026gt; [1] 12     \n  Bonus material If your breakout group is done with Exercise 1, you can have a look at the bonus material below which includes another exercise. You can also have a look at this as homework. Or not at all!\n readr options for challenging files Earlier, we successfully read in our file without specifying any arguments other than the file name to the read_tsv() function, i.e. with all the default options. It is not always this easy!\nSome options for more complex cases:\n  The more general counterpart of this function is read_delim(), which allows you to specify the delimiter using the sep argument, e.g. delim=\u0026quot;\\t\u0026quot; for tabs.\n  There are also arguments to these functions for when you need to skip lines, when you don\u0026rsquo;t have column headers, when you need to specify the column types of some or all the columns, and so forth \u0026ndash; see this example:\nmy_df \u0026lt;- read_delim( file = \"file.txt\", delim = \"\\t\", # Specify tab as delimiter col_names = FALSE, # First line is not a header skip = 3, # Skip the first three lines comment = \"#\", # Skip any line beginning with a \"#\" col_types = cols( # Specify column types col1 = col_character(), # ..We only need to specify columns for  col2 = col_double() # ..which we need non-automatic typing ) )       Exercise 2 (Optional) Read this file!\nTry to read the following file into R, which is a modified and much smaller version of the bird dataset.\nMake the function parse the \u0026ldquo;order\u0026rdquo; column as a factor, and the \u0026ldquo;year\u0026rdquo;, \u0026ldquo;month\u0026rdquo;, and \u0026ldquo;day\u0026rdquo; columns as whatever you think is sensible.\n# Download and read the file: birds2_file_url \u0026lt;- \"https://raw.githubusercontent.com/biodash/biodash.github.io/master/assets/data/birds/backyard-birds_read-challenge.txt\" birds2_file \u0026lt;- \"data/birds/backyard-birds_read-challenge.txt\" download.file(url = birds2_file_url, destfile = birds2_file)   # Your turn! birds2 \u0026lt;- read_ # Complete the command     Hints (click here)    The file is saved as .txt, so the delimiter is not obvious \u0026ndash; first have a look at it (open it in RStudio, a text editor, or the terminal) to determine the delimiter. Then, use read_delim() with manual specification of the delimiter using the delim argument, or use a specialized convenience function.\n  Besides a leading line with no data, there is another problematic line further down. You will need both the skip and comment arguments to circumvent these.\n  Note that readr erroneously parses month as a character column if you don\u0026rsquo;t manually specify its type.\n  Note that you can also use a succinct column type specification like col_types = \u0026quot;fc\u0026quot;, which would parse, for a two-column file, the first column as a factor and the second as a character \u0026ndash; type e.g. ?read_tsv for details.\n     Bare solution (click here)  # With succint column type specification: birds2 \u0026lt;- read_csv( file = birds2_file, skip = 1, comment = \"$\", col_types = \"fcdiii\" ) # With long column type specification: birds2 \u0026lt;- read_csv( file = birds2_file, skip = 1, comment = \"$\", col_types = cols( order = col_factor(), year = col_integer(), month = col_integer(), day = col_integer() ) )      Solution with explanations (click here)  # With succinct column type specification: birds2 \u0026lt;- read_csv( # `read_csv()`: file is comma-delimited file = birds2_file, skip = 1, # First line is not part of the dataframe comment = \"$\", # Line 228 is a comment that starts with `$` col_types = \"fcdiii\" # \"f\" for factor, \"c\" for character, ) # ..\"d\" for double (=numeric), # ..\"i\" for integer. # With long column type specification: birds2 \u0026lt;- read_csv( file = birds2_file, skip = 1, comment = \"$\", col_types = cols( # We can omit columns for which we order = col_factor(), # ..accept the automatic parsing, year = col_integer(), # ..when using the long specification.  month = col_integer(), day = col_integer() ) )      Other options for reading tabular data There are also functions in base R that read tabular data, such as read.table() and read.delim().\nThese are generally slower than the readr functions, and have less sensible default options to their arguments. Particularly relevant is how columns with characters (strings) are parsed \u0026ndash; until R 4.0, which was released earlier this year, base R\u0026rsquo;s default behavior was to parse them as factors, and this is generally not desirable1. readr functions will never convert columns with strings to factors.\nIf speed is important, such as when reading in very large files (~ 100s of MBs or larger), you should consider using the fread() function from the data.table package.\nFinally, some examples of reading other types of files:\n Read excel files directly using the readxl package. Read Google Sheets directly from the web using the googlesheets4 package. Read non-tabular data using the base R readLines() function.    \n  You can check which version of R you are running by typing sessionInfo(). You can also check directly how strings are read by default with default.stringsAsFactors(). To avoid conversion to factors, specify stringsAsFactors = FALSE in your read.table() / read.delim() function call. \u0026#x21a9;\u0026#xfe0e;\n   ","date":1604448000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605969915,"objectID":"5b8cbf273e25c833a3e7f4cd2615654b","permalink":"https://biodash.github.io/codeclub/01_backyard-birds/","publishdate":"2020-11-04T00:00:00Z","relpermalink":"/codeclub/01_backyard-birds/","section":"codeclub","summary":"In the first session of Code Club, we'll make sure that everyone is properly set up, create an RStudio Project, and start working with some data from the Great Backyard Bird Count.","tags":["codeclub","backyard-birds"],"title":"Session 1: Backyard Birds","type":"codeclub"},{"authors":["Jelmer Poelstra","Mike Sovic","Stephen Opiyo","Michael Broe","Jessica Cooperstone"],"categories":[],"content":" Welcome to OSU Code Club! Materials for each episode will be provided in posts like this one, collected in the Code Club Sessions page.\n  For more information about OSU Code Club, and a form to sign up, see the About Code Club page.\n  For info on upcoming sessions, see here.\n  You can code locally or in your browser, see our page with computer setup instructions.\n  If you are completely new to R, see our page with resources and tips.\n  You can also suggest a topic to be covered at Code Club.\n   \n","date":1603065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604625794,"objectID":"fa58e17bb7648ba3c1d43d18ecb64a3a","permalink":"https://biodash.github.io/codeclub/00_welcome-to-codeclub/","publishdate":"2020-10-19T00:00:00Z","relpermalink":"/codeclub/00_welcome-to-codeclub/","section":"codeclub","summary":"Welcome to OSU Code Club! In this brief post, we point you to information related to Code Club on the website.","tags":["codeclub"],"title":"Welcome to Code Club","type":"codeclub"},{"authors":null,"categories":null,"content":"All material is released under a Creative Commons Attribution-ShareAlike 4.0 International License.\n   ","date":1578092400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601687264,"objectID":"53e892b8b41cc4caece1cfd5ef21d6e7","permalink":"https://biodash.github.io/license/","publishdate":"2020-01-04T00:00:00+01:00","relpermalink":"/license/","section":"","summary":"All material is released under a Creative Commons Attribution-ShareAlike 4.0 International License.\n   ","tags":null,"title":"LICENSE: CC-BY-SA","type":"page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1616606334,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://biodash.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"},{"authors":null,"categories":null,"content":"About OSU Code Club   Code Club will restart for the Spring of \u0026lsquo;23 on January 12th — see the schedule. You can join us at any point — to do so, please fill out this form!\n  OSU Code Club is a regularly occurring, interactive, online and in-person gathering to improve coding skills. We aim for a supportive and fun culture of learning together, and hope to offer something to participants at any experience level.\nIn each meeting, a presenter first introduces a concept or tool to be used for a challenge. Then, we work on the challenge in small groups, and finally, we reconvene to see what approaches were taken and to share lessons learned.\nSo far, we have focused entirely on the R language and its surrounding ecosystem (R Markdown, Shiny, etc). Depending on interest, we may in the future also cover Python, the Unix shell (terminal), running your analyses at the Ohio Supercomputer Center, and so on.\nLearn more about -and get ready for- Code Club   Schedule\n  Form to sign up\n  Computer setup instructions\n  Resources and tips for R novices\n  List of previous sessions by semester in table form\n  List of previous sessions as a blogroll\n  Form to suggest topics\n   More information   Each week, materials and suggested reading will be posted up front at the Sessions page. Like at a Journal Club, doing some preparatory homework by reading these materials will help you get the most out of it. [Note for the Summer \u0026lsquo;22 sessions: as we will be working through a book, we may not post additional materials online.]\n  Each session is intended to be mostly stand-alone to allow for occasional participation.\n  To allow for a welcoming environment for participants at all levels of experience, we ask everyone to be respectful, patient, and collaborative when interacting at Code Club. This is not a competitive event.\n  There will be no consistent analysis type or data type \u0026mdash; instead, we will focus on building general skills and applying those to a wide variety of data.\n  The idea for this Code Club was taken from a recent paper in PLoS Computational Biology: Ten simple rules to increase computational skills among biologists with Code Clubs. We liked this idea because of the high level of interaction and because gradual, well-spaced practice is an excellent way to retain what you learn.\n   Organizers  Jelmer Poelstra - Bioinformatician at MCIC Wooster Mike Sovic - Asst. Director at the Infectious Diseases Institute AMSL - Genomics Lab Stephen Opiyo - Biostatistician at MCIC Columbus Michael Broe - Bioinformatician at EEOB Jessica Cooperstone - Asst. Professor at HCS \u0026amp; FST   Sign up To sign up, please fill out the Google Form below. Hope to see you at Code Club!\nLoading…   \n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670964554,"objectID":"affd8a75456abca4d01de73213cffddb","permalink":"https://biodash.github.io/codeclub-about/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/codeclub-about/","section":"","summary":"About OSU Code Club   Code Club will restart for the Spring of \u0026lsquo;23 on January 12th — see the schedule. You can join us at any point — to do so, please fill out this form!","tags":null,"title":"","type":"page"},{"authors":null,"categories":null,"content":"Code Club:  R \u0026ndash; Getting Started and Some Tips  New to R? If you are completely new to R, we recommend watching at least the first couple of videos from Mike Sovic\u0026rsquo;s Youtube playlist of short videos on R, and ideally all of them, prior to attending Code Club. Here is the first video:\n  In case you want to do more self-study (note that this is not required/needed), here are some additional resources:\n A useful and fun written tutorial is R for cats. For a more systematic and lengthy introduction to R, see A Tutorial Introduction to R (this gets fairly advanced after section 9). Excellent comprehensive introductions are the R Basics and Visualization classes by Rafael Irizarry that can be freely accessed; you do have to create an account.  Also, don\u0026rsquo;t hesitate to reach out to the Code Club organizers if you have any questions!\n Miscellaneous R tips Useful settings By default, R will try to save your \u0026ldquo;environment\u0026rdquo; (e.g., your loaded data, variables, etc) when you exit, and then reload everything the way it was upon restarting R. However, this is bad! You should always be able to reproduce your environment given a set of commands saved in an R script or R Markdown document, whereas saving and reloading your environment encourages you to be sloppy about this.\nTo disable this in RStudio, go to Tools \u0026gt; Global Options \u0026gt; General and set the options as follows:\n  Recommended R/RStudio settings  To start R in the same way from the command line:\nR --no-save --no-restore-data \n Installing R packages CRAN packages To install an R package that is available at CRAN, the default R package repository, from within R (e.g. in the R console in RStudio), use the install.packages() function.\nThe install.packages() function will handle dependencies within R \u0026ndash; i.e., it will install other R packages that your package depends on. Occasionally, when the install function needs to compile a package from source, errors arise that relate to missing system dependencies (i.e. software outside of R).\nOn Mac and Linux, these system dependencies are best installed outside of R, such as with homebrew on Mac or apt on Ubuntu. The errror message you got when trying to install an R package should tell you which system dependencies are needed.\nOn Windows, you can use the installr package to install such dependencies or other software from within R \u0026ndash; for example:\ninstall.packages(\u0026#34;installr\u0026#34;) # Install the installr package first installlr::install.RStudio() # Install RStudio installr::install.python() # Install Python \nSystem setup to installing packages \u0026ldquo;from source\u0026rdquo; Sometimes you need to install a package from source, that is, you need to compile the package rather than simply installing a pre-existing binary. (On Linux, where installing from source is often needed, this should work without additional steps.) On Windows and Mac, installing from source is generally only needed when you install a package from outside of CRAN (such as from Github, see below), but you will need to make sure you have the following non-R software:\nOn Windows, you will need Rtools (Rtools installation instructions).\nOn a Mac, you will need Xcode (which can be installed from the Mac App store).\nYou can test whether or not you are able to install packages from source using the devtools package:\ninstall.packages(\u0026#34;devtools\u0026#34;) # Install the devtools package devtools::has_devel() # Check whether you can install packages from source For a bit more info, see this page.\nInstalling packages from Github To install a package from Github, use either the devtools or the remotes package \u0026ndash; for example:\ninstall.packages(\u0026#34;devtools\u0026#34;) # Install the devtools package devtools::install_github(\u0026#34;kbroman/broman\u0026#34;) # Install from a repository using \u0026#34;\u0026lt;username\u0026gt;/\u0026lt;repo-name\u0026gt;\u0026#34; This will install the package from source, so you will need to make sure you are able to do so by following the instructions in the section right above this one.\nInstalling packages from Bioconductor If you\u0026rsquo;re doing bioinformatic analyses in R, you will probably run into packages that are not on CRAN but on Bioconductor. To install a package from Bioconductor, use the BiocManager package \u0026ndash; for example:\ninstall.packages(\u0026#34;BiocManager\u0026#34;) # Install the BiocManager package BiocManager::install(\u0026#34;edgeR\u0026#34;) # Install the edgeR package from Bioconductor \nUpdating R Consider updating R if you have an older version of R installed. Specifically, in the first session of Code Club, we\u0026rsquo;ve seen problems when installing the tidyverse with R versions below R 3.6.\nYou can check which version of R you have by looking at the first lines of output when running the following command inside R:\nsessionInfo() To update:   Windows: You can update R from within R. The updateR() function will also take care of updating your packages:\ninstall.packages(\u0026#34;installr\u0026#34;) installr::updateR()   Mac: Download and install the latest .pkg file as if you were installing it for the first time.\n  Linux: In Ubuntu, if you installed R with apt or apt-get, you can use apt-get upgrade in a terminal. Otherwise, download and install the latest version after removing the old one. Rtask has some instructions for upgrading to R 4.0 in Ubuntu (along with upgrading to Ubuntu 20.04).\n  Re-installing your packages after updating (Mac and Linux) While the installr::updateR() function for Windows users takes care of reinstalling your packages along with updating R, Mac and Linux users will have to manually re-install their packages. Some people prefer to re-install these packages on the fly, which can end up being a way to get rid of packages you no longer use.\nBut if you want immediately reinstall all your packages, run this before you upgrade:\nmy_packages \u0026lt;- installed.packages() saveRDS(my_packages, \u0026#34;my_packages.rds\u0026#34;) Then, after you\u0026rsquo;ve installed the latest R version:\nmy_packages \u0026lt;- readRDS(\u0026#34;CurrentPackages.rds\u0026#34;) install.packages(my_packages[1, ]) This will only work for packages available on CRAN. Of course, you can check your list for Github-only and Bioconductor packages and then install those with their respective commands (see below). Yes, this can be a bit of a hassle!\n  \n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638999211,"objectID":"ca094992cb695d0d14880a7b5e13427b","permalink":"https://biodash.github.io/codeclub-novice/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/codeclub-novice/","section":"","summary":"Code Club:  R \u0026ndash; Getting Started and Some Tips  New to R? If you are completely new to R, we recommend watching at least the first couple of videos from Mike Sovic\u0026rsquo;s Youtube playlist of short videos on R, and ideally all of them, prior to attending Code Club.","tags":null,"title":"","type":"page"},{"authors":null,"categories":null,"content":"Events, courses, etc.  On this page, we will list upcoming events related to computational biology, coding, and data analysis. If you know of an event that you think should be listed here, please use the form in the About page.\n Recurring  MCIC bioinformatics office hour  Need some quick advice or help? Drop by! Every Tuesday from 2-4 pm: Zoom link. Advance notice to Jelmer Poelstra is appreciated. Anyone at OSU is welcome, and if you\u0026rsquo;re outside of OSU, feel free to inquire.     Center of Microbiome Center working groups  CoMS is running Virome, Microbiome, and Advanced Ecological Statistics working groups, see their page for more details.     Upcoming  For Ohio Supercomputer Center events, such as regular introductory sessions to computing at OSC, see the OSC Events page.   Misc. Relevant OSU Courses  M5161: Introduction to Computational Genomics M8161: Microbiome Informatics PLNTPTH 7003.01: Agricultural Genomics: Principles and Applications HCS 7806: Current Topics and Methods Courses  Includes \u0026ldquo;Genome Analytics\u0026rdquo; and \u0026ldquo;Methods in Data Visualization\u0026rdquo;.   ENR8600: Introduction to R for Environmental Sciences MOLGEN 5645: Quantitative, Population, and Evolutionary Genetics MOLGEN 5623: Genetics and Genomics STAT 6625: Statistical Analysis of Genetic Data STAT 6730: Introduction to Computational Statistics FDSCTE 7600: Metabolomics, Principles and Practice    \n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1654380034,"objectID":"426d522fcb82973f95d828bcc08f03ff","permalink":"https://biodash.github.io/events/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/events/","section":"","summary":"Events, courses, etc.  On this page, we will list upcoming events related to computational biology, coding, and data analysis. If you know of an event that you think should be listed here, please use the form in the About page.","tags":null,"title":"","type":"page"},{"authors":null,"categories":null,"content":"Suggest a topic or event  If there is a topic you would like to see covered on this website, an event you would like to see happen, or an event you think should be listed under Events, please fill out the form below. You can also indicate whether you would like to help with this content or event!\nLoading…   \n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605479556,"objectID":"ec134bd5815c50401fba8e9a987eda12","permalink":"https://biodash.github.io/suggest/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/suggest/","section":"","summary":"Suggest a topic or event  If there is a topic you would like to see covered on this website, an event you would like to see happen, or an event you think should be listed under Events, please fill out the form below.","tags":null,"title":"","type":"page"},{"authors":null,"categories":null,"content":"Bioinformatics support  Available support Need assistance with bioinformatics, high-throughput sequencing data analysis, working with big data, etc? See below for some support options for OSU researchers.\nMCIC The Molecular and Cellular Imaging Center (MCIC) is an OSU core facility with locations at the Wooster and Columbus campuses. Among other things, the MCIC provides end-to-end support for genomics projects \u0026mdash; from experimental design, library preparation, and sequencing, to infrastructure for and assistance with data analysis.\nThe bioinformatics section of the MCIC, the MCBL (MCIC Computational Biology Laboratory), works based on a membership model. MCBL members have access to bioinformatics support, our project at the Ohio Supercomputer Center for storage and computing, our two in-house servers, our computer lab in Wooster, and free access to workshops. To become a member, please fill out this form.\nFor bioinformatics consultation, feel free to contact Jelmer Poelstra.\nEEOB Research Scientist Michael Broe provides bioinformatics support to faculty and students in the Department of Evolution, Ecology and Organismal Biology. His primary role is to assist graduate students learning various types of computational analysis, but he also works directly with PIs. Types of analysis include whole genome assembly, genome annotation, transcriptomics, RADseq, hybrid capture pipelines, variant calling etc. Michael also teaches a 7 week Introduction to Computation in Biology for incoming EEOB graduate students each year, covering R, Python, and Unix.\n Request support Loading…   \n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1662110058,"objectID":"6f03e42d6bae7b75ea525bec87eb719f","permalink":"https://biodash.github.io/support/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/support/","section":"","summary":"Bioinformatics support  Available support Need assistance with bioinformatics, high-throughput sequencing data analysis, working with big data, etc? See below for some support options for OSU researchers.\nMCIC The Molecular and Cellular Imaging Center (MCIC) is an OSU core facility with locations at the Wooster and Columbus campuses.","tags":null,"title":"","type":"page"},{"authors":null,"categories":null,"content":"An introduction to RNA-seq data processing and analysis     Date Topic Slides     2021-01-08 Introductory notes on RNA-seq and NGS data    2021-01-29 Running FastQC for many files at OSC    2021-02-05 Interpreting FastQC output    2021-02-12 Running MultiQC    2021-02-17 Preprocessing FASTQ files    2021-02-17 Intro to RNAseq alignment and STAR    2021-02-26 RNAseq read alignment with STAR    2021-03-05 Getting to know BAM files    2021-03-12/19 Creating a gene count table with feautureCounts    2021-03-26 Differential expression analysis with DESeq2 \u0026ndash; Part I       \n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632699211,"objectID":"55d204a66b59da84421ea7308e9df8ba","permalink":"https://biodash.github.io/tutorials/2021-01_rnaseq/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/tutorials/2021-01_rnaseq/","section":"tutorials","summary":"An introduction to RNA-seq data processing and analysis     Date Topic Slides     2021-01-08 Introductory notes on RNA-seq and NGS data    2021-01-29 Running FastQC for many files at OSC    2021-02-05 Interpreting FastQC output    2021-02-12 Running MultiQC    2021-02-17 Preprocessing FASTQ files    2021-02-17 Intro to RNAseq alignment and STAR    2021-02-26 RNAseq read alignment with STAR    2021-03-05 Getting to know BAM files    2021-03-12/19 Creating a gene count table with feautureCounts    2021-03-26 Differential expression analysis with DESeq2 \u0026ndash; Part I","tags":null,"title":"","type":"tutorials"},{"authors":null,"categories":null,"content":"\nThe BioDASH website aims to assemble bioinformatic and computational training resources for researchers at The Ohio State University. It\u0026rsquo;s a joint initiative by bioinformaticians at OSU\u0026rsquo;s Molecular and Cellular Imaging Center (MCIC) - Computational Biology Lab, the Center for Applied Plant Sciences (CAPS), and the Department of Evolution, Ecology and Organismal Biology (EEOB).\nMain Contributors   Jelmer Poelstra, MCIC Wooster\n  Mike Sovic, CAPS\n  Michael Broe, EEOB\n  Suggest a topic or event If there is a topic you would like to see covered on this website, an OSU event you would like to see happen, or an event you think should be listed under Events, please fill out the form below. You can also indicate whether you would like to help with this content or event!\nLoading…   \n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629301608,"objectID":"8576ec274c98b3831668a172fa632d80","permalink":"https://biodash.github.io/about/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/about/","section":"","summary":"The BioDASH website aims to assemble bioinformatic and computational training resources for researchers at The Ohio State University. It\u0026rsquo;s a joint initiative by bioinformaticians at OSU\u0026rsquo;s Molecular and Cellular Imaging Center (MCIC) - Computational Biology Lab, the Center for Applied Plant Sciences (CAPS), and the Department of Evolution, Ecology and Organismal Biology (EEOB).","tags":null,"title":"About the BioDASH website","type":"page"},{"authors":null,"categories":null,"content":" Here, you will find general information about computer setup for Code Club. Additional setup instructions for individual sessions will appear in the posts for each session.\nSummary   Install R and RStudio (if you have an older version of R, then consider updating R).\n  Install the tidyverse package and test if you can load it.\n  Prefer not to install anything locally? Contact Jelmer to be added to the Ohio Supercomputer project for Code Club.\n  Run into issues or have questions? Don\u0026rsquo;t hesitate to contact Jelmer or one of the other organizers. You can also come to Code Club Zoom 15 minutes early, and one or more of the organizers should be there already.\n   Local installation  Already have R installed?   Please check your version of R \u0026ndash; this information is printed to the console when you start R, and you can also get it by typing sessionInfo() and checking the first line of the output.\n  As of June 2022, ideally, your R version is at least 4.2.0. But if you\u0026rsquo;re reluctant to update, you should generally be fine with an R version of at least 4.0.\n  To update R, see this page for instructions.\n  Make sure you have installed the tidyverse package (try library(tidyverse)).\n    Install R  Windows: Download and run the .exe file for the latest version of R from https://cran.r-project.org/bin/windows/base/, by clicking the large Download R [version-number] for Windows link at the top of the gray box. Mac: Download and run the .pkg file for the latest version of R from https://cran.r-project.org/bin/macosx/, by clicking the link just below Latest release. On a Linux distribution, you can also install R using the website above, but you may prefer to use a package manager instead \u0026ndash; for instance, seee these instructions for installing the latest R version on Ubuntu 20.04 using the apt package manager.  Install RStudio RStudio is a so-called Integrated Development Environment (IDE) for R, with side-by-side panes for an R script, an R concole, plots, help documents, and much more. While it is perfectly possible to use R without RStudio, RStudio has become the de facto standard for working with R and is very useful.\nTo install RStudio, go to the RStudio download page and download and run the installer file for your operating system.\nInstall the tidyverse To install or update the tidyverse, which is a collection of useful R packages, copy the following command into an R console, and press Enter:\ninstall.packages(\u0026#34;tidyverse\u0026#34;) Test whether you can load the tidyverse When you issue the command library(tidyverse) in your R console inside RStudio, you should get output similar to what is shown below:\nlibrary(tidyverse) ── Attaching packages ────────────────────────────────────────────────────────── ───────────────────────────────────────────── tidyverse 1.3.1 ── ✔ ggplot2 3.3.6 ✔ purrr 0.3.4 ✔ tibble 3.1.7 ✔ dplyr 1.0.9 ✔ tidyr 1.2.0 ✔ stringr 1.4.0 ✔ readr 2.1.2 ✔ forcats 0.5.1 ── Conflicts ─────────────────────────────────────────────────────────────────── ─────────────────────────────────────── tidyverse_conflicts() ── ✖ dplyr::filter() masks stats::filter() ✖ dplyr::lag() masks stats::lag() If you get an error instead, please try to troubleshoot it by following any instructions given, or by Googling the error message. It may be necessary to update R itself, see here for instructions. You can also send the organizers of Code Club an email.\n Alternative: Use RStudio Server at OSC Upon request (contact Jelmer), you can get access to the Ohio Supercomputer Center (OSC) Classroom Project for Code Club (PAS1838). This way, you can code in RStudio from your browser rather than with a local installation. This is a good option if you prefer not to install anything or if you run into problems during installations.\nAfter you asked for access to the OSC project, you should receive an email from OSC that you have been added to the Code Club OSC project.\n  If you already have an OSC account, you shouldn\u0026rsquo;t need to do anything to gain access, although the email may ask you to confirm/accept your being added to project.\n  If you do not yet have an OSC account, the email you received from OSC should have a link to do so. Alternatively, follow the instructions below to sign up and get access to the project.\n  Instructions to sign up at OSC (click here)  To sign up:\n  Go to https://my.osc.edu/ and click the blue \u0026ldquo;Sign Up\u0026rdquo; bar.\n  In the bottom right portion of the form where you provide your info (see screenshot below), you should enter Code Club\u0026rsquo;s Project Code, which is PAS1838. If you want to use OSC, please do this on a day prior to your first Code Club participation. This way, there is time to troubleshoot if needed. Moreover, the Code Club option on the Interactive Apps page below can take a few hours to appear after you become a member of the project.\n    Enter Project Code PAS1838 in the red box (click to enlarge)     Run RStudio Server from the OSC website OSC OnDemand lets you access OSC resources through your browser and run applications like RStudio. It has a separate access point, https://class.osc.edu/, for \u0026ldquo;classroom projects\u0026rdquo; such as this one.\n  To get started, go to https://class.osc.edu/ and log in with your OSC username and password.\n  Click on Apps in the blue top bar, and select RStudio Server: Form, as shown below:\n     Now, you\u0026rsquo;re on a page from which you can launch an RStudio server that will run on an OSC cluster. As shown below, make sure that \u0026ldquo;Code Club\u0026rdquo; is selected under Class Materials, and change the select Number of hours to 2. Then click Launch.\n    You should see a box like this:\n    Your job usually starts running within seconds, and the color of the top bar will then switch from blue (\u0026ldquo;Queued\u0026rdquo; and then \u0026ldquo;Starting\u0026rdquo;) to green (\u0026ldquo;Running\u0026rdquo;):\n    Click Connect to RStudio Server at the bottom of the box, and an RStudio Server instance will open.\n  Test whether you can load the tidyverse Several commonly used packages will be automatically available to you at OSC, and that should include the tidyverse. Test whether you can load the tidyverse by running library(tidyverse), and check whether you get output similar to what is shown below (exact versions of packages may differ):\nlibrary(tidyverse) ── Attaching packages ────────────────────────────────────────────────────────── ───────────────────────────────────────────── tidyverse 1.3.1 ── ✔ ggplot2 3.3.6 ✔ purrr 0.3.4 ✔ tibble 3.1.7 ✔ dplyr 1.0.9 ✔ tidyr 1.2.0 ✔ stringr 1.4.0 ✔ readr 2.1.2 ✔ forcats 0.5.1 ── Conflicts ─────────────────────────────────────────────────────────────────── ─────────────────────────────────────── tidyverse_conflicts() ── ✖ dplyr::filter() masks stats::filter() ✖ dplyr::lag() masks stats::lag() If you get an error, install the tidyverse as follows:\ninstall.packages(\u0026#34;tidyverse\u0026#34;) More about OSC The above instructions should be all you need to access RStudio using OSC, but there is lot more to OSC than that! For more information about using OSC, see the excellent Getting Started materials on their website (make sure not to miss the HOWTOs). Also, Mike Sovic has a YouTube playlist \u0026ldquo;Getting Started With High Performance Computing (HPC)\u0026quot; at his channel The Data Point.\n More info Please see our R Resources and Tips page for:\n Resources to get started with R Useful R and RStudio settings The basics of installing packages in R Instructions for updating R    \n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1654709636,"objectID":"a19fd71e3dc86af820a45dabc994dda5","permalink":"https://biodash.github.io/codeclub-setup/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/codeclub-setup/","section":"","summary":"Here, you will find general information about computer setup for Code Club. Additional setup instructions for individual sessions will appear in the posts for each session.\nSummary   Install R and RStudio (if you have an older version of R, then consider updating R).","tags":null,"title":"Computer Setup for Code Club","type":"page"},{"authors":null,"categories":null,"content":" Introduction   Each Code Club session should be represented by one post on the website at https://biodash.github.io/codeclub/.\n  Regular presenters will be given direct access to the Github repository and will be able to push a new post to the website directly.\n  Occasional presenters can either send their material directly to Jelmer or create a \u0026ldquo;pull request\u0026rdquo; with their new post.\n  Content should be written in R Markdown (.Rmd) or \u0026ldquo;plain\u0026rdquo; Markdown (.md). If you write in .Rmd, you need to render to .md locally. Conversion of .md to an HTML file suitable for the website will be done automatically upon pushing the master branch of the repository.\n  Make sure to get the session materials onto the website at least several days before the session.\n   Getting your files onto the site 1: Get the repo You only need to do this if you want to create a pull request or push your content to the website directly. If you want to send your (R) Markdown file by email, skip this and continue to Step 2.\nThe following assumes you have git installed, set up, have a Github account, and have your git linked up to Github.\nOption A: Fork the repo to prep for a Pull Request   Fork the repo: go to https://github.com/biodash/biodash.github.io and click the Fork button way in the top-right corner of the page.\n  Get the URL for your repo: In your forked repo, click the green Code button and copy the URL for the repo to your clipboard (either the HTTPS or the SSH URL; the former will be less likely to lead to authentication problems).\n  Go to a dir that you would like to be the parent dir of the Biodash/Codeclub repo:\ncd my-dir   Clone your forked repo, using the URL that you copied to your clipboard:\ngit clone https://github.com/\u0026lt;YOUR-USERNAME\u0026gt;/biodash.github.io.git   Move into the newly cloned (downloaded) repository dir:\ncd biodash.github.io   Add the original repository as an \u0026ldquo;upstream\u0026rdquo; remote:\ngit remote add upstream https://github.com/biodash/biodash.github.io.git  You can check which remote repos (i.e., repos on Github) are linked to your local repo using:\ngit remote -v This should show your forked repo as \u0026ldquo;origin\u0026rdquo;, and the original repo as \u0026ldquo;upstream\u0026rdquo;. You won\u0026rsquo;t be able to push to the original repo, but you can push to your forked repo and then submit a pull request, as we\u0026rsquo;ll do below.\n    Option B: Clone the repo directly (direct access required)   Go to a dir that you would like to be the parent dir of the Biodash/Codeclub repo:\ncd my-dir   Clone the website repo:\ngit clone https://github.com/biodash/biodash.github.io.git # Using HTTPS # Or: `git clone git@github.com:biodash/biodash.github.io.git` using SSH   Create a new branch (by way of example called \u0026ldquo;my-branch\u0026rdquo;) and switch to it:\ngit checkout -b my-branch Creating a new branch is not strictly necessary but it may be safer/easier to experiment in.\n   2: Create a Code Club post   Here, we\u0026rsquo;ll use the hugodown package to create a Markdown skeleton for our post, and below we\u0026rsquo;ll also use hugodown to preview the site.\nNote that you can easily bypass hugodown by simply copying the YAML header from the first code club session (see here for the .Rmd file) into a new file and taking it from there.  If you don\u0026rsquo;t have the hugodown package installed, install it:\nremotes::install_github(\u0026#34;r-lib/hugodown\u0026#34;) # Or equivalently, use devtools::install_githhub()   A post bundle is a separate folder for a post which will hold the R Markdown file that contains the post, as well as associated images and so on. To create a post bundle along with a R Markdown file that already has many useful YAML header tags:\nhugodown::use_post(\u0026#39;codeclub/\u0026lt;session-number\u0026gt;_\u0026lt;short-title\u0026gt;\u0026#39;) # An example would be: hugodown::use_post(\u0026#39;codeclub/01_intro-to-R\u0026#39;) The \u0026lt;session-number\u0026gt; is the actual Code Club session number, and \u0026lt;short-title\u0026gt; is a short title that you would like to give the post, which will be used for links and the folder name.\n The name of the .Rmd file will be index.Rmd, and it should keep that name! Keep this name also if you create your .Rmd manually or by copying the file from another Code Club session. It will eventually turn into index.html, which is the name that will trigger the file to be displayed on the website.     Fill out some of the YAML, such as the title, subtitle, authors (in kebab-case, e.g. john-doe, to link to your author profile; note that Jelmer\u0026rsquo;s name here is \u0026ldquo;admin\u0026rdquo;), and optionally tags and summary (the summary will appear on Biodash\u0026rsquo;s front page in the \u0026ldquo;Recent Posts\u0026rdquo; widget; this can be good to fill out here because the default summary can be awkward, as it combines headers and paragraphs).\n If you specify a date using the `date` tag in the YAML, and this date is in the future (e.g. the date of the Code Club session), the page will not be built and will thus not appear on the website! Specifiying the date using `date` or `lastmod` in the YAML is not particularly useful anyway -- when you edit the post after the specified date, it will use the edit date.     Write the contents of your Code Club session that you would like to share with participants, in R Markdown format. For formatting tips, see below.\n   If you want participants to load an R Markdown file or script:\nAn easy solution is to place the file in the same directory as your post, and include it in your git commit, so it will be uploaded to Github. In that case, the URL to the file for direct downloads for participants will be: https://raw.githubusercontent.com/biodash/biodash.github.io/master/docs/codeclub/\u0026lt;session-number\u0026gt;_\u0026lt;short-title\u0026gt;/\u0026lt;filename\u0026gt;.\nIn your post, include a function call like file.download(\u0026lt;script-URL\u0026gt;) for participants to get the file \u0026ndash; this will work both for participants working locally and those working in an OSC RStudio Server instance.\nIf your session contains a dataset:\nLike for the markdown/script, place the file(s) in the same directory as your post. If you have a markdown/script for participants, include file.download(\u0026lt;dataset-URL\u0026gt;) in this file, otherwise include it directly in your post.\n    Convert your .Rmd (R Markdown) file to a .md (Markdown) file.\n Hugo renders .md but not .Rmd to HTML, so we have to always render to .md first when writing in .Rmd.   Since your output is specified as hugodown::md_document, this is done most easily by \u0026ldquo;knitting\u0026rdquo; your post in RStudio by clicking Knit in the top bar, or by pressing Ctrl + Shift + K.\n   3: Preview your post or build the website (optional) You can do this in two ways, from RStudio or from the command line.\nOption A: In RStudio   Install Hugo:\nhugodown::hugo_install(\u0026#34;0.66.0\u0026#34;)   Preview the website:\nhugodown::hugo_start() #\u0026gt; Starting server on port 1313 This will provide a preview RStudio. To look at it in a browser, go to localhost:1313, where 1313 corresponds to the port returned in the R console (see above).\n  Option B: From the command line   Install Hugo using these instructions.\n  Serve the website locally:\nhugo serve You will see a message that includes \u0026ldquo;Web Server is available at [\u0026hellip;]\u0026rdquo;. Click the link or copy and paste the address into a browser, and you will see the rendered website.\nThe server will keep running and will update whenever you save changes in a file that is within the website directory, until you stop it using Ctrl + C.\n   Side note: Building the website Note that you don\u0026rsquo;t need to build the website, because it will be built automatically from Markdown files whenever you push to (the master branch of) the Github repo.\nBut as background info, or in case automatic builds fail, here is how you would build the site:\n  Using Hugo from the shell:\nhugo -d docs/   Using hugodown in R:\nhugodown::hugo_build(dest = \u0026#34;docs\u0026#34;)   The entire rendered website is in the docs/ dir; HTML files rendered from Markdown files will be placed there, any images and other files will be copied there, and so on.\n   4: Commit   Add the files from your post:\ngit add codeclub/\u0026lt;your-post-name\u0026gt;/* ## Or, e.g. if you added files elswehere too, or have built the site: # git add *   Check if all your changes and new files have been staged:\ngit status   Commit:\ngit commit -m \u0026#34;Add CodeClub session \u0026lt;session-nr\u0026gt; by \u0026lt;your-name\u0026gt;\u0026#34;    5: Push or submit pull request Your Markdown (.md) file(s) will be built along with the rest of the website by Hugo. Using Github Actions, this will be done automatically upon pushing to the master branch on Github, which is all we need to do. Note that the built website will be committed by Github Actions not to the master branch but to the gh-actions branch.\nOption A: Create a pull request When you create a pull request, you are asking the maintainers of a repository to pull your changes into their repository.\n  Pull from the original repo to make sure your repo is up-to-date:\ngit pull upstream master # \u0026#34;upstream\u0026#34; refers to the original Github repo This will first fetch the upstream changes and then merge them into your local repo, thus keeping your local changes. If git does not manage to perform this merge automatically, which can happen if the same parts of the same files have been edited both locally and upstream, there will be a merge conflict which you will need to resolve manually.\n  Push to your forked repo:\ngit push origin master # \u0026#34;origin\u0026#34; refers to your forked Github repo   Create the pull request:\n Go to the Pull requests page of our repo at https://github.com/biodash/biodash.github.io/pulls. Click the green button on the right that says New pull request. Under the large Compare changes header, click Compare across forks. In the drop-down menu to the right of the arrow, select your fork. Enter a title (e.g. \u0026ldquo;New Post: Session 6\u0026quot;) and description (say a little more about the post) for the pull request. Click the green button Send pull request.    For a more detailed step-by-step of creating a pull request from a fork, see here.\nOption B: Push to the site repo (direct access required)   Merge your branch with the main (master) branch:\ngit checkout master # Move to the master branch prior to merging git merge my-branch # Merge into master (assuming your branch was named \u0026#34;my-branch\u0026#34;)   Push to the master branch:\ngit push origin master    6: Install packages at OSC (optional) Many R packages are already installed at OSC (nearly 200 for R 4.0.2), including the tidyverse. You can check which packages have been installed by typing, in an R session at OSC:\nlibrary() This will list packages by library, which should include two locations available to all OSC users (starting with /usr/local/R), your personal library, and the Code Club library (/fs/ess/PAS1838/CODECLUB/Rpkgs).\nIf you want to make another package available to Code Club participants, you can do so as follows in an RStudio Server session at OSC:\ninstall.packages(\u0026#34;\u0026lt;pkg-name\u0026gt;\u0026#34;, lib = \u0026#34;/fs/ess/PAS1838/CODECLUB/Rpkgs\u0026#34;) This library is available to all members of the Code Club OSC classroom project. To check specifically which packages are available in this library \u0026ndash; and whether your newly installed package has indeed been installed here, type:\nlibrary(lib.loc = \u0026#34;/fs/ess/PAS1838/CODECLUB/Rpkgs\u0026#34;) Alternatively, you can let participants working at OSC install the packages themselves, like participants that work locally will have to do.\n Formatting tips Miscellaneous   If you want a Table of Contents (TOC) for your file, add a line toc: true to the YAML (not indented, as it is not an option of the output format).\n  To add an image, put it in the same directory as the markdown file, and refer to it without prepending a path.\n  \u0026lt;br\u0026gt; will insert a line break, which can be useful to get more space between sections.\n  I add lines above each major section header using ---- (preceded by a \u0026lt;br\u0026gt;).\n  Add a line that reads source_extension: '.Rmd' (not indented) to your R Markdown, which will ensure that there is a link to the source document at the top of your post.\nEDIT: I have removed these source links for now. They were also visible in the \u0026ldquo;Recent Posts\u0026rdquo; widget on the home page, and some people clicked on that link rather than the website link. Then, they ended up on in the Github repo but didn\u0026rsquo;t even know they were in the wrong place since the contents of the post is present.\n  Hidden sections It can be useful to provide solutions to small challenges in the file, but to hide them by default. This can be done with a little HTML:\n\u0026lt;details\u0026gt; \u0026lt;summary\u0026gt; Solution (click here) \u0026lt;/summary\u0026gt; \u0026lt;br\u0026gt; ... Your solution - this can be a long section including a code block... ```{r} install.packages(\u0026quot;tidyverse\u0026quot;) ``` \u0026lt;/details\u0026gt; This is rendered as:\n  Solution (click here)  \u0026hellip; Your solution - this can be a long section including a code block\u0026hellip;\ninstall.packages(\u0026quot;tidyverse\u0026quot;)  Info/alert notes To produce boxes to draw attention to specific content, you can use two classes specific to the Hugo Academic Theme (now branded as \u0026ldquo;Wowchemy\u0026rdquo;).\n  alert-note for a blue box with an info symbol:\n\u0026lt;div class=\u0026quot;alert alert-note\u0026quot;\u0026gt; \u0026lt;div\u0026gt; This is an alert note. \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; Which is rendered as:\n This is an alert note.     alert-warning for a red box with a warning symbol:\n\u0026lt;div class=\u0026quot;alert alert-warning\u0026quot;\u0026gt; \u0026lt;div\u0026gt; This is an alert warning. \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; Which is rendered as:\n This is an alert warning.     I also added a custom class, puzzle:\n\u0026lt;div class=\u0026quot;alert puzzle\u0026quot;\u0026gt; \u0026lt;div\u0026gt; This is a puzzle div, for do-it-yourself challenges. \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt;  This is a puzzle div, for do-it-yourself challenges.   Custom classes and other custom formatting can be written in CSS in the assets/scss/custom.scss file.\n  All of these classes can also be called using pandoc\u0026rsquo;s ::: notation when you\u0026rsquo;re writing in .Rmd (but not if you\u0026rsquo;re writing in .md), e.g.:\n:::puzzle This is a puzzle div, for do-it-yourself challenges. :::   Code highlighting  Code highlighting doesn't work with out of the box with .Rmd files. But it should be possible to get it to work, stay tuned!   Hugo supports the highlighting of specific lines of code using the syntax below in md documents:\n```r {hl_lines=[1,\u0026quot;3-4\u0026quot;]} library(\u0026quot;tidyverse\u0026quot;) weight_df %\u0026gt;% mutate(mean_weight = mean(weight)) %\u0026gt;% select(mean_weight, everything()) dim(weight_df) ``` library(\u0026#34;tidyverse\u0026#34;) weight_df %\u0026gt;% mutate(mean_weight = mean(weight)) %\u0026gt;% select(mean_weight, everything()) dim(weight_df) Shortcodes  Like code highlighting, shortcodes only work with .md files. The blogdown package has a shortcode() function to support them (see here), but hugodown does not support them.   Hugo shortcodes are little code snippets for specific content. Some of these are specific to Wowchemy, and others are available for any Hugo site.\nHighlight text You can highlight text as follows:\nHere is some {{\u0026lt; hl \u0026gt;}}highlighted text{{\u0026lt; /hl \u0026gt;}}. This will render as:\nIcons Wowchemy supports shortcodes for icons, for instance:\n {{\u0026lt; icon name=\u0026#34;r-project\u0026#34; pack=\u0026#34;fab\u0026#34; \u0026gt;}}   {{\u0026lt; icon name=\u0026#34;python\u0026#34; pack=\u0026#34;fab\u0026#34; \u0026gt;}}   {{\u0026lt; icon name=\u0026#34;terminal\u0026#34; pack=\u0026#34;fas\u0026#34; \u0026gt;}} General Hugo shortcodes   To embed a Youtube video, use the following, replacing \u0026ldquo;videoID\u0026rdquo; by the actual ID (https://www.youtube.com/watch?v=ID) in\n{{\u0026lt; youtube ID \u0026gt;}}   To embed a Tweet, use the following, replacing \u0026ldquo;tweetID\u0026rdquo; by the actual ID (https://twitter.com/user/status/ID):\n{{\u0026lt; tweet ID \u0026gt;}}   For more info and more shortcodes, see the Hugo documentation on shortcodes.\n   \n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1642609187,"objectID":"d2a87fb5a2b4f8e331f2a7033b4ca3df","permalink":"https://biodash.github.io/codeclub-present/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/codeclub-present/","section":"","summary":"Introduction   Each Code Club session should be represented by one post on the website at https://biodash.github.io/codeclub/.\n  Regular presenters will be given direct access to the Github repository and will be able to push a new post to the website directly.","tags":null,"title":"Information for Code Club Presenters","type":"page"},{"authors":null,"categories":null,"content":" Spring \u0026lsquo;23 We are planning to meet on Thursdays at 3 pm, starting on January 12th.\nIf you haven\u0026rsquo;t previously signed up, do so using the form!\nYou can join us either on Zoom (sign up for the link) or in-person:\n In Columbus, Jessica will be in Howlett 340. In Wooster, Jelmer will be in Selby 203.  Upcoming sessions TBA\n Previous semesters Ask us for access to the Zoom recordings of previous Code Club sessions!\nSummer - Fall \u0026lsquo;22  R4DS = The R for Data Science book     Session Date Presenter Topic     S04E01 June 09 Jelmer R4DS, Ch. 1: Intro   S04E02 June 23 Michael B. R4DS, Ch. 3.1- 3.4: Data viz I   S04E03 June 30 Jessica R4DS, Ch. 3.5: Data viz II   S04E04 July 07 Jessica R4DS, Ch. 3.6: Data viz III   S04E05 July 14 Mike S. R4DS, Ch. 3.7: Data viz IV   S04E06 July 21 Jelmer R4DS, Ch. 3.8-3.10: Data viz V   S04E07 July 28 Michael B. R4DS, Ch. 27: R Markdown   S04E08 Sept 15 Jessica R4DS, Ch. 4, 6 \u0026amp; 8: Workflow   S04E09 Sept 22 Jelmer R4DS, Ch. 5.1-5.2: Data transformation I - intro and filter()   S04E10 Oct 06 Michael B. R4DS, Ch. 5.3: Data transformation II - arrange()   S04E11 Oct 20 Michael B. R4DS, Ch. 5.4: Data transformation III - select()   S04E12 Oct 27 Stephen R4DS, Ch. 5.5: Data transformation IV - mutate()   S04E13 Nov 03 Mike S. R4DS, Ch. 5.6: Data transformation V - summarize() part 1   S04E14 Nov 10 Jessica R4DS, Ch. 5.6: Data transformation V - summarize() part 2   S04E15 Nov 17 Jelmer R4DS, Ch. 7.1-7.3: Exploratory Data Analysis I   S04E16 Dec 01 Jelmer R4DS, Ch. 7.1-7.3: Exploratory Data Analysis II   S04E17 Dec 08 Michael B. R4DS, Ch. 10: Tibbles    Spring \u0026lsquo;22    Session Date Presenter Topic (+ link) Topic block     S03E01 Jan 13 Mike S. T-tests Basic statistics in R   S03E02 Jan 20 Jessica ANOVA Basic statistics in R   S03E03 Jan 27 Jelmer Principal Component Analysis Basic statistics in R   S03E04 Feb 03 Jessica ANOVA part II Basic statistics in R   S03E05 Feb 10 Stephen Correlation Basic statistics in R   S03E06 Feb 17 Michael B. Data structures and subsetting Basic statistics in R   S03E07 Feb 24 Jelmer Intro to \u0026ldquo;DRY\u0026rdquo; and iteration \u0026ldquo;Don\u0026rsquo;t repeat yourself\u0026rdquo; (DRY)   S03E08 Mar 03 Jelmer Loops \u0026ldquo;Don\u0026rsquo;t repeat yourself\u0026rdquo; (DRY)   S03E09 Mar 10 Mike S. Apply functions \u0026ldquo;Don\u0026rsquo;t repeat yourself\u0026rdquo; (DRY)   S03E10 Mar 24 Mike S. Purrr map functions \u0026ldquo;Don\u0026rsquo;t repeat yourself\u0026rdquo; (DRY)   S03E11 Apr 14 Michael B. Writing functions part I \u0026ldquo;Don\u0026rsquo;t repeat yourself\u0026rdquo; (DRY)   S03E12 Apr 21 Michael B. Incorporating your own functions into loops \u0026ldquo;Don\u0026rsquo;t repeat yourself\u0026rdquo; (DRY)    Fall \u0026lsquo;21    Session Date Presenter Topic (+ link) Topic block     S02E01 Aug 26 Jelmer R basics \u0026ndash; part I Back to basics in R   S02E02 Sep 2 Mike S. R basics \u0026ndash; part II Back to basics in R   S02E03 Sep 9 Michael B. R Markdown Back to basics in R   S02E04 Sep 16 Jessica Tidyverse intro \u0026ndash; part I Back to basics in R   S02E05 Sep 23 Stephen Tidyverse intro \u0026ndash; part II Back to basics in R   S02E06 Sep 30 Michael B. ggplot intro \u0026ndash; part I Plotting in R   S02E07 Oct 07 Jelmer ggplot intro \u0026ndash; part II Plotting in R   S02E08 Oct 21 Mike S. Faceting plots Plotting in R   S02E09 Oct 28 Mike S. Faceting and combining plots Plotting in R   S02E10 Nov 04 Daniel Quiroz Moreno (HCS) Adding statistical results to plots using ggpubr Plotting in R   S02E11 Nov 18 Julia Vrtilek (EEOB) Analysis of bat social vocalizations with R at OSC Putting code to practice   S02E12 Dec 02 Matthew Teegarden (FST) Shiny bright like a diamond: an intro to building interactive applications in R Putting code to practice   S02E13 Dec 09 Jessica Interactive plots with Plotly Plotting in R    Fall \u0026lsquo;20 - Spring \u0026lsquo;21    Session nr. Date Presenter Topic (+ link) Other     S01E01 Nov 18, 2020 Jelmer RStudio Projects \u0026amp; getting started slides   S01E02 Dec 2, 2020 Jessica dplyr core verbs    S01E03 Dec 9, 2020 Mike S. Joining datasets    S01E04 Dec 16, 2020 Michael B. ggplot2 \u0026ndash; round 1    S01E05 Jan 15, 2021 Jessica ggplot2 \u0026ndash; round 2    S01E06 Jan 22, 2021 Stephen Factors    S01E07 Jan 29, 2021 Jelmer R Markdown    S01E08 Feb 5, 2021 Mike S. Pivoting data    S01E09 Feb 12, 2021 Michael B. Subsetting data    S01E10 Feb 19, 2021 Jessica Faceting, animating, and combining plots    S01E11 Feb 26, 2021 Stephen Making maps with ggmap    S01E12 Mar 5, 2021 Jelmer Vectorization and loops    S01E13 Mar 12, 2021 Mike S. The apply family of functions    S01E14 Mar 19, 2021 Michael B. Writing your own functions    S01E15 Mar 26, 2021 Jessica Interactive plots with Plotly    S01E16 Apr 02, 2021 Stephen Working with dates with lubridate    S01E17 Apr 09, 2021 Jelmer Introduction to regular expressions    S01E18 Apr 16, 2021 Mike S. Regular Expressions: Part II    S01E19 Apr 23, 2021 Michael B. Word Clouds via tidytext    S01E20 Apr 30, 2021 Jessica Cleaning up variables names        \n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670964554,"objectID":"21036bb90326781dbc3f5f76a5396fb3","permalink":"https://biodash.github.io/codeclub-schedule/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/codeclub-schedule/","section":"","summary":"Spring \u0026lsquo;23 We are planning to meet on Thursdays at 3 pm, starting on January 12th.\nIf you haven\u0026rsquo;t previously signed up, do so using the form!\nYou can join us either on Zoom (sign up for the link) or in-person:","tags":null,"title":"Schedule for Code Club","type":"page"},{"authors":null,"categories":null,"content":" Please use the form below to suggest a topic or concept to be covered at Code Club. You are also welcome to leave a suggestion relating to the general format, organization, or presentation of Code Club.\nIf you suggest a topic, a broad range of suggestions are welcome \u0026ndash; it need not fit neatly as a single Code Club session. So your suggestion can be as broad as \u0026ldquo;object-oriented programming\u0026rdquo; or as specific as a single R function that you happen to struggle with or that you just really like.\nLoading…   \n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629301641,"objectID":"aa47a17260af4b87dc29397d821ae7fd","permalink":"https://biodash.github.io/codeclub-suggest/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/codeclub-suggest/","section":"","summary":"Please use the form below to suggest a topic or concept to be covered at Code Club. You are also welcome to leave a suggestion relating to the general format, organization, or presentation of Code Club.","tags":null,"title":"Suggest a Topic for Code Club!","type":"page"}]